<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="HIVE">
	<bug fixdate="2015-12-21 00:52:09" id="12644" opendate="2015-12-10 12:31:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Support for offset in HiveSortMergeRule</summary>
			
			
			<description>After HIVE-11531 goes in, HiveSortMergeRule needs to be extended to support offset properly when it merges operators that contain Limit. Otherwise, limit pushdown through outer join optimization (introduced in HIVE-11684) will not work properly.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortMergeRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11684</link>
			
			
			<link description="is broken by" type="Regression">11531</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-21 21:52:47" id="12635" opendate="2015-12-09 20:20:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive should return the latest hbase cell timestamp as the row timestamp value</summary>
			
			
			<description>When hive talks to hbase and maps hbase timestamp field to one hive column,  seems hive returns the first cell timestamp instead of the latest one as the timestamp value. 
Makes sense to return the latest timestamp since adding the latest cell can be  considered an update to the row. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.LazyHBaseRow.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-08 20:04:26" id="12792" opendate="2016-01-06 21:21:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HIVE-12075 didn&amp;apos;t update operation type for plugins</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12781</link>
			
			
			<link description="is broken by" type="Regression">12075</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-06 00:10:33" id="12990" opendate="2016-02-03 08:37:27" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: ORC cache NPE without FileID support</summary>
			
			
			<description>


   OrcBatchKey stripeKey = hasFileId ? new OrcBatchKey(fileId, -1, 0) : null;

   ...

          if (hasFileId &amp;amp;&amp;amp; metadataCache != null) {

            stripeKey.stripeIx = stripeIx;

            stripeMetadata = metadataCache.getStripeMetadata(stripeKey);

          }

...

  public void setStripeMetadata(OrcStripeMetadata m) {

    assert stripes != null;

    stripes[m.getStripeIx()] = m;

  }






Caused by: java.lang.NullPointerException

        at org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.getStripeIx(OrcStripeMetadata.java:106)

        at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.setStripeMetadata(OrcEncodedDataConsumer.java:70)

        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.readStripesMetadata(OrcEncodedDataReader.java:685)

        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:283)

        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:215)

        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:212)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:415)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:212)

        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:93)

        ... 5 more


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-12 18:57:52" id="11752" opendate="2015-09-08 01:55:19" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Pre-materializing complex CTE queries</summary>
			
			
			<description>Currently, hive regards CTE clauses as a simple alias to the query block, which makes redundant works if it&amp;amp;apos;s used multiple times in a query. This introduces a reference threshold for pre-materializing the CTE clause as a volatile table (which is not exists in any form of metastore and just accessible from QB).</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QueryPlan.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11049</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-19 07:01:46" id="13079" opendate="2016-02-18 01:18:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Allow reading log4j properties from default JAR resources</summary>
			
			
			<description>If the log4j2 configuration is not overriden by the user, the Slider pkg creation fails since the config is generated from a URL.
Allow for the .properties file to be created from default JAR resources if user provides no overrides.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-19 07:03:08" id="13077" opendate="2016-02-17 20:37:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Scrub daemon-site.xml from client configs</summary>
			
			
			<description>



 if (llapMode) {

      // add configs for llap-daemon-site.xml + localize llap jars

      // they cannot be referred to directly as it would be a circular depedency

      conf.addResource(&quot;llap-daemon-site.xml&quot;);






</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="depends upon" type="dependent">12967</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-20 02:05:37" id="13086" opendate="2016-02-18 18:47:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Programmatically initialize log4j2 to print out the properties location</summary>
			
			
			<description>In some cases, llap daemon gets initialized with different log4j2.properties than the expected llap-daemon-log4j2.properties. It will be easier if programmatically configure log4j2 so that we can print out the location of properties file that is used for initialization. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-22 09:48:28" id="13089" opendate="2016-02-18 20:55:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Rounding in Stats for equality expressions</summary>
			
			
			<description>Currently we divide numRows(long) by countDistinct(long), thus ignoring the decimals. We should do proper rounding.
This is specially useful for equality expressions over columns whose values are unique. As NDV estimates allow for a certain error, if countDistinct &amp;gt; numRows, we end up with 0 rows in the estimate for the expression.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-23 23:46:51" id="13110" opendate="2016-02-20 06:34:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Package log4j2 jars into Slider pkg</summary>
			
			
			<description>This forms the alternative path for HIVE-13015 (reverted), so that HIVE-13027 can pick up the right logger impl always.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="supercedes" type="Supercedes">13100</link>
			
			
			<link description="is depended upon by" type="dependent">13027</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-24 18:32:28" id="13128" opendate="2016-02-23 21:30:54" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NullScan fails on a secure setup</summary>
			
			
			<description>Nullscan provides uris of the form nullscan://null/ - which are added to the list of FileSystems for which Tez should obtain tokens.



2016-02-19T02:48:04,481 ERROR [main]: exec.Task (TezTask.java:execute(219)) - Failed to execute tez graph.

java.lang.IllegalArgumentException: java.net.UnknownHostException: null

  at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:406) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]

  at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:291) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]

  at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:302) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]

  at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:524) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]

  at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:508) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]

  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystemsInternal(TokenCache.java:107) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystemsInternal(TokenCache.java:86) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystems(TokenCache.java:76) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.tez.client.TezClientUtils.addFileSystemCredentialsFromURIs(TezClientUtils.java:338) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.tez.client.TezClientUtils.setupDAGCredentials(TezClientUtils.java:369) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.tez.client.TezClientUtils.prepareAndCreateDAGPlan(TezClientUtils.java:704) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:522) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.tez.client.TezClient.submitDAG(TezClient.java:468) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:466) ~[hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:187) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:158) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1858) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1600) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1373) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1196) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1184) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:228) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:180) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:395) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:331) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:428) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:444) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:744) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:711) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:640) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]

  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]

  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]

  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]

  at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]

  at org.apache.hadoop.util.RunJar.run(RunJar.java:221) [hadoop-common-2.7.1.2.3.5.1-26.jar:?]

  at org.apache.hadoop.util.RunJar.main(RunJar.java:136) [hadoop-common-2.7.1.2.3.5.1-26.jar:?]

Caused by: java.net.UnknownHostException: null

  ... 37 more



This is while trying to obtain tokens for 

2016-02-23T02:49:41,782 DEBUG [main]: tez.DagUtils (DagUtils.java:addCredentials(164)) - Marking URI as needing credentials: nullscan://null/default.studenttab10k/part_


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.NullScanFileSystem.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-26 09:58:24" id="13135" opendate="2016-02-24 03:27:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: HTTPS Webservices needs trusted keystore configs</summary>
			
			
			<description>ssl-server.xml is not picked up internally by the hive-common HttpServer impl, unlike the default configs.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="D">org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebApp.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-01 20:08:56" id="12757" opendate="2015-12-29 21:44:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix TestCodahaleMetrics#testFileReporting</summary>
			
			
			<description>Codahale Metrics file reporter is time based, hence test is as well.  On slow machines, sometimes the file is not written fast enough to be read.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.metrics.metrics2.TestCodahaleMetrics.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12628</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-06 07:01:54" id="13199" opendate="2016-03-03 00:50:45" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NDC stopped working in LLAP logging</summary>
			
			
			<description>NDC context were missing from the log lines. Reason for it is NDC class is part of log4j-1.2-api (bridge jar). This is added as compile time dependency. Due to the absence of this jar in llap daemons, the NDC context failed to initialize. Log4j2 replaced NDC with ThreadContext. Hence we need the bridge jar.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-07 17:04:33" id="13209" opendate="2016-03-04 21:39:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>metastore get_delegation_token fails with null ip address</summary>
			
			
			<description>After changes in HIVE-13169, metastore get_delegation_token fails with null ip address.



2016-03-03 07:45:31,055 ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/&amp;lt;hostname@realm&amp;gt; from IP null)

	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:606)

	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)

	at com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)

	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)

	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)

	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)

	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)

	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)




</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.IpAddressListener.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">13169</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-08 18:11:28" id="13210" opendate="2016-03-04 22:47:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Revert changes in HIVE-12994 related to metastore</summary>
			
			
			<description>As we do not control what is written in the physical layer and thus we cannot ensure NULLS ORDER (and even if we did, currently we do not take advantage of it), it seems exposing the NULLS ORDER property at metastore level does not make much sense. We will revert that part of patch HIVE-12994.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HbaseMetastoreProto.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestSharedStorageDescriptor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Order.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.model.MOrder.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">11160</link>
			
			
			<link description="is part of" type="Incorporates">12994</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-08 18:30:39" id="13227" opendate="2016-03-08 08:29:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Change daemon initialization logs from INFO to WARN</summary>
			
			
			<description>In production LLAP is typically run with WARN log level. It will be useful to print the llap daemon initialization configs at WARN level instead of INFO level so that we can verify if daemon configs are propagated properly. 
NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-08 21:27:01" id="13153" opendate="2016-02-25 02:50:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>SessionID is appended to thread name twice</summary>
			
			
			<description>HIVE-12249 added sessionId to thread name. In some cases the sessionId could be appended twice. Example log line



DEBUG [6432ec22-9f66-4fa5-8770-488a9d3f0b61 6432ec22-9f66-4fa5-8770-488a9d3f0b61 main]


 </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13327</link>
			
			
			<link description="is related to" type="Reference">13485</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-09 21:39:37" id="13247" opendate="2016-03-09 20:17:48" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>HIVE-13040 broke spark tests</summary>
			
			
			<description>I confirmed that spark tests are getting stuck due to HIVE-13040. join_empty is an example test; it gets stuck on master presently, presumably because 0 splits are generated. When I reverted  HIVE-13040 locally, it passed for me. We should fix this or revert HIVE-13040</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.spark.client.RemoteDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13223</link>
			
			
			<link description="is broken by" type="Regression">13040</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-14 15:10:33" id="13251" opendate="2016-03-09 22:29:59" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hive can&amp;apos;t read the decimal in AVRO file generated from previous version</summary>
			
			
			<description>HIVE-7174 makes the avro schema change to match avro definition, while it breaks the compatibility if the file is generated from the previous Hive although the file schema from the file for such decimal is not correct based on avro definition. We should allow to read old file format &quot;precision&quot; : &quot;4&quot;, &quot;scale&quot;: &quot;8&quot;, but when we write, we should write in the new format.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">7174</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-15 00:00:48" id="13185" opendate="2016-02-29 21:44:04" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>orc.ReaderImp.ensureOrcFooter() method fails on small text files with IndexOutOfBoundsException</summary>
			
			
			<description>Steps to reproduce:
1. Create a Text source table with one line of data:



create table src (id int);

insert overwrite table src values (1);



2. Create a target table:



create table trg (id int);



3. Try to load small text file to the target table:



load data inpath &amp;amp;apos;user/hive/warehouse/src/000000_0&amp;amp;apos; into table trg;



Error message:

FAILED: SemanticException Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException
Stack trace:

org.apache.hadoop.hive.ql.parse.SemanticException: Unable to load data to destination table. Error: java.lang.IndexOutOfBoundsException

	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.ensureFileFormatsMatch(LoadSemanticAnalyzer.java:340)

	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:224)

	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:242)

	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:481)

	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:317)

	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1190)

	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1285)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1104)

...


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">14556</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-15 09:44:24" id="13233" opendate="2016-03-08 19:18:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Use min and max values to estimate better stats for comparison operators</summary>
			
			
			<description>We should benefit from the min/max values for each column to calculate more precisely the number of rows produced by expressions with comparison operators</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-15 18:17:42" id="13226" opendate="2016-03-08 08:00:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Improve tez print summary to print query execution breakdown</summary>
			
			
			<description>When tez print summary is enabled, methods summary is printed which are difficult to correlate with the actual execution time. We can improve that to print  the execution times in the sequence of operations that happens behind the scenes.
Instead of printing the methods name it will be useful to print something like below
1) Query Compilation time
2) Query Submit to DAG Submit time
3) DAG Submit to DAG Accept time
4) DAG Accept to DAG Start time
5) DAG Start to DAG End time
With this it will be easier to find out where the actual time is spent. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13256</link>
			
			
			<link description="depends upon" type="dependent">12558</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-15 20:51:06" id="12995" opendate="2016-02-03 22:42:17" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Synthetic file ids need collision checks</summary>
			
			
			<description>LLAP synthetic file ids do not have any way of checking whether a collision occurs other than a data-error.
Synthetic file-ids have only been used with unit tests so far - but they will be needed to add cache mechanisms to non-HDFS filesystems.
In case of Synthetic file-ids, it is recommended that we track the full-tuple (path, mtime, len) in the cache so that a cache-hit for the synthetic file-id can be compared against the parameters &amp;amp; only accepted if those match.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.cache.Cache.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.io.DataCache.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcMetadataCache.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.cache.NoopCache.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
			
			
			<file type="M">org.apache.orc.FileMetadata.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.ql.io.orc.encoded.OrcCacheKey.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">13225</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-18 12:17:09" id="13242" opendate="2016-03-09 16:28:04" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>DISTINCT keyword is dropped by the parser for windowing</summary>
			
			
			<description>To reproduce, the following query can be used:

select distinct first_value(t) over ( partition by si order by i, b ) from over10k limit 100;



The distinct keyword is ignored and duplicates are produced.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14959</link>
			
			
			<link description="is related to" type="Reference">4662</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-21 17:56:36" id="13291" opendate="2016-03-16 03:32:56" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ORC BI Split strategy should consider block size instead of file size</summary>
			
			
			<description>When we force split strategy to use &quot;BI&quot; (using hive.exec.orc.split.strategy), entire file is considered as single split. This might be inefficient when the files are large. Instead, BI should consider splitting at block boundary. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-22 04:49:46" id="13327" opendate="2016-03-22 04:43:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>SessionID added to HS2 threadname does not trim spaces</summary>
			
			
			<description>HIVE-13153 introduced off-by-one in appending spaces to thread names. 
NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">13153</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-23 18:27:05" id="13246" opendate="2016-03-09 19:39:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add log line to ORC writer to print out the file path</summary>
			
			
			<description>Currently ORC writer does not log anything making it difficult find where the destination path is. 
NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.orc.impl.WriterImpl.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-25 01:53:31" id="13262" opendate="2016-03-11 00:57:54" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Remove log levels from DebugUtils</summary>
			
			
			<description>DebugUtils has many hardcoded log levels. To enable logging we need to recompile code with desired value. Instead configure add loggers for these classes with log levels via log4j properties. Also use parametrized logging in IO elevator. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.DebugUtils.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.old.ChunkPool.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.old.BufferPool.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.SimpleBufferManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.LlapDataBuffer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.old.BufferInProgress.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.old.CachePolicy.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.LogLevels.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-27 23:40:26" id="13256" opendate="2016-03-10 02:35:49" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>LLAP: RowGroup counter is wrong</summary>
			
			
			<description>Log line from LlapIOCounter



ROWS_EMITTED=23528469, SELECTED_ROWGROUPS=87



If rowgroups contain 10K rows by default then expected count is 235 for the above case.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13226</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-31 09:54:29" id="13255" opendate="2016-03-10 01:40:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>FloatTreeReader.nextVector is expensive </summary>
			
			
			<description>Some TPCDS queries on 1TB scale shows FloatTreeReader on profile samples. It is most likely because of multiple branching and polymorphic dispatch in FloatTreeReader.nextVector() implementation. See attached image for sampling profile output.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.orc.impl.RunLengthIntegerReader.java</file>
			
			
			<file type="M">org.apache.orc.impl.SerializationUtils.java</file>
			
			
			<file type="M">org.apache.orc.impl.TestSerializationUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			
			
			<file type="M">org.apache.orc.impl.RunLengthIntegerReaderV2.java</file>
			
			
			<file type="M">org.apache.orc.impl.IntegerReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-05 00:15:34" id="13396" opendate="2016-03-31 01:14:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Include hadoop-metrics2.properties file LlapServiceDriver</summary>
			
			
			<description>LlapServiceDriver should package hadoop-metrics2.properties file to support dumping llap daemon metrics to hadoop metric sinks. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.metrics.MetricsUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.metrics.LlapDaemonQueueMetrics.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-10 00:38:20" id="13434" opendate="2016-04-06 14:13:17" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>BaseSemanticAnalyzer.unescapeSQLString doesn&amp;apos;t unescape \u0000 style character literals.</summary>
			
			
			<description>BaseSemanticAnalyzer.unescapeSQLString method may have a fault. When &quot;\u0061&quot; style character literals are passed to the method, it&amp;amp;apos;s not unescaped successfully.
In Spark SQL project, we referenced the unescaping logic and noticed this issue (SPARK-14426)</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13412</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-14 21:22:43" id="13400" opendate="2016-03-31 13:17:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Following up HIVE-12481, add retry for Zookeeper service discovery</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14164</link>
			
			
			<link description="is related to" type="Reference">12481</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-15 11:03:48" id="13287" opendate="2016-03-15 09:56:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add logic to estimate stats for IN operator</summary>
			
			
			<description>Currently, IN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14018</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-21 20:50:09" id="13487" opendate="2016-04-12 01:12:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Finish time is wrong when perflog is missing SUBMIT_TO_RUNNING</summary>
			
			
			<description>Sometimes PerfLog misses SUBMIT_TO_RUNNING end time which makes finish time to be wrong. Like below



Compile Query                           0.60s

Prepare Plan                            0.44s

Submit Plan                             0.83s

Start                                   0.00s

Finish                              1460423234.44s



NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">14070</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-21 20:58:37" id="13488" opendate="2016-04-12 01:14:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Restore dag summary when tez exec print summary enabled and in-place updates disabled</summary>
			
			
			<description>Restore the old way of printing methods summary when file redirection is enabled. This may be used by some tools which will break because of the change introduced by HIVE-13226
NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-25 15:06:10" id="13533" opendate="2016-04-18 09:15:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Remove AST dump</summary>
			
			
			<description>For very large queries, dumping the AST can lead to OOM errors. Currently there are two places where we dump the AST:

CalcitePlanner if we are running in DEBUG mode (line 300).
ExplainTask if we use extended explain (line 179).

I guess the original reason to add the dump was to check whether the AST conversion from CBO was working properly, but I think we are past that stage now.
We will remove the logic to dump the AST in explain extended. For debug mode in CalcitePlanner, we will lower the level to LOG.TRACE.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ExplainWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-29 02:23:37" id="13572" opendate="2016-04-21 03:20:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Redundant setting full file status in Hive::copyFiles</summary>
			
			
			<description>We set full file status in each copy-file thread. I think it&amp;amp;apos;s redundant and hurts performance when we have multiple files to copy.



            if (inheritPerms) {

              ShimLoader.getHadoopShims().setFullFileStatus(conf, fullDestStatus, destFs, destf);

            }


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-29 10:38:14" id="13510" opendate="2016-04-13 23:20:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Dynamic partitioning doesnt work when remote metastore is used</summary>
			
			
			<description>Steps to reproduce:

Configure remote metastore (hive.metastore.uris)
Create table t1 (a string);
Create table t2 (a string) partitioned by (b string);
set hive.exec.dynamic.partition.mode=nonstrict;
Insert overwrite table t2 partition (b) select a,a from t1;

Result:

FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result

16/04/13 15:04:51 [c679e424-2501-4347-8146-cf1b1cae217c main]: ERROR ql.Driver: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result

org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result

        at org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.&amp;lt;init&amp;gt;(DynamicPartitionCtx.java:84)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6550)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9315)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:9204)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:10071)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9949)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10607)

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:358)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10618)

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:233)

        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:476)

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318)

        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1192)

        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1287)

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1118)

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1106)

        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:236)

        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187)

        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)

        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:339)

        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:748)

        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:721)

        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:648)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:606)

        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result

        at org.apache.hadoop.hive.ql.metadata.Hive.getMetaConf(Hive.java:3493)

        at org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.&amp;lt;init&amp;gt;(DynamicPartitionCtx.java:82)

        ... 29 more

Caused by: org.apache.thrift.TApplicationException: getMetaConf failed: unknown result

        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_getMetaConf(ThriftHiveMetastore.java:666)

        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.getMetaConf(ThriftHiveMetastore.java:646)

        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getMetaConf(HiveMetaStoreClient.java:550)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:606)

        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:153)

        at com.sun.proxy.$Proxy20.getMetaConf(Unknown Source)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:606)

        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2153)

        at com.sun.proxy.$Proxy20.getMetaConf(Unknown Source)

        at org.apache.hadoop.hive.ql.metadata.Hive.getMetaConf(Hive.java:3491)

        ... 30 more



During construction of DynamicPartitionCtx it tries to read hive.metastore.partition.name.whitelist.pattern from the metastore. If no value is configured in hive-site.xml then NULL will be returned. Thrift considers NULL as invalid result and fails with an exception.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12897</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-02 01:24:03" id="13485" opendate="2016-04-11 22:04:53" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Session id appended to thread name multiple times.</summary>
			
			
			<description>HIVE-13153 addressed a portion of this issue. Follow up from there.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13153</link>
			
			
			<link description="relates to" type="Reference">13885</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-02 19:12:51" id="13645" opendate="2016-04-28 19:42:56" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Beeline needs null-guard around hiveVars and hiveConfVars read</summary>
			
			
			<description>Beeline has a bug wherein if a user does a !save ever, then on next load, if beeline.hiveVariables or beeline.hiveconfvariables are empty, i.e. {} or unspecified, then it loads it as null, and then, on next connect, there is no null-check on these variables leading to an NPE.  </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.beeline.DatabaseConnection.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-05 06:11:42" id="12837" opendate="2016-01-11 18:45:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Better memory estimation/allocation for hybrid grace hash join during hash table loading</summary>
			
			
			<description>This is to avoid an edge case when the memory available is very little (less than a single write buffer size), and we start loading the hash table. Since the write buffer is lazily allocated, we will easily run out of memory before even checking if we should spill any hash partition.
e.g.
Total memory available: 210 MB
Size of ref array of BytesBytesMultiHashMap for each hash partition: ~16 MB
Size of write buffer: 8 MB (lazy allocation)
Number of hash partitions: 16
Number of hash partitions created in memory: 13
Number of hash partitions created on disk: 3
Available memory left after HybridHashTableContainer initialization: 210-16*13=2MB
Now let&amp;amp;apos;s say a row is to be loaded into a hash partition in memory, it will try to allocate an 8MB write buffer for it, but we only have 2MB, thus OOM.
Solution is to perform the check for possible spilling earlier so we can spill partitions if memory is about to be full, to avoid OOM.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">13730</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-06 02:44:15" id="13701" opendate="2016-05-06 00:52:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Use different prefix for llap task scheduler metrics</summary>
			
			
			<description>LLAP task scheduler runs inside AM and typically runs on different hosts than llap daemons. Using the same prefix &quot;llapdaemon&quot; for daemon metrics and task scheduler metrics will cause conflicts when these metrics are published with hadoop-metrics2.
NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.metrics.LlapTaskSchedulerMetrics.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-09 01:53:27" id="13525" opendate="2016-04-15 11:46:56" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HoS hangs when job is empty</summary>
			
			
			<description>Observed in local tests. This should be the cause of HIVE-13402.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.spark.client.RemoteDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13223</link>
			
			
			<link description="relates to" type="Reference">13402</link>
			
			
			<link description="relates to" type="Reference">13843</link>
			
			
			<link description="is related to" type="Reference">14958</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-09 18:32:26" id="13712" opendate="2016-05-07 05:36:53" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: LlapServiceDriver should package hadoop-metrics2-llapdaemon.properties when available</summary>
			
			
			<description>HIVE-13701 renamed hadoop-metrics2.properties to hadoop-metrics2-llapdaemon.properties to avoid conflicts in classpath lookup. MetricsSystem first looks for hadoop-metrics2-llapdaemon.properties file first before falling back to hadoop-metrics2.properties. Make LlapServiceDriver package hadoop-metrics2-llapdaemon.properties first and fallback to hadoop-metrics2.properties.
NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-09 23:21:39" id="13700" opendate="2016-05-05 22:18:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestHiveOperationType is failing on master</summary>
			
			
			<description>Presumably be broken by HIVE-13351</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-10 21:04:58" id="13342" opendate="2016-03-23 22:33:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Improve logging in llap decider and throw exception in case llap mode is all but we cannot run in llap.</summary>
			
			
			<description>Currently we do not log our decisions with respect to llap. Are we running everything in llap mode or only parts of the plan. We need more logging. Also, if llap mode is all but for some reason, we cannot run the work in llap mode, fail and throw an exception advise the user to change the mode to auto.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-11 23:55:04" id="13458" opendate="2016-04-07 21:51:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Heartbeater doesn&amp;apos;t fail query when heartbeat fails</summary>
			
			
			<description>When a heartbeat fails to locate a lock, it should fail the current query. That doesn&amp;amp;apos;t happen, which is a bug.
Another thing is, we need to make sure stopHeartbeat really stops the heartbeat, i.e. no additional heartbeat will be sent, since that will break the assumption and cause the query to fail.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12366</link>
			
			
			<link description="is related to" type="Reference">12634</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-13 15:49:27" id="13728" opendate="2016-05-10 16:35:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestHBaseSchemaTool fails on master</summary>
			
			
			<description>Presumably because of HIVE-13597</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-14 15:22:11" id="13743" opendate="2016-05-12 00:06:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Data move codepath is broken with hive (2.1.0-SNAPSHOT)</summary>
			
			
			<description>Data move codepath is broken with hive 2.1.0-SNAPSHOT with hadoop 2.8.0-snapshot.

Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path not found: /apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1

        at org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp.getEZForPath(FSDirEncryptionZoneOp.java:178)

        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZForPath(FSNamesystem.java:7336)

        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEZForPath(NameNodeRpcServer.java:1973)

        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEZForPath(ClientNamenodeProtocolServerSideTranslatorPB.java:1376)

        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)

        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:645)

        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)

        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2339)

        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2335)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:422)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)

        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2333)



        at org.apache.hadoop.ipc.Client.call(Client.java:1448)

        at org.apache.hadoop.ipc.Client.call(Client.java:1385)

        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)

        at com.sun.proxy.$Proxy30.getEZForPath(Unknown Source)/apps/hive/warehouse/tpcds_bin_partitioned_orc_200.db/



...

...

...

2016-05-11T09:40:43,760 ERROR [main]: ql.Driver (:()) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to move source hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/.hive-staging_hive_2016-05-11_09-40-42_489_5056654133706433454-1/-ext-10002 to destination hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1



https://github.com/apache/hive/blob/26b5c7b56a4f28ce3eabc0207566cce46b29b558/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2836
hdfsEncryptionShim.isPathEncrypted(destf) in Hive could end up throwing FileNotFoundException as the destf is not present yet.  This causes moveFile to fail.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-14 16:10:39" id="13686" opendate="2016-05-04 10:35:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestRecordReaderImpl is deleting target/tmp causing all the tests after it to fail</summary>
			
			
			<description>The issue was introduced in HIVE-12159 (https://github.com/apache/hive/blame/master/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java). This test deletes target/tmp directory. Because of this, the subsequent tests don&amp;amp;apos;t get to read target/tmp/conf/hive-site.xml, which contains test-specific configurations. Also, target/tmp has metastore db directory, which also gets deleted causing subsequent tests that use metastore db to fail too. 
I&amp;amp;apos;m surprised this issue wasn&amp;amp;apos;t caught in pre-commit builds. Sergio Pea I see that even the latest pre-commit jobs aren&amp;amp;apos;t reporting any errors, while building on local and running hive-exec test cases causes a bunch of tests to fail. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">12159</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-17 16:13:24" id="13767" opendate="2016-05-16 17:31:25" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Wrong type inferred in Semijoin condition leads to AssertionError</summary>
			
			
			<description>Following query fails to run:

SELECT

    COALESCE(498, LEAD(COALESCE(-973, -684, 515)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50), FLOOR(t1.double_col_16) DESC), 524) AS int_col,

    (t2.int_col_10) + (t1.smallint_col_50) AS int_col_1,

    FLOOR(t1.double_col_16) AS float_col,

    COALESCE(SUM(COALESCE(62, -380, -435)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50) DESC, FLOOR(t1.double_col_16) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 48 FOLLOWING), 704) AS int_col_2

FROM table_1 t1

INNER JOIN table_18 t2 ON (((t2.tinyint_col_15) = (t1.bigint_col_7)) AND

                           ((t2.decimal2709_col_9) = (t1.decimal2016_col_26))) AND

                           ((t2.tinyint_col_20) = (t1.tinyint_col_3))

WHERE (t2.smallint_col_19) IN (SELECT

    COALESCE(-92, -994) AS int_col

    FROM table_1 tt1

    INNER JOIN table_18 tt2 ON (tt2.decimal1911_col_16) = (tt1.decimal2612_col_77)

    WHERE (t1.timestamp_col_9) = (tt2.timestamp_col_18));



Following error is seen in the logs:

2016-04-27T04:32:09,605 WARN  [...2a24 HiveServer2-Handler-Pool: Thread-211]: thrift.ThriftCLIService (ThriftCLIService.java:ExecuteStatement(501)) - Error executing statement:

org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.AssertionError: mismatched type $8 TIMESTAMP(9)

        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:178) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:216) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:327) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:458) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:435) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:272) ~[hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:492) [hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317) [hive-service-rpc-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302) [hive-service-rpc-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) [hive-service-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]

        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]

Caused by: java.lang.AssertionError: mismatched type $8 TIMESTAMP(9)

        at org.apache.calcite.rex.RexUtil$FixNullabilityShuttle.visitInputRef(RexUtil.java:2042) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexUtil$FixNullabilityShuttle.visitInputRef(RexUtil.java:2020) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexShuttle.visitList(RexShuttle.java:144) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:93) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:36) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexCall.accept(RexCall.java:108) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:275) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexShuttle.mutate(RexShuttle.java:234) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexShuttle.apply(RexShuttle.java:252) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rex.RexUtil.fixUp(RexUtil.java:1239) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.rel.rules.FilterJoinRule.perform(FilterJoinRule.java:232) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule$HiveFilterJoinMergeRule.onMatch(HiveFilterJoinRule.java:78) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule$HiveFilterJoinMergeRule.onMatch(HiveFilterJoinRule.java:78) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:514) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:392) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:285) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:72) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.hepPlan(CalcitePlanner.java:1293) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPreJoinOrderingTransforms(CalcitePlanner.java:1166) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:956) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:887) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:113) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:969) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:149) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:106) ~[calcite-core-1.6.0.2.5.0.0-248.jar:1.6.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:706) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:274) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10642) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:233) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:476) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1191) ~[hive-exec-2.1.0.2.5.0.0-248.jar:2.1.0.2.5.0.0-248]

        ... 15 more



Hive DDL for supporting tables are:

CREATE TABLE table_1 (timestamp_col_1 TIMESTAMP, decimal3003_col_2 DECIMAL(30, 3), tinyint_col_3 TINYINT, decimal0101_col_4 DECIMAL(1, 1), boolean_col_5 BOOLEAN, float_col_6 FLOAT, bigint_col_7 BIGINT, varchar0098_col_8 VARCHAR(98), timestamp_col_9 TIMESTAMP, bigint_col_10 BIGINT, decimal0903_col_11 DECIMAL(9, 3), timestamp_col_12 TIMESTAMP, timestamp_col_13 TIMESTAMP, float_col_14 FLOAT, char0254_col_15 CHAR(254), double_col_16 DOUBLE, timestamp_col_17 TIMESTAMP, boolean_col_18 BOOLEAN, decimal2608_col_19 DECIMAL(26, 8), varchar0216_col_20 VARCHAR(216), string_col_21 STRING, bigint_col_22 BIGINT, boolean_col_23 BOOLEAN, timestamp_col_24 TIMESTAMP, boolean_col_25 BOOLEAN, decimal2016_col_26 DECIMAL(20, 16), string_col_27 STRING, decimal0202_col_28 DECIMAL(2, 2), float_col_29 FLOAT, decimal2020_col_30 DECIMAL(20, 20), boolean_col_31 BOOLEAN, double_col_32 DOUBLE, varchar0148_col_33 VARCHAR(148), decimal2121_col_34 DECIMAL(21, 21), tinyint_col_35 TINYINT, boolean_col_36 BOOLEAN, boolean_col_37 BOOLEAN, string_col_38 STRING, decimal3420_col_39 DECIMAL(34, 20), timestamp_col_40 TIMESTAMP, decimal1408_col_41 DECIMAL(14, 8), string_col_42 STRING, decimal0902_col_43 DECIMAL(9, 2), varchar0204_col_44 VARCHAR(204), boolean_col_45 BOOLEAN, timestamp_col_46 TIMESTAMP, boolean_col_47 BOOLEAN, bigint_col_48 BIGINT, boolean_col_49 BOOLEAN, smallint_col_50 SMALLINT, decimal0704_col_51 DECIMAL(7, 4), timestamp_col_52 TIMESTAMP, boolean_col_53 BOOLEAN, timestamp_col_54 TIMESTAMP, int_col_55 INT, decimal0505_col_56 DECIMAL(5, 5), char0155_col_57 CHAR(155), boolean_col_58 BOOLEAN, bigint_col_59 BIGINT, boolean_col_60 BOOLEAN, boolean_col_61 BOOLEAN, char0249_col_62 CHAR(249), boolean_col_63 BOOLEAN, timestamp_col_64 TIMESTAMP, decimal1309_col_65 DECIMAL(13, 9), int_col_66 INT, float_col_67 FLOAT, timestamp_col_68 TIMESTAMP, timestamp_col_69 TIMESTAMP, boolean_col_70 BOOLEAN, timestamp_col_71 TIMESTAMP, double_col_72 DOUBLE, boolean_col_73 BOOLEAN, char0222_col_74 CHAR(222), float_col_75 FLOAT, string_col_76 STRING, decimal2612_col_77 DECIMAL(26, 12), timestamp_col_78 TIMESTAMP, char0128_col_79 CHAR(128), timestamp_col_80 TIMESTAMP, double_col_81 DOUBLE, timestamp_col_82 TIMESTAMP, float_col_83 FLOAT, decimal2622_col_84 DECIMAL(26, 22), double_col_85 DOUBLE, float_col_86 FLOAT, decimal0907_col_87 DECIMAL(9, 7)) STORED AS orc;



CREATE TABLE table_18 (boolean_col_1 BOOLEAN, boolean_col_2 BOOLEAN, decimal2518_col_3 DECIMAL(25, 18), float_col_4 FLOAT, timestamp_col_5 TIMESTAMP, double_col_6 DOUBLE, double_col_7 DOUBLE, char0035_col_8 CHAR(35), decimal2709_col_9 DECIMAL(27, 9), int_col_10 INT, timestamp_col_11 TIMESTAMP, decimal3604_col_12 DECIMAL(36, 4), string_col_13 STRING, int_col_14 INT, tinyint_col_15 TINYINT, decimal1911_col_16 DECIMAL(19, 11), float_col_17 FLOAT, timestamp_col_18 TIMESTAMP, smallint_col_19 SMALLINT, tinyint_col_20 TINYINT, timestamp_col_21 TIMESTAMP, boolean_col_22 BOOLEAN, int_col_23 INT) STORED AS orc;



The problem is that the reference indices in the condition (and thus, their type) are inferred incorrectly.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-18 16:52:36" id="13730" opendate="2016-05-10 19:52:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Avoid double spilling the same partition when memory threshold is set very low</summary>
			
			
			<description>I am seeing hybridgrace_hashjoin_1.q getting stuck on master.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13755</link>
			
			
			<link description="relates to" type="Reference">12837</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-18 18:06:51" id="13343" opendate="2016-03-23 23:08:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Need to disable hybrid grace hash join in llap mode except for dynamically partitioned hash join</summary>
			
			
			<description>Due to performance reasons, we should disable use of hybrid grace hash join in llap when dynamic partition hash join is not used. With dynamic partition hash join, we need hybrid grace hash join due to the possibility of skews.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-19 11:13:49" id="11049" opendate="2015-06-18 23:31:20" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>With Clause should cache data &amp; reuse </summary>
			
			
			<description>Hive supports with clause. However Hive don&amp;amp;apos;t cache the result set of with clause and reuse it.
Instead we inline the query definition of the with clause. This results in re execution of with clause query every where it is referenced.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QueryPlan.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11752</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-19 15:49:36" id="13786" opendate="2016-05-18 21:21:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix the unit test failure org.apache.hive.service.cli.session.TestHiveSessionImpl.testLeakOperationHandle</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13702</link>
			
			
			<link description="is duplicated by" type="Duplicate">13791</link>
			
			
			<link description="is required by" type="Required">4924</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-19 16:06:08" id="13791" opendate="2016-05-19 12:40:49" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Fix  failure Unit Test TestHiveSessionImpl.testLeakOperationHandle</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Test</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13786</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-21 00:53:18" id="13702" opendate="2016-05-06 01:10:44" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>TestHiveSessionImpl fails on master</summary>
			
			
			<description>Presumably broken by HIVE-4924</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13786</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-21 00:53:20" id="13223" opendate="2016-03-08 00:13:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HoS  may hang for queries that run on 0 splits </summary>
			
			
			<description>Can be seen on all timed out tests after HIVE-13040 went in</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.spark.client.RemoteDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13525</link>
			
			
			<link description="is duplicated by" type="Duplicate">13355</link>
			
			
			<link description="is duplicated by" type="Duplicate">13247</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-23 17:51:28" id="13628" opendate="2016-04-27 17:17:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Support for permanent functions - error handling if no restart</summary>
			
			
			<description>Support for permanent functions - error handling if no restart</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-25 08:31:31" id="13832" opendate="2016-05-24 15:35:04" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add missing license header to files</summary>
			
			
			<description>Preparing to cut the branch for 2.1.0.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.stats.IExtrapolatePartStatus.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.model.MConstraint.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.TestTezWorkConcurrency.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.status.impl.SparkJobUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.ThriftFormatter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.AcidWriteSetService.java</file>
			
			
			<file type="M">org.apache.orc.impl.TestDataReaderProperties.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.util.JavaDataModelTest.java</file>
			
			
			<file type="M">org.apache.orc.impl.DataReaderProperties.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13438</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-25 21:59:56" id="13720" opendate="2016-05-09 17:31:53" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestLlapTaskCommunicator fails on master</summary>
			
			
			<description>Can be reproduced locally as well</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-26 00:13:43" id="13267" opendate="2016-03-11 09:50:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: Add SelectLikeStringColScalar for non-filter operations</summary>
			
			
			<description>FilterStringColLikeStringScalar only applies to the values within filter clauses.
Borrow the Checker impls and extend to the value generation - for cases like
select col is null or col like &amp;amp;apos;%(null)%&amp;amp;apos; </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFLike.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-26 01:25:11" id="13821" opendate="2016-05-23 18:28:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>OrcSplit groups all delta files together into a single split</summary>
			
			
			<description>HIVE-7428 had fix for worst case column projection size estimate. It was removed in HIVE-10397 to return file length but for ACID strategy file length is passed as 0. In worst case, this always return 0 and all files ends up in single split. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ColumnarSplitSizeEstimator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-26 09:11:07" id="13269" opendate="2016-03-11 20:31:12" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Simplify comparison expressions using column stats</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">13822</link>
			
			
			<link description="is blocked by" type="Blocker">13542</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-27 23:44:07" id="13841" opendate="2016-05-25 02:04:25" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Orc split generation returns different strategies with cache enabled vs disabled</summary>
			
			
			<description>Split strategy chosen by OrcInputFormat should not change when enabling or disabling footer cache. Currently if footer cache is disabled minSplits in OrcInputFormat.Context will be set to -1 which is used during determination of split strategies. minSplits should be set to requested value or some default instead of cache size</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-30 14:47:17" id="13861" opendate="2016-05-26 10:03:54" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix up nullability issue that might be created by pull up constants rules</summary>
			
			
			<description>When we pull up constants through Union or Sort operators, we might end up rewriting the original expression into an expression whose schema has different nullability properties for some of its columns.
This results in AssertionError of the following kind:

...

org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.AssertionError: Internal error: Cannot add expression of different type to set:

...


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveUnionPullUpConstantsRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is required by" type="Required">13807</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-30 23:08:42" id="13849" opendate="2016-05-25 16:58:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Wrong plan for hive.optimize.sort.dynamic.partition=true</summary>
			
			
			<description>To reproduce:

set hive.support.concurrency=true;

set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set hive.exec.dynamic.partition.mode=nonstrict;

set hive.optimize.sort.dynamic.partition=true;



CREATE TABLE non_acid(key string, value string) PARTITIONED BY(ds string, hr int) CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC;



explain insert into table non_acid partition(ds,hr) select * from srcpart sort by value;



CC&amp;amp;apos;ed Ashutosh Chauhan, Eugene Koifman</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-30 23:53:26" id="13818" opendate="2016-05-23 07:32:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fast Vector MapJoin Long hashtable has to handle all integral types</summary>
			
			
			<description>Changes for HIVE-13682 did fix a bug in Fast Hash Tables, but evidently not this issue according to Gopal/Rajesh/Nita.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is part of" type="Incorporates">13874</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-31 14:09:39" id="13863" opendate="2016-05-26 13:52:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Improve AnnotateWithStatistics with support for cartesian product</summary>
			
			
			<description>Currently cartesian product stats based on cardinality of inputs are not inferred correctly.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-31 14:19:54" id="13831" opendate="2016-05-24 10:58:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Error pushing predicates to HBase storage handler</summary>
			
			
			<description>Discovered while working on HIVE-13693.
There is an error on the predicates that we can push to HBaseStorageHandler. In particular, range predicates of the shape (bounded, open) and (open, bounded) over long or int columns get pushed and return wrong results.
The problem has to do with the storage order for keys in HBase. Keys are sorted lexicographically. Since the byte representation of negative values comes after the positive values, open range predicates need special handling that we do not have right now.
Thus, for instance, when we push the predicate key &amp;gt; 2, we return all records with column key greater than 2, plus the records with negative values for the column key. This problem does not get exposed if a filter is kept in the Hive operator tree, but we should not assume the latest.
This fix avoids pushing this kind of predicates to the storage handler, returning them in the residual part of the predicate that cannot be pushed. In the future, special handling might be added to support this kind of predicates.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveStoragePredicateHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.AbstractHBaseKeyFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.IndexSearchCondition.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">13693</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-31 18:42:36" id="13859" opendate="2016-05-26 05:04:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>mask() UDF not retaining day and month field values</summary>
			
			
			<description>For date type parameters, mask() UDF replaces year/month/day field values with the values given in arguments to the UDF. Argument value -1 is treated as special, to specify that mask() should retain the value in the parameter. This allows to selectively mask only year/month/day fields.
Specifying &quot;-1&quot; does not retain the values for day/month fields; however the year value is retained, as shown below.



0: jdbc:hive2://localhost:10000&amp;gt; select id, join_date from employee where id &amp;lt; 4;

+-----+-------------+--+

| id  |  join_date  |

+-----+-------------+--+

| 1   | 2012-01-01  |

| 2   | 2014-02-01  |

| 3   | 2013-03-01  |

+-----+-------------+--+

3 rows selected (0.435 seconds)

0: jdbc:hive2://localhost:10000&amp;gt; select id, mask(join_date, -1, -1, -1, -1,-1, -1,-1,-1) join_date from employee where id &amp;lt; 4;

+-----+-------------+--+

| id  |  join_date  |

+-----+-------------+--+

| 1   | 2012-01-01  |

| 2   | 2014-01-01  |

| 3   | 2013-01-01  |

+-----+-------------+--+

3 rows selected (0.344 seconds)


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFMask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13568</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-31 18:46:34" id="13719" opendate="2016-05-09 17:28:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestConverters fails on master</summary>
			
			
			<description>Can be reproduced locally also.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-31 18:50:58" id="13840" opendate="2016-05-25 01:23:53" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Orc split generation is reading file footers twice</summary>
			
			
			<description>Recent refactorings to move orc out introduced a regression in split generation. This leads to reading the orc file footers twice during split generation.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.orc.impl.ReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-31 20:56:04" id="13751" opendate="2016-05-12 19:35:30" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LlapOutputFormatService should have a configurable send buffer size</summary>
			
			
			<description>Netty channel buffer size is hard-coded 128KB now. It should be made configurable.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-01 02:45:05" id="13867" opendate="2016-05-26 22:22:14" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>restore HiveAuthorizer interface changes</summary>
			
			
			<description>TLDR: Some of the changes to hive authorizer interface made as part of HIVE-13360 are inappropriate and need to be restored.
Regarding the move of ip address from the query context object (HiveAuthzContext) to HiveAuthenticationProvider. That isn&amp;amp;apos;t the right place for it.
In HS2 HTTP mode, when proxies and knox servers are between end user and HS2 , every request for single session does not have to come via a single IP address.
Current assumption in hive code base is that the IP address is valid for the entire session. This might not hold true for ever.
A limitation in HS2 that it holds state for the session would currently force the user configure proxies and knox to remember which next Host it was using, because they need to have state to remember the HS2 instance to be used! But that is a limitation that ideally goes away some day, and when that happens, HiveAuthzContext would be the right place for keeping the IP address!</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerShowFilters.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.DummyAuthenticator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.ql.security.authorization.plugin.QueryContext.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.operation.MetadataOperation.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.processors.CommandUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TableMask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">13360</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-01 10:46:03" id="13876" opendate="2016-05-27 09:28:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: Port HIVE-11544 to LazySimpleDeserializeRead</summary>
			
			
			<description>High CPU usage due to exception handling code.



 TezTaskRunner [RUNNABLE] [DAEMON]

java.lang.Throwable.fillInStackTrace(int) Throwable.java (native)

java.lang.Throwable.fillInStackTrace() Throwable.java:783

java.lang.Throwable.&amp;lt;init&amp;gt;(String) Throwable.java:265

java.lang.Exception.&amp;lt;init&amp;gt;(String) Exception.java:66

java.lang.RuntimeException.&amp;lt;init&amp;gt;(String) RuntimeException.java:62

java.lang.IllegalArgumentException.&amp;lt;init&amp;gt;(String) IllegalArgumentException.java:52

java.lang.NumberFormatException.&amp;lt;init&amp;gt;(String) NumberFormatException.java:55

sun.misc.FloatingDecimal.readJavaFormatString(String) FloatingDecimal.java:1842

sun.misc.FloatingDecimal.parseFloat(String) FloatingDecimal.java:122

java.lang.Float.parseFloat(String) Float.java:451

org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.readCheckNull() LazySimpleDeserializeRead.java:309

org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeRowColumn(VectorizedRowBatch, int, int) VectorDeserializeRow.java:346

org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorizedRowBatch, int) VectorDeserializeRow.java:659

org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(Writable) VectorMapOperator.java:814

org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(Object) MapRecordSource.java:86

org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord() MapRecordSource.java:70

org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run() MapRecordProcessor.java:361

org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(Map, Map) TezProcessor.java:172

org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(Map, Map) TezProcessor.java:160

org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run() LogicalIOProcessorRuntimeTask.java:370

org.apache.tez.runtime.task.TaskRunner2Callable$1.run() TaskRunner2Callable.java:73

org.apache.tez.runtime.task.TaskRunner2Callable$1.run() TaskRunner2Callable.java:61

java.security.AccessController.doPrivileged(PrivilegedExceptionAction, AccessControlContext) AccessController.java (native)

javax.security.auth.Subject.doAs(Subject, PrivilegedExceptionAction) Subject.java:422

org.apache.hadoop.security.UserGroupInformation.doAs(PrivilegedExceptionAction) UserGroupInformation.java:1657

org.apache.tez.runtime.task.TaskRunner2Callable.callInternal() TaskRunner2Callable.java:61

org.apache.tez.runtime.task.TaskRunner2Callable.callInternal() TaskRunner2Callable.java:37

org.apache.tez.common.CallableWithNdc.call() CallableWithNdc.java:36

java.util.concurrent.FutureTask.run() FutureTask.java:266

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor$Worker) ThreadPoolExecutor.java:1142

java.util.concurrent.ThreadPoolExecutor$Worker.run() ThreadPoolExecutor.java:617

java.lang.Thread.run() Thread.java:745




</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is part of" type="Incorporates">13878</link>
			
			
			<link description="is required by" type="Required">13887</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-02 17:24:57" id="13927" opendate="2016-06-02 17:12:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Adding missing header to Java files</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCliServiceMessageSizeTest.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-03 10:16:45" id="13924" opendate="2016-06-02 06:53:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>(Vectorization) Error evaluating ((bool0 and (not bool1)) or (bool1 and (not bool0)))</summary>
			
			
			<description>Scratch column(s) in child expressions shouldn&amp;amp;apos;t be returned to the pool for PROJECTION.
Problem introduced with HIVE-13084.
Symptom:



Caused by: java.lang.IllegalStateException

at com.google.common.base.Preconditions.checkState(Preconditions.java:133)

        at org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.evaluate(ColOrCol.java:544)


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0, 2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14182</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-03 17:41:16" id="13882" opendate="2016-05-27 20:38:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>When hive.server2.async.exec.async.compile is turned on, from JDBC we will get &quot;The query did not generate a result set&quot; </summary>
			
			
			<description> The following would fail with  &quot;The query did not generate a result set&quot;
    stmt.execute(&quot;SET hive.driver.parallel.compilation=true&quot;);
    stmt.execute(&quot;SET hive.server2.async.exec.async.compile=true&quot;);
    ResultSet res =  stmt.executeQuery(&quot;SELECT * FROM &quot; + tableName);
    res.next();</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.OperationStatus.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
			
			
			<file type="M">org.apache.hive.service.rpc.thrift.TGetOperationStatusResp.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">4239</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-07 03:38:12" id="13599" opendate="2016-04-24 05:06:05" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Incorrect handling of the preemption queue on finishable state updates</summary>
			
			
			<description>When running some tests with pre-emption enabled, got the following exception
Looks like a race condition when removing items from pre-emption queue.



16/04/23 23:32:00 [Wait-Queue-Scheduler-0[]] ERROR impl.TaskExecutorService : Wait queue scheduler worker exited with failure!

java.util.NoSuchElementException

        at java.util.AbstractQueue.remove(AbstractQueue.java:117) ~[?:1.7.0_55]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.removeAndGetFromPreemptionQueue(TaskExecutorService.java:568) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.handleScheduleAttemptedRejection(TaskExecutorService.java:493) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1100(TaskExecutorService.java:81) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$WaitQueueWorker.run(TaskExecutorService.java:285) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[?:1.7.0_55]

        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_55]

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_55]

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_55]

        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_55]

16/04/23 23:32:00 [Wait-Queue-Scheduler-0[]] INFO impl.LlapDaemon : UncaughtExceptionHandler invoked

16/04/23 23:32:00 [Wait-Queue-Scheduler-0[]] ERROR impl.LlapDaemon : Thread Thread[Wait-Queue-Scheduler-0,5,main] threw an Exception. Shutting down now...

java.util.NoSuchElementException

        at java.util.AbstractQueue.remove(AbstractQueue.java:117) ~[?:1.7.0_55]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.removeAndGetFromPreemptionQueue(TaskExecutorService.java:568) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.handleScheduleAttemptedRejection(TaskExecutorService.java:493) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1100(TaskExecutorService.java:81) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$WaitQueueWorker.run(TaskExecutorService.java:285) ~[hive-llap-server-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[?:1.7.0_55]

        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [?:1.7.0_55]

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_55]

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_55]

        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_55]


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-07 15:32:08" id="13911" opendate="2016-06-01 23:57:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>load inpath fails throwing org.apache.hadoop.security.AccessControlException</summary>
			
			
			<description>Similar to HIVE-13857</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-09 09:30:02" id="13973" opendate="2016-06-08 14:37:12" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Extend support for other primitive types in windowing expressions</summary>
			
			
			<description>Following windowing query using boolean column in partitioning clause



create table all100k(t tinyint, si smallint, i int,

    b bigint, f float, d double, s string,

    dc decimal(38,18), bo boolean, v varchar(25),

    c char(25), ts timestamp, dt date);

select  rank() over (partition by i order by bo  nulls first, b nulls last range between unbounded preceding and current row),

    row_number()  over (partition by bo order by si desc, b nulls last range between unbounded preceding and unbounded following) as fv

from all100k order by fv;



fails with the following error:

FAILED: SemanticException Failed to breakup Windowing invocations into Groups. At least 1 group must only depend on input columns. Also check for circular dependencies.

Underlying error: Primitve type BOOLEAN not supported in Value Boundary expression


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.PTFTranslator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-09 18:27:49" id="13563" opendate="2016-04-20 22:39:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive Streaming does not honor orc.compress.size and orc.stripe.size table properties</summary>
			
			
			<description>According to the doc:
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-HiveQLSyntax
One should be able to specify tblproperties for many ORC options.
But the settings for orc.compress.size and orc.stripe.size don&amp;amp;apos;t take effect.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.java</file>
			
			
			<file type="M">org.apache.orc.OrcConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12498</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-19 19:30:42" id="14003" opendate="2016-06-13 15:13:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>queries running against llap hang at times - preemption issues</summary>
			
			
			<description>The preemption logic in the Hive processor needs some more work. There are definitely windows where the abort flag is completely dropped within the Hive processor.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapredContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-20 15:37:53" id="14059" opendate="2016-06-20 15:02:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Missing license headers for two files</summary>
			
			
			<description>As noted by Sushanth Sowmyan, two files are missing the Apache headers:

/Users/sush/t/rel/apache-hive-2.1.0-src/common/src/java/org/apache/hive/common/util/DateParser.java

/Users/sush/t/rel/apache-hive-2.1.0-src/common/src/test/org/apache/hive/common/util/TestDateParser.java


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.common.util.DateParser.java</file>
			
			
			<file type="M">org.apache.hive.common.util.TestDateParser.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-22 02:46:41" id="14060" opendate="2016-06-20 21:18:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive: Remove bogus &quot;localhost&quot; from Hive splits</summary>
			
			
			<description>On remote filesystems like Azure, GCP and S3, the splits contain a filler location of &quot;localhost&quot;.
This is worse than having no location information at all - on large clusters yarn waits upto 200[1] seconds for heartbeat from &quot;localhost&quot; before allocating a container.
To speed up this process, the split affinity provider should scrub the bogus &quot;localhost&quot; from the locations and allow for the allocation of &quot;*&quot; containers instead on each heartbeat.
[1] - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-22 14:46:18" id="14062" opendate="2016-06-20 22:03:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Changes from HIVE-13502 overwritten by HIVE-13566</summary>
			
			
			<description>Appears that changes from HIVE-13566 overwrote the changes from HIVE-13502. I will confirm with the author that it was inadvertent before I re-add it. Thanks</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.beeline.Commands.java</file>
			
			
			<file type="M">org.apache.hive.beeline.BeeLine.java</file>
			
			
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-23 10:42:40" id="13872" opendate="2016-05-27 03:57:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: Fix cross-product reduce sink serialization</summary>
			
			
			<description>TPC-DS Q13 produces a cross-product without CBO simplifying the query



Caused by: java.lang.RuntimeException: null STRING entry: batchIndex 0 projection column num 1

        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.nullBytesReadError(VectorExtractRow.java:349)

        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.extractRowColumn(VectorExtractRow.java:267)

        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.extractRow(VectorExtractRow.java:343)

        at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:103)

        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)

        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)

        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)

        ... 18 more



Simplified query



set hive.cbo.enable=false;



-- explain



select count(1)  

 from store_sales

     ,customer_demographics

 where (

( 

  customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk

  and customer_demographics.cd_marital_status = &amp;amp;apos;M&amp;amp;apos;

     )or

     (

   customer_demographics.cd_demo_sk = ss_cdemo_sk

  and customer_demographics.cd_marital_status = &amp;amp;apos;U&amp;amp;apos;

     ))

;






        Map 3 

            Map Operator Tree:

                TableScan

                  alias: customer_demographics

                  Statistics: Num rows: 1920800 Data size: 717255532 Basic stats: COMPLETE Column stats: NONE

                  Reduce Output Operator

                    sort order: 

                    Statistics: Num rows: 1920800 Data size: 717255532 Basic stats: COMPLETE Column stats: NONE

                    value expressions: cd_demo_sk (type: int), cd_marital_status (type: string)

            Execution mode: vectorized, llap

            LLAP IO: all inputs


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.NullRowsInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorAssignRow.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			
			
			<file type="M">org.apache.orc.impl.TreeReaderFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-27 15:54:01" id="13997" opendate="2016-06-12 02:39:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Insert overwrite directory doesn&amp;apos;t overwrite existing files</summary>
			
			
			<description>Can be easily reproduced by running INSERT OVERWRITE DIRECTORY to the same dir twice.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">13918</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-28 02:09:13" id="14073" opendate="2016-06-21 22:35:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>update config whiltelist for sql std authorization </summary>
			
			
			<description>New configs that should go in security whitelist have been added. Whitelist needs updating.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">14900</link>
			
			
			<link description="is related to" type="Reference">10678</link>
			
			
			<link description="is related to" type="Reference">2355</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-01 17:44:53" id="14126" opendate="2016-06-29 17:59:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>With ranger enabled, partitioned columns is returned first when you execute select star</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-02 06:31:36" id="14122" opendate="2016-06-29 07:50:40" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>VectorMapOperator: Missing update to AbstractMapOperator::numRows</summary>
			
			
			<description>The INPUT_RECORDS counter is out of sync with the actual # of rows-read in vectorized and non-vectorized modes.
This means Tez record summaries are off by a large margin or is 0 for those vertices.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13871</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-07 19:50:31" id="13945" opendate="2016-06-03 23:59:56" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Decimal value is displayed as rounded when selecting where clause with that decimal value.</summary>
			
			
			<description>Create a table withe a column of decimal type(38,18) and insert &amp;amp;apos;4327269606205.029297&amp;amp;apos;. Then select with that value displays its rounded value, which is 4327269606205.029300000000000000

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; drop table if exists test;

No rows affected (0.229 seconds)

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; create table test (dc decimal(38,18));

No rows affected (0.125 seconds)

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; insert into table test values (4327269606205.029297);

No rows affected (2.372 seconds)

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; select * from test;

+-----------------------------------+--+

|              test.dc              |

+-----------------------------------+--+

| 4327269606205.029297000000000000  |

+-----------------------------------+--+

1 row selected (0.123 seconds)

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt;

0: jdbc:hive2://os-r7-mvjkcu-hiveserver2-11-4&amp;gt; select * from test where dc = 4327269606205.029297000000000000;

+-----------------------------------+--+

|              test.dc              |

+-----------------------------------+--+

| 4327269606205.029300000000000000  |

+-----------------------------------+--+

1 row selected (0.109 seconds)


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
			
			
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			
			
			<file type="M">org.apache.orc.impl.RecordReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13946</link>
			
			
			<link description="relates to" type="Reference">14077</link>
			
			
			<link description="is related to" type="Reference">14134</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-08 21:05:46" id="14197" opendate="2016-07-08 17:45:53" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP service driver precondition failure should include the values</summary>
			
			
			<description>LLAP service driver&amp;amp;apos;s precondition failure message are like below



Working memory + cache has to be smaller than the container sizing



It will be better to include the actual values for the sizes in the precondition failure message.
NO PRECOMMIT TESTS</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-09 10:31:52" id="14176" opendate="2016-07-06 22:45:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO nesting windowing function within each other when merging Project operators</summary>
			
			
			<description>The translation into a physical plan does not support this way of expressing windowing functions. Instead, we will not merge the Project operators when we find this pattern.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-10 17:22:56" id="14115" opendate="2016-06-28 05:52:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Custom FetchFormatter is not supported</summary>
			
			
			<description>The following code is supported only FetchFormatter of ThriftFormatter and DefaultFetchFormatter. It can not be used Custom FetchFormatter.



        if (SessionState.get().isHiveServerQuery()) {

          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER,ThriftFormatter.class.getName());

        } else {

          conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, DefaultFetchFormatter.class.getName());

        }


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-10 18:12:57" id="13887" opendate="2016-05-29 05:34:19" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LazySimpleSerDe should parse &quot;NULL&quot; dates faster</summary>
			
			
			<description>Date string which contain &quot;NULL&quot; or &quot;(null)&quot; are being parsed through a very slow codepath involving exception handling as a normal codepath.
These are currently ~4x slower than parsing an actual date field.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.java</file>
			
			
			<file type="M">org.apache.hive.benchmark.serde.LazySimpleSerDeBench.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyDate.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is part of" type="Incorporates">13878</link>
			
			
			<link description="requires" type="Required">13876</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-11 18:57:00" id="14141" opendate="2016-06-30 21:45:44" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix for HIVE-14062 breaks indirect urls in beeline</summary>
			
			
			<description>Looks like the patch for HIVE-14062 breaks indirect urls which uses environment variables to get the url in beeline
In order to reproduce this issue:

$ export BEELINE_URL_DEFAULT=&quot;jdbc:hive2://localhost:10000&quot;

$ beeline -u default


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.beeline.Commands.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-13 16:53:03" id="13258" opendate="2016-03-10 02:50:44" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Add hdfs bytes read and spilled bytes to tez print summary</summary>
			
			
			<description>When printing counters to console it will be useful to print hdfs bytes read and spilled bytes which will help with debugging issues faster. </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.counters.QueryFragmentCounters.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.LlapUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.counters.LlapIOCounters.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">3250</link>
			
			
			<link description="is related to" type="Reference">3290</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-14 23:11:04" id="13871" opendate="2016-05-27 02:01:42" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Tez exec summary does not get the HIVE counters right</summary>
			
			
			<description>


2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) - HIVE:

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    CREATED_FILES: 1

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    DESERIALIZE_ERRORS: 0

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_1: 0

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_4: 0

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_5: 0

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_6: 0

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_IN_Map_7: 0

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_0: 10

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_1: 418284

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_4: 27440

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_5: 365

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_6: 101

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Map_7: 48000

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    RECORDS_OUT_INTERMEDIATE_Reducer_2: 10

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) - Shuffle Errors:



However, the actual operator counters do indicate the total # of vectors.



2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) - TaskCounter_Map_1_INPUT_Map_4:

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    FIRST_EVENT_RECEIVED: 1

2016-05-26T21:59:51,421 INFO  [main]: exec.Task (:()) -    INPUT_RECORDS_PROCESSED: 27440


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14122</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-15 08:43:01" id="14144" opendate="2016-07-01 07:20:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Permanent functions are showing up in show functions, but describe says it doesn&amp;apos;t exist</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.util.ResourceDownloader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">13903</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-18 20:51:35" id="14258" opendate="2016-07-16 05:06:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Reduce task timed out because CommonJoinOperator.genUniqueJoinObject took too long to finish without reporting progress</summary>
			
			
			<description>Reduce task timed out because CommonJoinOperator.genUniqueJoinObject took too long to finish without reporting progress.
This timeout happened when reducer.close() is called in ReduceTask.java.
CommonJoinOperator.genUniqueJoinObject() called by reducer.close() will loop over every row in the AbstractRowContainer. This can take a long time if there are a large number or rows, and during this time, it does not report progress. If this runs for long enough more than &quot;mapreduce.task.timeout&quot;, ApplicationMaster will kill the task for failing to report progress.
we configured &quot;mapreduce.task.timeout&quot; as 10 minutes. I captured the stack trace in the 10 minutes before AM killed the reduce task at 2016-07-15 07:19:11.
The following three stack traces can prove it:
at 2016-07-15 07:09:42:



&quot;main&quot; prio=10 tid=0x00007f90ec017000 nid=0xd193 runnable [0x00007f90f62e5000]

   java.lang.Thread.State: RUNNABLE

        at java.io.FileInputStream.readBytes(Native Method)

        at java.io.FileInputStream.read(FileInputStream.java:272)

        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:154)

        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)

        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)

        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)

        - locked &amp;lt;0x00000007deecefb0&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)

        at java.io.DataInputStream.read(DataInputStream.java:149)

        at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)

        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:252)

        at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)

        at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:214)

        at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:232)

        at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)

        - locked &amp;lt;0x00000007deecb978&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)

        at java.io.DataInputStream.readFully(DataInputStream.java:195)

        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)

        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)

        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2359)

        - locked &amp;lt;0x00000007deec8f70&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)

        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2491)

        - locked &amp;lt;0x00000007deec8f70&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)

        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)

        - locked &amp;lt;0x00000007deec82f0&amp;gt; (a org.apache.hadoop.mapred.SequenceFileRecordReader)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:360)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:267)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:74)

        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:644)

        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:750)

        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)

        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)

        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)

        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)

        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:415)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)



at 2016-07-15 07:15:35



&quot;main&quot; prio=10 tid=0x00007f90ec017000 nid=0xd193 runnable [0x00007f90f62e5000]

   java.lang.Thread.State: RUNNABLE

        at java.util.zip.CRC32.updateBytes(Native Method)

        at java.util.zip.CRC32.update(CRC32.java:65)

        at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:316)

        at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:279)

        at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:214)

        at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:232)

        at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)

        - locked &amp;lt;0x00000007d68db510&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)

        at java.io.DataInputStream.readFully(DataInputStream.java:195)

        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)

        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)

        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2359)

        - locked &amp;lt;0x00000007d68d8b68&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)

        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2491)

        - locked &amp;lt;0x00000007d68d8b68&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)

        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)

        - locked &amp;lt;0x00000007d68d7f08&amp;gt; (a org.apache.hadoop.mapred.SequenceFileRecordReader)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:360)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:267)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:74)

        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:644)

        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:750)

        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)

        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)

        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)

        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)

        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:415)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)

        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)



at 2016-07-15 07:19:10



&quot;main&quot; prio=10 tid=0x00007f90ec017000 nid=0xd193 runnable [0x00007f90f62e5000]

   java.lang.Thread.State: RUNNABLE

        at java.io.FileInputStream.readBytes(Native Method)

        at java.io.FileInputStream.read(FileInputStream.java:272)

        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:154)

        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)

        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)

        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)

        - locked &amp;lt;0x00000007df731218&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)

        at java.io.DataInputStream.read(DataInputStream.java:149)

        at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)

        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:252)

        at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)

        at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:214)

        at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:232)

        at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)

        - locked &amp;lt;0x00000007df72dc20&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)

        at java.io.DataInputStream.readFully(DataInputStream.java:195)

        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)

        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)

        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2359)

        - locked &amp;lt;0x00000007df72b278&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)

        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2491)

        - locked &amp;lt;0x00000007df72b278&amp;gt; (a org.apache.hadoop.io.SequenceFile$Reader)

        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)

        - locked &amp;lt;0x00000007df72a618&amp;gt; (a org.apache.hadoop.mapred.SequenceFileRecordReader)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:360)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(RowContainer.java:373)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:267)

        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next(RowContainer.java:74)

        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:644)

        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:750)

        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)

        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)

        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)

        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)

        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:415)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)

        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)



You can see all three stack traces show CommonJoinOperator.genUniqueJoinObject was called by ExecReducer.close.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-19 16:39:23" id="14254" opendate="2016-07-15 21:05:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Correct the hive version by changing &quot;svn&quot; to &quot;git&quot;</summary>
			
			
			<description>When running &quot;hive --version&quot;, &quot;subversion&quot; is displayed below, which should be &quot;git&quot;.
$ hive --version
Hive 2.1.0-SNAPSHOT
Subversion git://</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.common.util.HiveVersionInfo.java</file>
			
			
			<file type="M">org.apache.hive.common.HiveVersionAnnotation.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-22 20:33:44" id="13934" opendate="2016-06-02 23:45:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Configure Tez to make nocondiional task size memory available for the Processor</summary>
			
			
			<description>Currently, noconditionaltasksize is not validated against the container size, the reservations made in the container by Tez for Inputs / Outputs etc.
Check this at compile time to see if enough memory is available, or set up the vertex to reserve additional memory for the Processor.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">13809</link>
			
			
			<link description="is blocked by" type="Blocker">3286</link>
			
			
			<link description="relates to" type="Reference">3353</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-25 16:39:24" id="14308" opendate="2016-07-21 15:47:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>While using column stats estimated data size may become 0</summary>
			
			
			<description>Found during a run of HIVE-12181</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-25 23:18:41" id="14326" opendate="2016-07-25 13:04:25" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Merging outer joins without conditions can lead to wrong results</summary>
			
			
			<description>HIVE-13069 enabled cartesian product merging. However, merge should only be performed between INNER joins.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-27 22:08:45" id="14293" opendate="2016-07-19 23:56:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>PerfLogger.openScopes should be transient</summary>
			
			
			<description>See the following exception when running Hive e2e tests:



0: jdbc:hive2://nat-r6-ojss-hsihs2-1.openstac&amp;gt; SELECT s.name, s2.age, s.gpa, v.registration, v2.contributions FROM student s INNER JOIN voter v ON (s.name = v.name) INNER JOIN student s2 ON (s2.age = v.age and s.name = s2.name) INNER JOIN voter v2 ON (v2.name = s2.name and v2.age = s2.age) WHERE v2.age = s.age ORDER BY s.name, s2.age, s.gpa, v.registration, v2.contributions;

INFO  : Compiling command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8): SELECT s.name, s2.age, s.gpa, v.registration, v2.contributions FROM student s INNER JOIN voter v ON (s.name = v.name) INNER JOIN student s2 ON (s2.age = v.age and s.name = s2.name) INNER JOIN voter v2 ON (v2.name = s2.name and v2.age = s2.age) WHERE v2.age = s.age ORDER BY s.name, s2.age, s.gpa, v.registration, v2.contributions

INFO  : Semantic Analysis Completed

INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:s.name, type:string, comment:null), FieldSchema(name:s2.age, type:int, comment:null), FieldSchema(name:s.gpa, type:double, comment:null), FieldSchema(name:v.registration, type:string, comment:null), FieldSchema(name:v2.contributions, type:float, comment:null)], properties:null)

INFO  : Completed compiling command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8); Time taken: 1.165 seconds

INFO  : Executing command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8): SELECT s.name, s2.age, s.gpa, v.registration, v2.contributions FROM student s INNER JOIN voter v ON (s.name = v.name) INNER JOIN student s2 ON (s2.age = v.age and s.name = s2.name) INNER JOIN voter v2 ON (v2.name = s2.name and v2.age = s2.age) WHERE v2.age = s.age ORDER BY s.name, s2.age, s.gpa, v.registration, v2.contributions

INFO  : Query ID = hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8

INFO  : Total jobs = 1

INFO  : Launching Job 1 out of 1

INFO  : Starting task [Stage-1:MAPRED] in serial mode

INFO  : Session is already open

INFO  : Dag name: SELECT s.name, s2.age, s....v2.contributions(Stage-1)

ERROR : Failed to execute tez graph.

java.lang.RuntimeException: Error caching map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException

Serialization trace:

classes (sun.misc.Launcher$AppClassLoader)

classloader (java.security.ProtectionDomain)

context (java.security.AccessControlContext)

acc (org.apache.hadoop.hive.ql.exec.UDFClassLoader)

classLoader (org.apache.hadoop.hive.conf.HiveConf)

conf (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics)

metrics (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$CodahaleMetricsScope)

openScopes (org.apache.hadoop.hive.ql.log.PerfLogger)

perfLogger (org.apache.hadoop.hive.ql.exec.MapJoinOperator)

childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)

childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)

childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)

childOperators (org.apache.hadoop.hive.ql.exec.FilterOperator)

childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)

aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork)

	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:582) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Utilities.java:516) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:601) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1147) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:390) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:164) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1865) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1569) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1321) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1090) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083) [hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_77]

	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_77]

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) [hadoop-common-2.7.1.2.5.0.0-1009.jar:?]

	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347) [hive-service-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]

	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]

	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]

	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]

Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException

Serialization trace:

classes (sun.misc.Launcher$AppClassLoader)

classloader (java.security.ProtectionDomain)

context (java.security.AccessControlContext)

acc (org.apache.hadoop.hive.ql.exec.UDFClassLoader)

classLoader (org.apache.hadoop.hive.conf.HiveConf)

conf (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics)

metrics (org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$CodahaleMetricsScope)

openScopes (org.apache.hadoop.hive.ql.log.PerfLogger)

perfLogger (org.apache.hadoop.hive.ql.exec.MapJoinOperator)

childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)

childOperators (org.apache.hadoop.hive.ql.exec.MapJoinOperator)

childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)

childOperators (org.apache.hadoop.hive.ql.exec.FilterOperator)

childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)

aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork)

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:87) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializeObjectByKryo(SerializationUtilities.java:578) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:454) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:435) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:537) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	... 26 more

Caused by: java.util.ConcurrentModificationException

	at java.util.Vector$Itr.checkForComodification(Vector.java:1184) ~[?:1.8.0_77]

	at java.util.Vector$Itr.next(Vector.java:1137) ~[?:1.8.0_77]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:87) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:113) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializeObjectByKryo(SerializationUtilities.java:578) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:454) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.SerializationUtilities.serializePlan(SerializationUtilities.java:435) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:537) ~[hive-exec-2.1.0.2.5.0.0-1009.jar:2.1.0.2.5.0.0-1009]

	... 26 more

ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask

INFO  : Completed executing command(queryId=hive_20160717224915_3a52719f-539f-4f82-a9cd-0c0af4e09ef8); Time taken: 0.171 seconds

Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask (state=08S01,code=1)



The reason is openScopes shall not be serialized to the backend. Need to mark it transient.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14232</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-28 00:00:41" id="14338" opendate="2016-07-26 17:19:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Delete/Alter table calls failing with HiveAccessControlException</summary>
			
			
			<description>Many Hcatalog/Webhcat tests are failing with below error, when tests try to alter/delete/describe tables. Error is thrown when the same user or a different user (same group) who created the table is trying to run the delete/alter table call.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.cli.HCatCli.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14221</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-28 06:51:17" id="14333" opendate="2016-07-26 01:29:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ORC schema evolution from float to double changes precision and breaks filters</summary>
			
			
			<description>ORC vs text schema evolution from float to double changes precision
Text Schema Evolution


hive&amp;gt; create table float_text(f float);

hive&amp;gt; insert into float_text values(74.72);

hive&amp;gt; select f from float_text;

OK

74.72

hive&amp;gt; alter table float_text change column f f double;

hive&amp;gt; select f from float_text;

OK

74.72



Orc Schema Evolution


hive&amp;gt; create table float_orc(f float) stored as orc;

hive&amp;gt; insert into float_orc values(74.72);

hive&amp;gt; select f from float_orc;

OK

74.72

hive&amp;gt; alter table float_orc change column f f double;

hive&amp;gt; select f from float_orc;

OK

74.72000122070312



This will break all filters on the evolved column &quot;f&quot;
Filter returning no results


hive&amp;gt; set hive.optimize.index.filter=false;

hive&amp;gt; select f from float_orc where f=74.72;

OK


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.orc.impl.TestSchemaEvolution.java</file>
			
			
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">14310</link>
			
			
			<link description="relates to" type="Reference">14214</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-28 19:07:55" id="14310" opendate="2016-07-21 22:52:17" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ORC schema evolution should not completely disable PPD</summary>
			
			
			<description>Follow up for HIVE-14214 which completely shuts off PPD when there is any schema evolution. Some evolutions are safer for PPD like
byte -&amp;gt; short -&amp;gt; int -&amp;gt; long
float -&amp;gt; double (This is unsafe, see comments in SchemaEvolution.java in the patch)
varchar &amp;lt;-&amp;gt; string (string to char, varchar to char and vice versa is also unsafe conversion as Orc stores internal index with padded spaces for char)
For all other conversions we can disable PPD for that specific column that has evolved by returning TruthValue.YES_NO</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
			
			<file type="M">org.apache.orc.impl.SchemaEvolution.java</file>
			
			
			<file type="M">org.apache.orc.impl.TestSchemaEvolution.java</file>
			
			
			<file type="M">org.apache.orc.impl.RecordReaderImpl.java</file>
			
			
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">14355</link>
			
			
			<link description="is blocked by" type="Blocker">14324</link>
			
			
			<link description="is blocked by" type="Blocker">14333</link>
			
			
			<link description="is related to" type="Reference">14214</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-31 03:55:42" id="14355" opendate="2016-07-27 09:03:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Schema evolution for ORC in llap is broken for int to string conversion</summary>
			
			
			<description>When schema is evolved from any integer type to string then following exceptions are thrown in LLAP (Works fine in Tez). I guess this should happen even for other conversions.



hive&amp;gt; create table orc_integer(b bigint) stored as orc;

hive&amp;gt; insert into orc_integer values(100);

hive&amp;gt; select count(*) from orc_integer where b=100;

OK

1

hive&amp;gt; alter table orc_integer change column b b string;

hive&amp;gt; select count(*) from orc_integer where b=100;

// FAIL with following exception



When vectorization is enabled


2016-07-27T01:48:05,611  INFO [TezTaskRunner ()] vector.VectorReduceSinkOperator: RECORDS_OUT_INTERMEDIATE_Map_1:0,

2016-07-27T01:48:05,611 ERROR [TezTaskRunner ()] tez.TezProcessor: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)

        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)

        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)

        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)

        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)

        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:422)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)

        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)

        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)

        at java.util.concurrent.FutureTask.run(FutureTask.java:266)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

        at java.lang.Thread.run(Thread.java:745)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row

        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:866)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)

        ... 18 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

        at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringGroupColEqualStringGroupScalarBase.evaluate(FilterStringGroupColEqualStringGroupScalarBase.java:42)

        at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:110)

        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)

        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)

        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:774)

        ... 19 more



When vectorization is disabled


2016-07-27T01:52:43,328  INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.ReduceSinkOperator: Using tag = -1

2016-07-27T01:52:43,328  INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.OperatorUtils: Setting output collector: RS[4] --&amp;gt; Reducer 2

2016-07-27T01:52:43,329 ERROR [TezTaskRunner (1469608604787_0002_26_00_000000_0)] io.BatchToRowReader: Error at row 0/1, column 0/1 org.apache.hadoop.hive.ql.exec.vector.LongColumnVector@7630e56a

java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

        at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextString(BatchToRowReader.java:334) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextValue(BatchToRowReader.java:602) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:149) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:78) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151) ~[tez-mapreduce-0.8.4.jar:0.8.4]

        at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) ~[tez-mapreduce-0.8.4.jar:0.8.4]

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) ~[tez-runtime-internals-0.8.4.jar:0.8.4]

        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) ~[tez-runtime-internals-0.8.4.jar:0.8.4]

        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4]

        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_91]

        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_91]

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) ~[hadoop-common-2.6.0.jar:?]

        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4]

        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) ~[tez-runtime-internals-0.8.4.jar:0.8.4]

        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.8.4.jar:0.8.4]

        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) ~[hive-llap-server-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_91]

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91]

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91]

        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]



</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ReadPipeline.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">14310</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-08-03 20:27:12" id="14397" opendate="2016-08-01 07:57:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Queries ran after reopening of tez session launches additional sessions</summary>
			
			
			<description>Say we have configured hive.server2.tez.default.queues with 2 queues q1 and q2 with default expiry interval of 5 mins.
After 5 mins of non-usage the sessions corresponding to queues q1 and q2 will be expired. When new set of queries are issue after this expiry, the default sessions backed by q1 and q2 and reopened again. Now when we run more queries the reopened sessions are not used instead new session is opened. 
At this point there will be 4 sessions running (2 abandoned sessions and 2 current sessions). </description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-05 10:33:16" id="14402" opendate="2016-08-02 18:17:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: Fix Mapjoin overflow deserialization </summary>
			
			
			<description>This is in a codepath currently disabled in master, however enabling it triggers OOB.



Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024

        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setRef(BytesColumnVector.java:92)

        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeRowColumn(VectorDeserializeRow.java:415)

        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorDeserializeRow.java:674)

        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultLargeMultiValue(VectorMapJoinGenerateResultOperator.java:307)

        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultMultiValue(VectorMapJoinGenerateResultOperator.java:226)

        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultRepeatedAll(VectorMapJoinGenerateResultOperator.java:391)


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-05 18:40:48" id="14424" opendate="2016-08-04 08:25:33" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Address CLIRestoreTest failure</summary>
			
			
			<description>


java.lang.RuntimeException: Error applying authorization policy on hive configuration: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest



	at org.apache.hive.service.cli.CLIService.init(CLIService.java:113)

	at org.apache.hive.service.cli.CLIServiceRestoreTest.getService(CLIServiceRestoreTest.java:48)

	at org.apache.hive.service.cli.CLIServiceRestoreTest.&amp;lt;init&amp;gt;(CLIServiceRestoreTest.java:28)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)

	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)

	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)

	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)

	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)

	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)

	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)

	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)

	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)

	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)

	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)

	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)

	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)

	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)

	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:234)

	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:74)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest

	at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:836)

	at org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy(SessionState.java:1602)

	at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:126)

	at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)

	... 22 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest

	at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizeProviderManager(HiveUtils.java:385)

	at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:812)

	... 25 more

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest

	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)

	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)

	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)

	at java.lang.Class.forName0(Native Method)

	at java.lang.Class.forName(Class.java:348)

	at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizeProviderManager(HiveUtils.java:375)

	... 26 more





But is caused by HIVE-14221. Code changes are here: https://github.com/apache/hive/commit/de5ae86ee70d9396d5cefc499507b5f31fecc916
So the issue is that, in this patch, everywhere the class org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory has been mentioned, except at one place. That one place is using org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest class, which happens to not be in the classpath while running hive-service tests. 
Seems like the wrong class was mentioned by mistake in the patch. 
Pengcheng Xiong Since you are the original author, can you confirm whether it indeed was a mistake. 
</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.CLIServiceRestoreTest.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-15 20:59:37" id="14483" opendate="2016-08-09 14:22:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary> java.lang.ArrayIndexOutOfBoundsException org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays</summary>
			
			
			<description>Error message:
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024
at org.apache.orc.impl.RunLengthIntegerReaderV2.nextVector(RunLengthIntegerReaderV2.java:369)
at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays(TreeReaderFactory.java:1231)
at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.readOrcByteArrays(TreeReaderFactory.java:1268)
at org.apache.orc.impl.TreeReaderFactory$StringDirectTreeReader.nextVector(TreeReaderFactory.java:1368)
at org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector(TreeReaderFactory.java:1212)
at org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector(TreeReaderFactory.java:1902)
at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch(TreeReaderFactory.java:1737)
at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1045)
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ensureBatch(RecordReaderImpl.java:77)
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext(RecordReaderImpl.java:89)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:230)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:205)
at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
... 22 more
How to reproduce?
Configure StringTreeReader  which contains StringDirectTreeReader as TreeReader (DIRECT or DIRECT_V2 column encoding)
batchSize = 1026;
invoke method nextVector(ColumnVector previousVector,boolean[] isNull, final int batchSize)
scratchlcv is LongColumnVector with long[] vector  (length 1024)
 which execute BytesColumnVectorUtil.readOrcByteArrays(stream, lengths, scratchlcv,result, batchSize);
as result in method commonReadByteArrays(stream, lengths, scratchlcv,
            result, (int) batchSize) we received ArrayIndexOutOfBoundsException.
If we use StringDictionaryTreeReader, then there is no exception, as we have a verification  scratchlcv.ensureSize((int) batchSize, false) before reader.nextVector(scratchlcv, scratchlcv.vector, batchSize);
These changes were made for Hive 2.1.0 by corresponding commit https://github.com/apache/hive/commit/0ac424f0a17b341efe299da167791112e4a953e9#diff-a1cec556fb2db4b69a1a4127a6908177R1467 for task  https://issues.apache.org/jira/browse/HIVE-12159 by Owen O&amp;amp;apos;Malley
How to fix?
add  only one line :
scratchlcv.ensureSize((int) batchSize, false) ;
in method org.apache.orc.impl.TreeReaderFactory#BytesColumnVectorUtil#commonReadByteArrays(InStream stream, IntegerReader lengths,
        LongColumnVector scratchlcv,
        BytesColumnVector result, final int batchSize) before invocation lengths.nextVector(scratchlcv, scratchlcv.vector, batchSize);
</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1, 2.0.2</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.orc.impl.TreeReaderFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14968</link>
			
			
			<link description="is broken by" type="Regression">12159</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-08-16 18:08:03" id="14463" opendate="2016-08-08 08:21:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hcatalog server extensions test cases getting stuck</summary>
			
			
			<description>The module is getting stuck in tests and not coming out for as long as 2 days. 
Specifically, TestMsgBusConnection is the test case which has this problem. I ran the tests on local environment and took a thread dump after it got stuck. 

Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.80-b11 mixed mode):

&quot;InactivityMonitor Async Task: java.util.concurrent.ThreadPoolExecutor$Worker@2c040428[State = -1, empty queue]&quot; daemon prio=5 tid=0x00007fe90d89e000 nid=0x8827 waiting on condition [0x0000000117b74000]

   java.lang.Thread.State: TIMED_WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x000000078166f0b8&amp;gt; (a java.util.concurrent.SynchronousQueue$TransferStack)

	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)

	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)

	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)

	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)

	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

&quot;InactivityMonitor Async Task: java.util.concurrent.ThreadPoolExecutor$Worker@182a483f[State = -1, empty queue]&quot; daemon prio=5 tid=0x00007fe90d801000 nid=0x585f waiting on condition [0x000000011786b000]

   java.lang.Thread.State: TIMED_WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x000000078166f0b8&amp;gt; (a java.util.concurrent.SynchronousQueue$TransferStack)

	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)

	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)

	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)

	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)

	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

&quot;ActiveMQ Transport: tcp:///127.0.0.1:56883&quot; daemon prio=5 tid=0x00007fe90c83e800 nid=0x8403 runnable [0x00000001196ab000]

   java.lang.Thread.State: RUNNABLE

	at java.net.SocketInputStream.socketRead0(Native Method)

	at java.net.SocketInputStream.read(SocketInputStream.java:152)

	at java.net.SocketInputStream.read(SocketInputStream.java:122)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)

	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)

	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)

	at java.io.DataInputStream.readInt(DataInputStream.java:387)

	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)

	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)

	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)

	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)

	at java.lang.Thread.run(Thread.java:745)

&quot;ActiveMQ Transport: tcp://localhost/127.0.0.1:61616&quot; prio=5 tid=0x00007fe90b81e800 nid=0x8003 runnable [0x00000001194a5000]

   java.lang.Thread.State: RUNNABLE

	at java.net.SocketInputStream.socketRead0(Native Method)

	at java.net.SocketInputStream.read(SocketInputStream.java:152)

	at java.net.SocketInputStream.read(SocketInputStream.java:122)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)

	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)

	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)

	at java.io.DataInputStream.readInt(DataInputStream.java:387)

	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)

	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)

	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)

	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)

	at java.lang.Thread.run(Thread.java:745)

&quot;ActiveMQConnection[ID:IM1258-X1-56879-1470642083403-2:2] Scheduler&quot; daemon prio=5 tid=0x00007fe90c861800 nid=0x7e03 in Object.wait() [0x000000011926e000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007f594cc20&amp;gt; (a java.util.TaskQueue)

	at java.lang.Object.wait(Object.java:503)

	at java.util.TimerThread.mainLoop(Timer.java:526)

	- locked &amp;lt;0x00000007f594cc20&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.run(Timer.java:505)

&quot;BoneCP-pool-watch-thread&quot; daemon prio=5 tid=0x00007fe90b23b000 nid=0x6d07 waiting on condition [0x0000000118a4f000]

   java.lang.Thread.State: WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x00000007f5b29bc0&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)

	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)

	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)

	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)

	at com.jolbox.bonecp.PoolWatchThread.run(PoolWatchThread.java:75)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

&quot;BoneCP-keep-alive-scheduler&quot; daemon prio=5 tid=0x00007fe90c937000 nid=0x7107 waiting on condition [0x0000000118746000]

   java.lang.Thread.State: TIMED_WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x00000007f5b40a30&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)

	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)

	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)

	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)

	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)

	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

&quot;com.google.common.base.internal.Finalizer&quot; daemon prio=5 tid=0x00007fe90c8fd000 nid=0x7707 in Object.wait() [0x0000000118540000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007f5b40608&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)

	- locked &amp;lt;0x00000007f5b40608&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)

	at com.google.common.base.internal.Finalizer.run(Finalizer.java:132)

	at java.lang.Thread.run(Thread.java:745)

&quot;BoneCP-pool-watch-thread&quot; daemon prio=5 tid=0x00007fe90b464800 nid=0x7c03 waiting on condition [0x00000001193a2000]

   java.lang.Thread.State: WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x00000007f5b41470&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)

	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)

	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)

	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)

	at com.jolbox.bonecp.PoolWatchThread.run(PoolWatchThread.java:75)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

&quot;BoneCP-keep-alive-scheduler&quot; daemon prio=5 tid=0x00007fe90c07f800 nid=0x7a03 waiting on condition [0x0000000118e28000]

   java.lang.Thread.State: TIMED_WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x00000007f5b41748&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)

	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)

	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)

	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)

	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)

	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

&quot;com.google.common.base.internal.Finalizer&quot; daemon prio=5 tid=0x00007fe90c157800 nid=0x300b in Object.wait() [0x0000000118c55000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007f5b41b18&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)

	- locked &amp;lt;0x00000007f5b41b18&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)

	at com.google.common.base.internal.Finalizer.run(Finalizer.java:132)

	at java.lang.Thread.run(Thread.java:745)

&quot;derby.rawStoreDaemon&quot; daemon prio=5 tid=0x00007fe90c866800 nid=0x3207 in Object.wait() [0x0000000118b52000]

   java.lang.Thread.State: TIMED_WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007f58bb788&amp;gt; (a org.apache.derby.impl.services.daemon.BasicDaemon)

	at org.apache.derby.impl.services.daemon.BasicDaemon.rest(Unknown Source)

	- locked &amp;lt;0x00000007f58bb788&amp;gt; (a org.apache.derby.impl.services.daemon.BasicDaemon)

	at org.apache.derby.impl.services.daemon.BasicDaemon.run(Unknown Source)

	at java.lang.Thread.run(Thread.java:745)

&quot;Timer-0&quot; daemon prio=5 tid=0x00007fe90c842000 nid=0x2f07 in Object.wait() [0x00000001144ed000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007f59c9230&amp;gt; (a java.util.TaskQueue)

	at java.lang.Object.wait(Object.java:503)

	at java.util.TimerThread.mainLoop(Timer.java:526)

	- locked &amp;lt;0x00000007f59c9230&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.run(Timer.java:505)

&quot;InactivityMonitor WriteCheck&quot; daemon prio=5 tid=0x00007fe90b21d000 nid=0x7503 in Object.wait() [0x000000011894c000]

   java.lang.Thread.State: TIMED_WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x0000000781663e20&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.mainLoop(Timer.java:552)

	- locked &amp;lt;0x0000000781663e20&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.run(Timer.java:505)

&quot;InactivityMonitor ReadCheck&quot; daemon prio=5 tid=0x00007fe90b21c000 nid=0x7303 in Object.wait() [0x0000000118849000]

   java.lang.Thread.State: TIMED_WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x000000078166ec48&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.mainLoop(Timer.java:552)

	- locked &amp;lt;0x000000078166ec48&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.run(Timer.java:505)

&quot;ActiveMQ Transport: tcp:///127.0.0.1:56880&quot; daemon prio=5 tid=0x00007fe90b227800 nid=0x6f03 runnable [0x0000000118643000]

   java.lang.Thread.State: RUNNABLE

	at java.net.SocketInputStream.socketRead0(Native Method)

	at java.net.SocketInputStream.read(SocketInputStream.java:152)

	at java.net.SocketInputStream.read(SocketInputStream.java:122)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)

	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)

	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)

	at java.io.DataInputStream.readInt(DataInputStream.java:387)

	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)

	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)

	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)

	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)

	at java.lang.Thread.run(Thread.java:745)

&quot;ActiveMQ Transport: tcp://localhost/127.0.0.1:61616&quot; prio=5 tid=0x00007fe9099a7000 nid=0x6b03 runnable [0x000000011843d000]

   java.lang.Thread.State: RUNNABLE

	at java.net.SocketInputStream.socketRead0(Native Method)

	at java.net.SocketInputStream.read(SocketInputStream.java:152)

	at java.net.SocketInputStream.read(SocketInputStream.java:122)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.fill(TcpBufferedInputStream.java:50)

	at org.apache.activemq.transport.tcp.TcpTransport$2.fill(TcpTransport.java:576)

	at org.apache.activemq.transport.tcp.TcpBufferedInputStream.read(TcpBufferedInputStream.java:58)

	at org.apache.activemq.transport.tcp.TcpTransport$2.read(TcpTransport.java:561)

	at java.io.DataInputStream.readInt(DataInputStream.java:387)

	at org.apache.activemq.openwire.OpenWireFormat.unmarshal(OpenWireFormat.java:269)

	at org.apache.activemq.transport.tcp.TcpTransport.readCommand(TcpTransport.java:227)

	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:219)

	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)

	at java.lang.Thread.run(Thread.java:745)

&quot;ActiveMQConnection[ID:IM1258-X1-56879-1470642083403-2:1] Scheduler&quot; daemon prio=5 tid=0x00007fe90aa08800 nid=0x6903 in Object.wait() [0x000000011833a000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007816689e8&amp;gt; (a java.util.TaskQueue)

	at java.lang.Object.wait(Object.java:503)

	at java.util.TimerThread.mainLoop(Timer.java:526)

	- locked &amp;lt;0x00000007816689e8&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.run(Timer.java:505)

&quot;ActiveMQ Transport Server: tcp://localhost:61616?broker.persistent=false&quot; daemon prio=5 tid=0x00007fe90b211000 nid=0x6703 runnable [0x0000000118237000]

   java.lang.Thread.State: RUNNABLE

	at java.net.PlainSocketImpl.socketAccept(Native Method)

	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)

	at java.net.ServerSocket.implAccept(ServerSocket.java:530)

	at java.net.ServerSocket.accept(ServerSocket.java:498)

	at org.apache.activemq.transport.tcp.TcpTransportServer.run(TcpTransportServer.java:280)

	at java.lang.Thread.run(Thread.java:745)

&quot;ActiveMQ Transport Server Thread Handler: tcp://localhost:61616?broker.persistent=false&quot; daemon prio=5 tid=0x00007fe90b1ff000 nid=0x6503 waiting on condition [0x0000000118134000]

   java.lang.Thread.State: TIMED_WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x000000078009d210&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)

	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)

	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)

	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)

	at org.apache.activemq.transport.tcp.TcpTransportServer$1.run(TcpTransportServer.java:352)

	at java.lang.Thread.run(Thread.java:745)

&quot;ActiveMQ Broker[localhost] Scheduler&quot; daemon prio=5 tid=0x00007fe90aa08000 nid=0x6303 in Object.wait() [0x0000000117e7d000]

   java.lang.Thread.State: TIMED_WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x000000078159b640&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.mainLoop(Timer.java:552)

	- locked &amp;lt;0x000000078159b640&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.run(Timer.java:505)

&quot;ActiveMQ Data File Writer&quot; daemon prio=5 tid=0x00007fe90b201800 nid=0x6103 in Object.wait() [0x0000000117d7a000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007814e62f8&amp;gt; (a org.apache.kahadb.journal.DataFileAppender$1)

	at java.lang.Object.wait(Object.java:503)

	at org.apache.kahadb.journal.DataFileAppender.processQueue(DataFileAppender.java:312)

	- locked &amp;lt;0x00000007814e62f8&amp;gt; (a org.apache.kahadb.journal.DataFileAppender$1)

	at org.apache.kahadb.journal.DataFileAppender$2.run(DataFileAppender.java:203)

&quot;ActiveMQ Journal Checkpoint Worker&quot; daemon prio=5 tid=0x00007fe90b1fc000 nid=0x5f03 waiting on condition [0x0000000117c77000]

   java.lang.Thread.State: TIMED_WAITING (sleeping)

	at java.lang.Thread.sleep(Native Method)

	at org.apache.activemq.store.kahadb.MessageDatabase$3.run(MessageDatabase.java:296)

&quot;KahaDB Scheduler&quot; daemon prio=5 tid=0x00007fe90aad6000 nid=0x4f07 in Object.wait() [0x0000000117455000]

   java.lang.Thread.State: TIMED_WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007814f6ea0&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.mainLoop(Timer.java:552)

	- locked &amp;lt;0x00000007814f6ea0&amp;gt; (a java.util.TaskQueue)

	at java.util.TimerThread.run(Timer.java:505)

&quot;RMI RenewClean-[10.14.123.167:56875]&quot; daemon prio=5 tid=0x00007fe90b1fe000 nid=0x5b03 in Object.wait() [0x0000000117a71000]

   java.lang.Thread.State: TIMED_WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x0000000781607678&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)

	- locked &amp;lt;0x0000000781607678&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at sun.rmi.transport.DGCClient$EndpointEntry$RenewCleanThread.run(DGCClient.java:535)

	at java.lang.Thread.run(Thread.java:745)

&quot;RMI Scheduler(0)&quot; daemon prio=5 tid=0x00007fe90b1fd800 nid=0x5903 waiting on condition [0x000000011796e000]

   java.lang.Thread.State: TIMED_WAITING (parking)

	at sun.misc.Unsafe.park(Native Method)

	- parking to wait for  &amp;lt;0x00000007801bcf88&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)

	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)

	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)

	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)

	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)

	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

&quot;GC Daemon&quot; daemon prio=5 tid=0x00007fe9099c8800 nid=0x5503 in Object.wait() [0x000000011775e000]

   java.lang.Thread.State: TIMED_WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x00000007800106b0&amp;gt; (a sun.misc.GC$LatencyLock)

	at sun.misc.GC$Daemon.run(GC.java:117)

	- locked &amp;lt;0x00000007800106b0&amp;gt; (a sun.misc.GC$LatencyLock)

&quot;RMI Reaper&quot; prio=5 tid=0x00007fe90b1c0800 nid=0x5303 in Object.wait() [0x000000011765b000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x0000000780000b50&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)

	- locked &amp;lt;0x0000000780000b50&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)

	at sun.rmi.transport.ObjectTable$Reaper.run(ObjectTable.java:351)

	at java.lang.Thread.run(Thread.java:745)

&quot;RMI TCP Accept-0&quot; daemon prio=5 tid=0x00007fe90b1ad800 nid=0x5103 runnable [0x0000000117558000]

   java.lang.Thread.State: RUNNABLE

	at java.net.PlainSocketImpl.socketAccept(Native Method)

	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)

	at java.net.ServerSocket.implAccept(ServerSocket.java:530)

	at java.net.ServerSocket.accept(ServerSocket.java:498)

	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:399)

	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:371)

	at java.lang.Thread.run(Thread.java:745)

&quot;RMI TCP Accept-1099&quot; daemon prio=5 tid=0x00007fe90b0b0800 nid=0x4e07 runnable [0x0000000117352000]

   java.lang.Thread.State: RUNNABLE

	at java.net.PlainSocketImpl.socketAccept(Native Method)

	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)

	at java.net.ServerSocket.implAccept(ServerSocket.java:530)

	at java.net.ServerSocket.accept(ServerSocket.java:498)

	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:399)

	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:371)

	at java.lang.Thread.run(Thread.java:745)

&quot;Service Thread&quot; daemon prio=5 tid=0x00007fe909852000 nid=0x4503 runnable [0x0000000000000000]

   java.lang.Thread.State: RUNNABLE

&quot;C2 CompilerThread1&quot; daemon prio=5 tid=0x00007fe90b023000 nid=0x4303 waiting on condition [0x0000000000000000]

   java.lang.Thread.State: RUNNABLE

&quot;C2 CompilerThread0&quot; daemon prio=5 tid=0x00007fe90b021800 nid=0x4103 waiting on condition [0x0000000000000000]

   java.lang.Thread.State: RUNNABLE

&quot;Signal Dispatcher&quot; daemon prio=5 tid=0x00007fe90b01c800 nid=0x340f waiting on condition [0x0000000000000000]

   java.lang.Thread.State: RUNNABLE

&quot;Finalizer&quot; daemon prio=5 tid=0x00007fe909848800 nid=0x2d03 in Object.wait() [0x00000001143a9000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x0000000780019310&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)

	- locked &amp;lt;0x0000000780019310&amp;gt; (a java.lang.ref.ReferenceQueue$Lock)

	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)

	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

&quot;Reference Handler&quot; daemon prio=5 tid=0x00007fe90b00c800 nid=0x2b03 in Object.wait() [0x00000001142a6000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x0000000780018fd8&amp;gt; (a java.lang.ref.Reference$Lock)

	at java.lang.Object.wait(Object.java:503)

	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)

	- locked &amp;lt;0x0000000780018fd8&amp;gt; (a java.lang.ref.Reference$Lock)

&quot;main&quot; prio=5 tid=0x00007fe90980b000 nid=0x1303 in Object.wait() [0x0000000109b16000]

   java.lang.Thread.State: WAITING (on object monitor)

	at java.lang.Object.wait(Native Method)

	- waiting on &amp;lt;0x0000000781669298&amp;gt; (a java.lang.Object)

	at java.lang.Object.wait(Object.java:503)

	at org.apache.activemq.SimplePriorityMessageDispatchChannel.dequeue(SimplePriorityMessageDispatchChannel.java:87)

	- locked &amp;lt;0x0000000781669298&amp;gt; (a java.lang.Object)

	at org.apache.activemq.ActiveMQMessageConsumer.dequeue(ActiveMQMessageConsumer.java:452)

	at org.apache.activemq.ActiveMQMessageConsumer.receive(ActiveMQMessageConsumer.java:504)

	at org.apache.hive.hcatalog.listener.TestMsgBusConnection.testConnection(TestMsgBusConnection.java:91)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:606)

	at junit.framework.TestCase.runTest(TestCase.java:176)

	at junit.framework.TestCase.runBare(TestCase.java:141)

	at junit.framework.TestResult$1.protect(TestResult.java:122)

	at junit.framework.TestResult.runProtected(TestResult.java:142)

	at junit.framework.TestResult.run(TestResult.java:125)

	at junit.framework.TestCase.run(TestCase.java:129)

	at junit.framework.TestSuite.runTest(TestSuite.java:255)

	at junit.framework.TestSuite.run(TestSuite.java:250)

	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)

	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)

	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)

	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)

	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)

	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)

	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)

&quot;VM Thread&quot; prio=5 tid=0x00007fe90a804000 nid=0x2903 runnable

&quot;GC task thread#0 (ParallelGC)&quot; prio=5 tid=0x00007fe90b004800 nid=0x2103 runnable

&quot;GC task thread#1 (ParallelGC)&quot; prio=5 tid=0x00007fe90b005800 nid=0x2303 runnable

&quot;GC task thread#2 (ParallelGC)&quot; prio=5 tid=0x00007fe90b006000 nid=0x2503 runnable

&quot;GC task thread#3 (ParallelGC)&quot; prio=5 tid=0x00007fe90a006000 nid=0x2703 runnable

&quot;VM Periodic Task Thread&quot; prio=5 tid=0x00007fe90b013000 nid=0x4703 waiting on condition

JNI global references: 156

Heap

 PSYoungGen      total 526336K, used 184063K [0x00000007d5500000, 0x00000007f6d00000, 0x0000000800000000)

  eden space 503808K, 33% used [0x00000007d5500000,0x00000007dfa320c0,0x00000007f4100000)

  from space 22528K, 66% used [0x00000007f5700000,0x00000007f658de08,0x00000007f6d00000)

  to   space 22528K, 0% used [0x00000007f4100000,0x00000007f4100000,0x00000007f5700000)

 ParOldGen       total 174592K, used 27797K [0x0000000780000000, 0x000000078aa80000, 0x00000007d5500000)

  object space 174592K, 15% used [0x0000000780000000,0x0000000781b257c8,0x000000078aa80000)

 PSPermGen       total 53760K, used 53538K [0x0000000760000000, 0x0000000763480000, 0x0000000780000000)

  object space 53760K, 99% used [0x0000000760000000,0x0000000763448810,0x0000000763480000)





 Ashutosh Chauhan Can you check since you&amp;amp;apos;re the author of this test. 
</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.listener.TestMsgBusConnection.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-16 22:22:07" id="12656" opendate="2015-12-11 19:18:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Turn hive.compute.query.using.stats on by default</summary>
			
			
			<description>We now have hive.compute.query.using.stats=false by default. We plan to turn it on by default so that we can have better performance. We can also set it to false in some test cases to maintain the original purpose of those tests..</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11160</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-08-19 18:29:52" id="14563" opendate="2016-08-17 23:49:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>StatsOptimizer treats NULL in a wrong way</summary>
			
			
			<description>


OSTHOOK: query: explain select count(key) from (select null as key from src)src

POSTHOOK: type: QUERY

STAGE DEPENDENCIES:

  Stage-0 is a root stage



STAGE PLANS:

  Stage: Stage-0

    Fetch Operator

      limit: 1

      Processor Tree:

        ListSink



PREHOOK: query: select count(key) from (select null as key from src)src

PREHOOK: type: QUERY

PREHOOK: Input: default@src

#### A masked pattern was here ####

POSTHOOK: query: select count(key) from (select null as key from src)src

POSTHOOK: type: QUERY

POSTHOOK: Input: default@src

#### A masked pattern was here ####

500


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			
			
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-25 18:17:28" id="14617" opendate="2016-08-24 16:30:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NPE in UDF MapValues() if input is null</summary>
			
			
			<description>For query



select exploded_traits from hdrone.vehiclestore_udr_vehicle 

lateral view explode(map_values(vehicle_traits.vehicle_traits)) traits as exploded_traits 

where datestr &amp;gt; &amp;amp;apos;2016-08-22&amp;amp;apos; LIMIT 100



Job fails with error msg as follows:



Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {&quot;ts&quot;:null,&quot;_max_added_id&quot;:null,&quot;identity_info&quot;:null,&quot;vehicle_specs&quot;:null,&quot;tracking_info&quot;:null,&quot;color_info&quot;:null,&quot;vehicle_traits&quot;:null,&quot;detail_info&quot;:null,&quot;_row_key&quot;:null,&quot;_shard&quot;:null,&quot;image_info&quot;:null,&quot;vehicle_tags&quot;:null,&quot;activation_info&quot;:null,&quot;flavor_info&quot;:null,&quot;sounds&quot;:null,&quot;legacy_info&quot;:null,&quot;images&quot;:null,&quot;datestr&quot;:&quot;2016-08-24&quot;} at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:179) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {&quot;ts&quot;:null,&quot;_max_added_id&quot;:null,&quot;identity_info&quot;:null,&quot;vehicle_specs&quot;:null,&quot;tracking_info&quot;:null,&quot;color_info&quot;:null,&quot;vehicle_traits&quot;:null,&quot;detail_info&quot;:null,&quot;_row_key&quot;:null,&quot;_shard&quot;:null,&quot;image_info&quot;:null,&quot;vehicle_tags&quot;:null,&quot;activation_info&quot;:null,&quot;flavor_info&quot;:null,&quot;sounds&quot;:null,&quot;legacy_info&quot;:null,&quot;images&quot;:null,&quot;datestr&quot;:&quot;2016-08-24&quot;} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170) ... 8 more Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating map_values(vehicle_traits.vehicle_traits) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:82) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.processOp(LateralViewForwardOperator.java:37) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497) ... 9 more Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.evaluate(GenericUDFMapValues.java:64) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:185) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:77) ... 15 more 



It appears that null is not properly handled in GenericUDFMapValues.evaluate() method.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-26 00:31:00" id="14619" opendate="2016-08-24 19:05:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CASE folding can produce wrong expression</summary>
			
			
			<description>This is a case that was not covered in the testsuite. For the following query:



select (CASE WHEN key = value THEN &amp;amp;apos;1&amp;amp;apos; WHEN true THEN &amp;amp;apos;0&amp;amp;apos; ELSE NULL END)

from src



Currently, we end up folding the select expression to &amp;amp;apos;0&amp;amp;apos;, as we fail bail out in the second statement and fail to account that there are two different possible values for the CASE expression (&amp;amp;apos;1&amp;amp;apos; and &amp;amp;apos;0&amp;amp;apos;).</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-31 22:43:40" id="14652" opendate="2016-08-26 01:33:05" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>incorrect results for not in on partition columns</summary>
			
			
			<description>
create table foo (i int) partitioned by (s string);



insert overwrite table foo partition(s=&amp;amp;apos;foo&amp;amp;apos;) select cint from alltypesorc limit 10;

insert overwrite table foo partition(s=&amp;amp;apos;bar&amp;amp;apos;) select cint from alltypesorc limit 10;



select * from foo where s not in (&amp;amp;apos;bar&amp;amp;apos;);



No results. IN ... works correctly</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">11424</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-09-02 07:11:22" id="14530" opendate="2016-08-12 08:17:14" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Union All query returns incorrect results</summary>
			
			
			<description>
create table dw_tmp.l_test1 (id bigint,val string,trans_date string) row format delimited fields terminated by &amp;amp;apos; &amp;amp;apos; ;
create table dw_tmp.l_test2 (id bigint,val string,trans_date string) row format delimited fields terminated by &amp;amp;apos; &amp;amp;apos; ;  
select * from dw_tmp.l_test1;
1       table_1      2016-08-11
select * from dw_tmp.l_test2;
2       table_2      2016-08-11
 right like this
select 
    id,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date
from dw_tmp.l_test1
union all
select 
    id,
    val,
    trans_date
from dw_tmp.l_test2 ;
1       table_1     2016-08-11
2       table_2     2016-08-11
 incorrect
select 
    id,
    999,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date
from dw_tmp.l_test1
union all
select 
    id,
    999,
    val,
    trans_date
from dw_tmp.l_test2 ;
1       999     table_1     2016-08-11
2       999     table_1     2016-08-11     &amp;lt;-- here is wrong
 incorrect
select 
    id,
    999,
    666,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date
from dw_tmp.l_test1
union all
select 
    id,
    999,
    666,
    val,
    trans_date
from dw_tmp.l_test2 ;
1       999     666     table_1 2016-08-11
2       999     666     table_1 2016-08-11     &amp;lt;-- here is wrong
 right
select 
    id,
    999,
    &amp;amp;apos;table_1&amp;amp;apos; ,
    trans_date,
    &amp;amp;apos;2016-11-11&amp;amp;apos;
from dw_tmp.l_test1
union all
select 
    id,
    999,
    val,
    trans_date,
    trans_date
from dw_tmp.l_test2 ;
1       999     table_1 2016-08-11      2016-11-11
2       999     table_2 2016-08-11      2016-08-11</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">13639</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-09-05 15:03:24" id="14697" opendate="2016-09-02 16:54:04" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Can not access kerberized HS2 Web UI</summary>
			
			
			<description>Failed to access kerberized HS2 WebUI with following error msg:



curl -v -u : --negotiate http://util185.phx2.cbsig.net:10002/ 

&amp;gt; GET / HTTP/1.1 

&amp;gt; Host: util185.phx2.cbsig.net:10002 

&amp;gt; Authorization: Negotiate YIIU7...[redacted]... 

&amp;gt; User-Agent: curl/7.42.1 

&amp;gt; Accept: */* 

&amp;gt; 

&amp;lt; HTTP/1.1 413 FULL head 

&amp;lt; Content-Length: 0 

&amp;lt; Connection: close 

&amp;lt; Server: Jetty(7.6.0.v20120127) 



It is because the Jetty default request header (4K) is too small in some kerberos case.
So this patch is to increase the request header to 64K.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.http.HttpServer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-13 14:48:27" id="14726" opendate="2016-09-08 23:15:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>delete statement fails when spdo is on</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14783</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-09-14 06:18:44" id="13878" opendate="2016-05-27 14:22:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: Column pruning for Text vectorization</summary>
			
			
			<description>Column pruning in TextFile vectorization does not work with Vector SerDe settings due to LazySimple deser codepath issues.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.fast.DeserializeRead.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VerifyFastRow.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinaryFast.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleFast.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.CheckFastRowHashMap.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.VerifyFast.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringCommon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableFast.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="incorporates" type="Incorporates">13887</link>
			
			
			<link description="incorporates" type="Incorporates">13876</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-09-21 20:23:47" id="14098" opendate="2016-06-27 09:01:13" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Logging task properties, and environment variables might contain passwords</summary>
			
			
			<description>Hive MapredLocalTask Can Print Environment Passwords, like -Djavax.net.ssl.trustStorePassword.
The same could happen, when logging spark properties</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-29 20:59:36" id="14819" opendate="2016-09-22 19:16:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>FunctionInfo for permanent functions shows TEMPORARY FunctionType</summary>
			
			
			<description>The FunctionInfo has a FunctionType field which describes if the function is a builtin/persistent/temporary function. But for permanent functions, the FunctionInfo being returned by the FunctionRegistry is showing the type to be TEMPORARY.
This affects things which may be depending on function type, for example LlapDecider, which will allow builtin/persistent UDFs to be used in LLAP but not temporary functions.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-10-05 16:55:43" id="14873" opendate="2016-10-01 06:44:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add UDF for extraction of &amp;apos;day of week&amp;apos;</summary>
			
			
			<description/>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14579</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-10-14 16:04:30" id="14839" opendate="2016-09-25 12:07:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Improve the stability of TestSessionManagerMetrics</summary>
			
			
			<description>The TestSessionManagerMetrics fails occasionally with the following error: 

org.junit.ComparisonFailure: expected:&amp;lt;[0]&amp;gt; but was:&amp;lt;[1]&amp;gt;

	at org.apache.hive.service.cli.session.TestSessionManagerMetrics.testThreadPoolMetrics(TestSessionManagerMetrics.java:98)



Failed tests: 

  TestSessionManagerMetrics.testThreadPoolMetrics:98 expected:&amp;lt;[0]&amp;gt; but was:&amp;lt;[1]&amp;gt;



This test starts four background threads with a &quot;wait&quot; call in their run method. The threads are using the common &quot;barrier&quot; object as lock. 
The expected behaviour is that two threads will be in the async pool (because the hive.server2.async.exec.threads is set to 2) and the other two thread will be waiting in the queue. This condition is checked like this:

MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_POOL_SIZE, 2);

MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_QUEUE_SIZE, 2);



Then a notifyAll is called on the lock object, so the two threads in the pool should &quot;wake up&quot; and complete and the other two threads should go from the queue to the pool. This is checked like this in the test:

MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_POOL_SIZE, 2);

MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.GAUGE, MetricsConstant.EXEC_ASYNC_QUEUE_SIZE, 0);



There are two use cases which can cause error in this test:

The notifyAll call happens before both threads in the pool are up and running and in the &quot;wait&quot; phase.
In this case the thread which is not up in time will stuck in the pool, so the other two threads can not move from the queue to the pool. 
After the notifyAll call, the threads in the pool &quot;wake up&quot; with some delay. So they don&amp;amp;apos;t complete and removed from the pool and the other two threads are not moved from the queue to the pool until the metrics are checked. Therefore the check fails, since the queue is not empty.

</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-10-17 19:41:30" id="14959" opendate="2016-10-14 07:13:03" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix DISTINCT with windowing when CBO is enabled/disabled</summary>
			
			
			<description>For instance, the following query with CBO off:



select distinct last_value(i) over ( partition by si order by i ),

  first_value(t)  over ( partition by si order by i )

from over10k limit 50;



will fail, with the following message:

SELECT DISTINCT not allowed in the presence of windowing functions when CBO is off


</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13242</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-10-25 11:15:33" id="15029" opendate="2016-10-21 11:29:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add logic to estimate stats for BETWEEN operator</summary>
			
			
			<description>Currently, BETWEEN operator is considered in the default case: reduces the input rows to the half. This may lead to wrong estimates for the number of rows produced by Filter operators.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-10-25 11:18:55" id="15030" opendate="2016-10-21 12:08:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fixes in inference of collation for Tez cost model</summary>
			
			
			<description>Tez cost model might get NPE if collation returned by join algorithm is null.</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveAlgorithmsUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-11-08 07:17:02" id="14924" opendate="2016-10-10 22:40:22" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>MSCK REPAIR table with single threaded is throwing null pointer exception</summary>
			
			
			<description>MSCK REPAIR TABLE is throwing Null Pointer Exception while running on single threaded mode (hive.mv.files.thread=0)
Error:
2016-10-10T22:27:13,564 ERROR [e9ce04a8-2a84-426d-8e79-a2d15b8cee09 main([])]: exec.DDLTask (DDLTask.java:failed(581)) - java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkPartitionDirs(HiveMetaStoreChecker.java:423)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.findUnknownPartitions(HiveMetaStoreChecker.java:315)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:291)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:236)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkMetastore(HiveMetaStoreChecker.java:113)
	at org.apache.hadoop.hive.ql.exec.DDLTask.msck(DDLTask.java:1834)
In order to reproduce:
set hive.mv.files.thread=0 and run MSCK REPAIR TABLE command</description>
			
			
			<version>2.1.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">13984</link>
			
		
		</links>
		
	
	</bug>
</bugrepository>
