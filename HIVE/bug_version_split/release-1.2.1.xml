<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="HIVE">
	<bug fixdate="2015-05-08 23:59:33" id="9486" opendate="2015-01-28 00:58:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Use session classloader instead of application loader</summary>
			
			
			<description>From http://www.mail-archive.com/dev@hive.apache.org/msg107615.html
Looks reasonable</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.Utils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatSplit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.JavaUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11733</link>
			
			
			<link description="is related to" type="Reference">10066</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-01 01:13:41" id="11504" opendate="2015-08-10 03:29:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Predicate pushing down doesn&amp;apos;t work for float type for Parquet</summary>
			
			
			<description>Predicate builder should use PrimitiveTypeName type in parquet side to construct predicate leaf instead of the type provided by PredicateLeaf.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">15004</link>
			
			
			<link description="duplicates" type="Duplicate">13114</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-22 20:50:13" id="11850" opendate="2015-09-16 20:04:56" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>On Windows, creating udf function using wasb fail throwing java.lang.RuntimeException: invalid url: wasb:///...  expecting ( file | hdfs | ivy)  as url scheme.</summary>
			
			
			<description>
hive&amp;gt; drop function if exists gencounter;

OK

Time taken: 2.614 seconds

On Humboldt, creating UDF function fail as follows:

hive&amp;gt; create function gencounter as &amp;amp;apos;org.apache.hive.udf.generic.GenericUDFGenCounter&amp;amp;apos; using jar &amp;amp;apos;wasb:///tmp/hive-udfs-0.1.jar&amp;amp;apos;;

invalid url: wasb:///tmp/hive-udfs-0.1.jar, expecting ( file | hdfs | ivy)  as url scheme.

Failed to register default.gencounter using class org.apache.hive.udf.generic.GenericUDFGenCounter

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask



The jar exists in wasb dir:

hrt_qa@headnode0:~$ hadoop fs -ls wasb:///tmp/

Found 2 items

-rw-r--r--   1 hrt_qa supergroup       4472 2015-09-16 11:50 wasb:///tmp/hive-udfs-0.1.jar

drwxrwxrwx   - hdfs   supergroup          0 2015-09-16 12:00 wasb:///tmp/aa


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.2.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11920</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-01 17:36:56" id="12006" opendate="2015-10-01 17:28:02" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Enable Columnar Pushdown for RC/ORC File for HCatLoader</summary>
			
			
			<description>This initially enabled by HIVE-5193. However, HIVE-10752 reverted it since there is issue in original implementation.
We shall fix the issue an reenable it.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10755</link>
			
			
			<link description="supercedes" type="Supercedes">5193</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-11 15:22:25" id="12087" opendate="2015-10-11 07:18:07" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>IMPORT TABLE fails</summary>
			
			
			<description>IMPORT TABLE fails for larger tables with:



0: jdbc:hive2://hdpprdhiv01.prd.xxx:10001/&amp;gt; import from &amp;amp;apos;/tmp/export/repository/res_sales_navigator&amp;amp;apos;;

INFO  : Copying data from hdfs://hdpprdmas01.prd.xxx:8020/tmp/export/repository/res_sales_navigator/valid_from=201508250000 to hdfs://hdpprdmas01.prd.xxx:8020/tmp/export/repository/res_sales_navigator/.hive-staging_hive_2015-10-07_20-55-37_456_5706704167497413401-2/-ext-10000

INFO  : Copying file: hdfs://hdpprdmas01.prd.xxx:8020/tmp/export/repository/res_sales_navigator/valid_from=201508250000/part-r-00000

ERROR : Failed with exception Cannot get DistCp constructor: org.apache.hadoop.tools.DistCp.&amp;lt;init&amp;gt;()

java.io.IOException: Cannot get DistCp constructor: org.apache.hadoop.tools.DistCp.&amp;lt;init&amp;gt;()

	at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1160)

	at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)

	at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)

	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)

	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)

	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1653)

	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1412)

	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)

	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)

	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)

	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)



Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.CopyTask (state=08S01,code=1)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11607</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-21 21:44:14" id="12218" opendate="2015-10-20 22:15:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Unable to create a like table for an hbase backed table</summary>
			
			
			<description>For an HBase backed table:



CREATE TABLE hbasetbl (key string, state string, country string, country_id int)

STORED BY &amp;amp;apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&amp;amp;apos;

WITH SERDEPROPERTIES (

&quot;hbase.columns.mapping&quot; = &quot;info:state,info:country,info:country_id&quot;

);



Create its like table using query such as 
create table hbasetbl_like like hbasetbl;
It fails with error:
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.hadoop.hive.ql.metadata.HiveException: must specify an InputFormat class
</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-10-25 01:03:34" id="11733" opendate="2015-09-04 07:07:36" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>UDF GenericUDFReflect cannot find classes added by &quot;ADD JAR&quot;</summary>
			
			
			<description>When run below command:

hive -e &quot;add jar /root/hive/TestReflect.jar; \
select reflect(&amp;amp;apos;com.yshi.hive.TestReflect&amp;amp;apos;, &amp;amp;apos;testReflect&amp;amp;apos;, code) from sample_07 limit 3&quot;
Get below error:

Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate



The full stack trace is:

15/09/04 07:00:37 [main]: INFO compress.CodecPool: Got brand-new decompressor [.bz2]

Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate

15/09/04 07:00:37 [main]: ERROR CliDriver: Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate

java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate

	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:152)

	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1657)

	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)

	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)

	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)

	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)

	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)

	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:606)

	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: UDFReflect evaluate

	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.evaluate(GenericUDFReflect.java:107)

	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:185)

	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)

	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)

	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:77)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)

	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:424)

	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:416)

	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)

	... 13 more

Caused by: java.lang.ClassNotFoundException: com.yshi.hive.TestReflect

	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)

	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)

	at java.security.AccessController.doPrivileged(Native Method)

	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)

	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)

	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)

	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)

	at java.lang.Class.forName0(Native Method)

	at java.lang.Class.forName(Class.java:190)

	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.evaluate(GenericUDFReflect.java:105)

	... 22 more


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.Utils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.HCatSplit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.JavaUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.templeton.tool.JobState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.WriterImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9486</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-30 17:47:00" id="12280" opendate="2015-10-28 00:47:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveConnection does not try other HS2 after failure for service discovery</summary>
			
			
			<description>Found this while mocking some bad connection data in znode.. will try to add a test for this.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11581</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-30 18:45:22" id="12248" opendate="2015-10-23 19:50:20" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>The rawStore used in DBTokenStore should be thread-safe</summary>
			
			
			<description>A non-thread-safe implementation of RawStore, particularly ObjectStore, set in DBTokenStore is being shared by multi-threads, which causes the race condition in DataNuclues to access the backend DB. 
The DN PersistenceManager(PM) in ObjectStore is not thread safe, so DBTokenStore should use a ThreadLocal ObjectStore.
Following errors might be root caused by the race condition in DN PM.



Object of type &quot;org.apache.hadoop.hive.metastore.model.MDelegationToken&quot; is detached. Detached objects cannot be used with this operation.

org.datanucleus.exceptions.ObjectDetachedException: Object of type &quot;org.apache.hadoop.hive.metastore.model.MDelegationToken&quot; is detached. Detached objects cannot be used with this operation.

at org.datanucleus.ExecutionContextImpl.assertNotDetached(ExecutionContextImpl.java:5728)

at org.datanucleus.ExecutionContextImpl.retrieveObject(ExecutionContextImpl.java:1859)

at org.datanucleus.ExecutionContextThreadedImpl.retrieveObject(ExecutionContextThreadedImpl.java:203)

at org.datanucleus.api.jdo.JDOPersistenceManager.jdoRetrieve(JDOPersistenceManager.java:605)

at org.datanucleus.api.jdo.JDOPersistenceManager.retrieveAll(JDOPersistenceManager.java:693)

at org.datanucleus.api.jdo.JDOPersistenceManager.retrieveAll(JDOPersistenceManager.java:713)

at org.apache.hadoop.hive.metastore.ObjectStore.getAllTokenIdentifiers(ObjectStore.java:6517) 


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			
			
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.DBTokenStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.TestDBTokenStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11616</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-30 21:53:30" id="11616" opendate="2015-08-21 09:00:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>DelegationTokenSecretManager reuses the same objectstore, which has concurrency issues</summary>
			
			
			<description>sometime in metastore log, will get below exception,  after analysis, we found that :
when hivemetastore start, the DelegationTokenSecretManager will maintain the same objectstore, see here



saslServer.startDelegationTokenSecretManager(conf, *baseHandler.getMS()*, ServerMode.METASTORE);



this lead to the cocurrent issue.



2015-08-18 20:59:10,520 | ERROR | pool-6-thread-200 | Error occurred during processing of message. | org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:296)

org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started

	at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnRawStore(DBTokenStore.java:154)

	at org.apache.hadoop.hive.thrift.DBTokenStore.getToken(DBTokenStore.java:88)

	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.retrievePassword(TokenStoreDelegationTokenSecretManager.java:112)

	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.retrievePassword(TokenStoreDelegationTokenSecretManager.java:56)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$SaslDigestCallbackHandler.getPassword(HadoopThriftAuthBridge.java:565)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$SaslDigestCallbackHandler.handle(HadoopThriftAuthBridge.java:596)

	at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:589)

	at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244)

	at org.apache.thrift.transport.TSaslTransport$SaslParticipant.evaluateChallengeOrResponse(TSaslTransport.java:539)

	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:283)

	at org.apache.thrift.transport.HiveTSaslServerTransport.open(HiveTSaslServerTransport.java:133)

	at org.apache.thrift.transport.HiveTSaslServerTransport$Factory.getTransport(HiveTSaslServerTransport.java:261)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory$1.run(HadoopThriftAuthBridge.java:739)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory$1.run(HadoopThriftAuthBridge.java:736)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:360)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1652)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory.getTransport(HadoopThriftAuthBridge.java:736)

	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started

	at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)

	at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)

	at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)

	at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)

	at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:420)

	at org.apache.hadoop.hive.metastore.ObjectStore.getToken(ObjectStore.java:6455)

	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)

	at com.sun.proxy.$Proxy4.getToken(Unknown Source)

	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnRawStore(DBTokenStore.java:146)

	... 21 more


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			
			
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.DBTokenStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.TestDBTokenStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12248</link>
			
			
			<link description="relates to" type="Reference">12270</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-30 22:05:35" id="12249" opendate="2015-10-23 20:56:25" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Improve logging with tez</summary>
			
			
			<description>We need to improve logging across the board. TEZ-2851 added a caller context so that one can correlate logs with the application. This jira adds a new configuration for users that can be used to correlate the logs.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">9184</link>
			
			
			<link description="relates to" type="Reference">12419</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-30 22:32:46" id="12232" opendate="2015-10-22 12:04:16" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Create external table failed when enabled StorageBasedAuthorization</summary>
			
			
			<description>Please look at the stacktrace, when enabled StorageBasedAuthorization, creating external table will failed with write permission about the default warehouse path &quot;/user/hive/warehouse&quot;: 
&amp;gt; CREATE EXTERNAL TABLE test(id int) LOCATION &amp;amp;apos;/tmp/wangmeng/test&amp;amp;apos;  ;
Error: Error while compiling statement: FAILED: HiveException java.security.AccessControlException: Permission denied: user=wangmeng, access=WRITE, inode=&quot;/user/hive/warehouse&quot;:hive:hive:drwxr-x--t.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12231</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-03 04:11:43" id="12238" opendate="2015-10-23 01:10:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: Thread-safety errors in VectorUDFDate</summary>
			
			
			<description>


Caused by: java.lang.NumberFormatException: For input string: &quot;&quot;

        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)

        at java.lang.Long.parseLong(Long.java:601)

        at java.lang.Long.parseLong(Long.java:631)

        at java.text.DigitList.getLong(DigitList.java:195)

        at java.text.DecimalFormat.parse(DecimalFormat.java:2051)

        at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869)

        at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514)        at java.text.DateFormat.parse(DateFormat.java:364)

        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString$1.evaluate(VectorUDFDateString.java:48)

        at org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.evaluate(StringUnaryUDF.java:90)        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorExpression.java:121)

        at org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatStringScalar.evaluate(StringGroupColConcatStringScalar.java:50)

        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorExpression.java:121)

        at org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.evaluate(VectorUDFAdaptor.java:112)

        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorExpression.java:121)

        at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.evaluate(VectorUDFTimestampFieldLong.java:93)

        at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:123)

        ... 22 more


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-03 09:11:59" id="12093" opendate="2015-10-12 09:45:32" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary> launch local task to process map join cost long time </summary>
			
			
			<description> launch local task to process map join cost long time   
2015-10-08 19:34:35 INFO 2015-10-08 19:34:35  Starting to launch local task to process map join;  maximum memory = 1908932608
2015-10-08 20:07:43 INFO 2015-10-08 20:07:43  Dump the side-table for tag: 1 with group count: 148024 into file: file:/tmp/test/6b99a4b8-0db3-4c62-a0f3-20547504b2b4/hive_2015-10-08_19-30-11_948_5184081524408167915-1/-local-10015/HashTable-Stage-33/MapJoin-mapfile71--.hashtable
2015-10-08 20:07:43 INFO 2015-10-08 20:07:43  Uploaded 1 File to: file:/tmp/test/6b99a4b8-0db3-4c62-a0f3-20547504b2b4/hive_2015-10-08_19-30-11_948_5184081524408167915-1/-local-10015/HashTable-Stage-33/MapJoin-mapfile71--.hashtable (8922201 bytes)
2015-10-08 20:07:43 INFO 2015-10-08 20:07:43  End of local task; Time Taken: 1987.642 sec.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11502</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-06 13:59:39" id="12346" opendate="2015-11-05 13:49:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Internally used variables in HiveConf should not be settable via command</summary>
			
			
			<description>Some HiveConf variables such as hive.added.jars.path are only for internal use and should not be settable via set command. 
We saw a lot of cases that users mistakenly set these variables using set command despite some of them have been documented as &quot;internal parameter&quot; in Hive. The command usually succeeds but it sometimes does not effect, which causes some confusions. For example, the hive.added.jars.path can be set via set command but it is sometimes overridden by session resource jars during runtime.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-07 19:17:57" id="12263" opendate="2015-10-26 03:16:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive SchemaTool does not tolerate leading spaces in JDBC url</summary>
			
			
			<description>With configuration as below:
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;
javax.jdo.option.ConnectionURL
&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;
jdbc:mysql://host/hive?createDatabaseIfNotExist=true
&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
SchemaTool will failed:
export HIVE_CONF_DIR=$HIVE_CONF_DIR; schematool -dbType mysql -userName hive -passWord xxx -initSchema -verbose
exception as below:
Metastore connection URL:
jdbc:mysql://host/hive?createDatabaseIfNotExist=true
Metastore Connection Driver : com.mysql.jdbc.Driver
Metastore connection User: hive
org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.
org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.
at org.apache.hive.beeline.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:77)
at org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(HiveSchemaTool.java:113)
at org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore(HiveSchemaTool.java:159)
at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:257)
at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:243)
at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:473)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.sql.SQLException: No suitable driver found for
jdbc:mysql://host/hive?createDatabaseIfNotExist=true
at java.sql.DriverManager.getConnection(DriverManager.java:689)
at java.sql.DriverManager.getConnection(DriverManager.java:247)
at org.apache.hive.beeline.HiveSchemaHelper.getConnectionToMetastore(HiveSchemaHelper.java:73)
... 11 more</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.beeline.HiveSchemaHelper.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11287</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-09 01:21:48" id="12311" opendate="2015-10-31 15:18:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>explain CTAS fails if the table already exists</summary>
			
			
			<description>Explain of a CTAS will fail if the table already exists.
This is an annoyance when you&amp;amp;apos;re seeing if a large body of SQL queries will function by putting explain in front of every query. 



hive&amp;gt; create table temp (x int);

OK

Time taken: 0.252 seconds

hive&amp;gt; create table temp2 (x int);

OK

Time taken: 0.407 seconds

hive&amp;gt; explain create table temp as select * from temp2;

FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Table already exists: mydb.temp



If we compare to Postgres &quot;The Zinc Standard of SQL Compliance&quot;:



carter=# create table temp (x int);

CREATE TABLE

carter=# create table temp2 (x int);

CREATE TABLE

carter=# explain create table temp as select * from temp2;

                       QUERY PLAN

---------------------------------------------------------

 Seq Scan on temp2  (cost=0.00..34.00 rows=2400 width=4)

(1 row)



If the CTAS is something complex it would be nice to see the query plan in advance.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-09 18:27:46" id="12312" opendate="2015-11-01 03:48:37" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Excessive logging in PPD code</summary>
			
			
			<description>One of my very complex queries takes about 14 minutes to compile with PPD on. Profiling it I saw a lot of time spent in this stack which is called many many thousands of times.



java.lang.Throwable.getStackTraceElement(-2)

java.lang.Throwable.getOurStackTrace(827)

java.lang.Throwable.getStackTrace(816)

sun.reflect.GeneratedMethodAccessor5.invoke(-1)

sun.reflect.DelegatingMethodAccessorImpl.invoke(43)

java.lang.reflect.Method.invoke(497)

org.apache.log4j.spi.LocationInfo.&amp;lt;init&amp;gt;(139)

org.apache.log4j.spi.LoggingEvent.getLocationInformation(253)

org.apache.log4j.helpers.PatternParser$LocationPatternConverter.convert(500)

org.apache.log4j.helpers.PatternConverter.format(65)

org.apache.log4j.PatternLayout.format(506)

org.apache.log4j.WriterAppender.subAppend(310)

org.apache.log4j.DailyRollingFileAppender.subAppend(369)

org.apache.log4j.WriterAppender.append(162)

org.apache.log4j.AppenderSkeleton.doAppend(251)

org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(66)

org.apache.log4j.Category.callAppenders(206)

org.apache.log4j.Category.forcedLog(391)

org.apache.log4j.Category.log(856)

org.apache.commons.logging.impl.Log4JLogger.info(176)

org.apache.hadoop.hive.ql.ppd.OpProcFactory$DefaultPPD.logExpr(707)

org.apache.hadoop.hive.ql.ppd.OpProcFactory$DefaultPPD.mergeWithChildrenPred(752)

org.apache.hadoop.hive.ql.ppd.OpProcFactory$FilterPPD.process(437)



logExpr is set to log at INFO level, but I think DEBUG is more appropriate. When I set log level to debug I see &amp;gt; 20% speedup in compile time:
Before:



real    14m47.972s

user    15m25.609s

sys    0m20.282s



After:



real    11m30.946s

user    12m10.870s

sys    0m7.320s



It looks like there&amp;amp;apos;s a lot of stuff in the PPD code that could be optimized, when I turn PPD off the query compiles in 2m 30s. But this seems like an easy and low risk win.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-11 23:54:51" id="12365" opendate="2015-11-07 15:30:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Added resource path is sent to cluster as an empty string when externally removed</summary>
			
			
			<description>Sometimes the resources (e.g. jar) added via command like &quot;add jars &amp;lt;filepath&amp;gt;&quot; are removed externally from their filepath for some reasons. Their paths are sent to cluster as empty strings which causes the failures to the query that even do not need these jars in execution. The error look like as following:



15/11/06 21:56:44 INFO mapreduce.JobSubmitter: Cleaning up the staging area file:/tmp/hadoop-ctang/mapred/staging/ctang734817191/.staging/job_local734817191_0003

java.lang.IllegalArgumentException: Can not create a Path from an empty string

	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:127)

	at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:135)

	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:215)

	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:390)

	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:483)

	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)

	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-24 01:04:15" id="12432" opendate="2015-11-17 09:28:18" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hive on Spark Counter &quot;RECORDS_OUT&quot; always  be zero</summary>
			
			
			<description>A simple way to reproduce :
set hive.execution.engine=spark;
CREATE TABLE  test(id INT);
insert into test values (1) ,(2);</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12466</link>
			
			
			<link description="relates to" type="Reference">12382</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-25 03:04:46" id="12466" opendate="2015-11-19 12:29:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>SparkCounter not initialized error</summary>
			
			
			<description>During a query, lots of the following error found in executor&amp;amp;apos;s log:

03:47:28.759 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.

03:47:28.762 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_0] has not initialized before.

03:47:30.707 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.tmp_tmp] has not initialized before.

03:47:33.385 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.

03:47:33.388 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.

03:47:33.495 [Executor task launch worker-0] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.

03:47:35.141 [Executor task launch worker-1] ERROR org.apache.hive.spark.counter.SparkCounters - counter[HIVE, RECORDS_OUT_1_default.test_table] has not initialized before.



...........


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12432</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-25 09:09:23" id="12463" opendate="2015-11-19 08:13:49" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>VectorMapJoinFastKeyStore has Array OOB errors</summary>
			
			
			<description>When combining different sized keys, observing an occasional error in hashtable probes.



Caused by: java.lang.ArrayIndexOutOfBoundsException: 162046429

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.equalKey(VectorMapJoinFastKeyStore.java:150)

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.findReadSlot(VectorMapJoinFastBytesHashTable.java:191)

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.lookup(VectorMapJoinFastBytesHashMap.java:76)

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.process(VectorMapJoinInnerMultiKeyOperator.java:300)

	... 26 more






    // Our reading is positioned to the key.

    writeBuffers.getByteSegmentRefToCurrent(byteSegmentRef, keyLength, readPos);



    byte[] currentBytes = byteSegmentRef.getBytes();

    int currentStart = (int) byteSegmentRef.getOffset();



    for (int i = 0; i &amp;lt; keyLength; i++) {

      if (currentBytes[currentStart + i] != keyBytes[keyStart + i]) {

        // LOG.debug(&quot;VectorMapJoinFastKeyStore equalKey no match on bytes&quot;);

        return false;

      }

    }



This needs an identical fix to match 



    // Rare case of buffer boundary. Unfortunately we&amp;amp;apos;d have to copy some bytes.



   // Rare case of buffer boundary. Unfortunately we&amp;amp;apos;d have to copy some bytes.

    byte[] bytes = new byte[length];

    int destOffset = 0;

    while (destOffset &amp;lt; length) {

      ponderNextBufferToRead(readPos);


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-01 05:38:38" id="12184" opendate="2015-10-15 06:20:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>DESCRIBE of fully qualified table fails when db and table name match and non-default database is in use</summary>
			
			
			<description>DESCRIBE of fully qualified table fails when db and table name match and non-default database is in use.
Repro:



: jdbc:hive2://localhost:10000/default&amp;gt; create database foo;

No rows affected (0.116 seconds)

0: jdbc:hive2://localhost:10000/default&amp;gt; create table foo.foo(i int);



0: jdbc:hive2://localhost:10000/default&amp;gt; describe foo.foo;

+-----------+------------+----------+--+

| col_name  | data_type  | comment  |

+-----------+------------+----------+--+

| i         | int        |          |

+-----------+------------+----------+--+

1 row selected (0.049 seconds)



0: jdbc:hive2://localhost:10000/default&amp;gt; use foo;



0: jdbc:hive2://localhost:10000/default&amp;gt; describe foo.foo;

Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Error in getting fields from serde.Invalid Field foo (state=08S01,code=1)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11241</link>
			
			
			<link description="is duplicated by" type="Duplicate">11261</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-02 00:58:04" id="11241" opendate="2015-07-13 21:29:06" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Database prefix does not work properly if table has same name</summary>
			
			
			<description>If you do the following it will fail: 



0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; create database test4; 

No rows affected (0.881 seconds) 

0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; use test4; 

No rows affected (0.1 seconds) 

0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; create table test4 (c1 char(200)); 

No rows affected (0.306 seconds) 

0: jdbc:hive2://cdh54-1.test.com:10000/defaul&amp;gt; desc test4.test4; 

Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. cannot find field test4 from [0:c1] (state=08S01,code=1)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12184</link>
			
			
			<link description="is related to" type="Reference">11261</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-04 18:26:39" id="12444" opendate="2015-11-17 22:04:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Global Limit optimization on ACID table without base directory may throw exception</summary>
			
			
			<description>Steps to reproduce:
set hive.fetch.task.conversion=minimal;
set hive.limit.optimize.enable=true;
create table acidtest1(
 c_custkey int,
 c_name string,
 c_nationkey int,
 c_acctbal double)
clustered by (c_nationkey) into 3 buckets
stored as orc
tblproperties(&quot;transactional&quot;=&quot;true&quot;);
insert into table acidtest1
select c_custkey, c_name, c_nationkey, c_acctbal from tpch_text_10.customer;
select cast (c_nationkey as string) from acidtest.acidtest1 limit 10;



DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1447362491939_0020_1_00, diagnostics=[Vertex vertex_1447362491939_0020_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: acidtest1 initializer failed, vertex=vertex_1447362491939_0020_1_00 [Map 1], java.lang.RuntimeException: serious problem

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1035)

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1062)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:308)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:410)

	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:246)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:240)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:240)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:227)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: delta_0000017_0000017 does not start with base_

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)

	at java.util.concurrent.FutureTask.get(FutureTask.java:192)

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1012)

	... 15 more

Caused by: java.lang.IllegalArgumentException: delta_0000017_0000017 does not start with base_

	at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)

	at org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(AcidUtils.java:172)

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:667)

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:625)

	... 4 more

]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-04 19:44:00" id="12505" opendate="2015-11-24 04:00:04" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Insert overwrite in same encrypted zone silently fails to remove some existing files</summary>
			
			
			<description>With HDFS Trash enabled but its encryption zone lower than Hive data directory, insert overwrite command silently fails to trash the existing files during overwrite, which could lead to unexpected incorrect results (more rows returned than expected)</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-14 20:59:41" id="12473" opendate="2015-11-19 23:33:13" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>DPP: UDFs on the partition column side does not evaluate correctly</summary>
			
			
			<description>Related to HIVE-12462





select count(1) from accounts a, transactions t where year(a.dt) = year(t.dt) and account_id = 22;



$hdt$_0:$hdt$_1:a

  TableScan (TS_2)

    alias: a

    filterExpr: (((account_id = 22) and year(dt) is not null) and (year(dt)) IN (RS[6])) (type: boolean)



Ends up being evaluated as year(cast(dt as int)) because the pruner only checks for final type, not the column type.



    ObjectInspector oi =

        PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(TypeInfoFactory

            .getPrimitiveTypeInfo(si.fieldInspector.getTypeName()));



    Converter converter =

        ObjectInspectorConverters.getConverter(

            PrimitiveObjectInspectorFactory.javaStringObjectInspector, oi);


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.DynamicPruningEventDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12667</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-17 00:51:25" id="12610" opendate="2015-12-07 22:52:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hybrid Grace Hash Join should fail task faster if processing first batch fails, instead of continuing processing the rest</summary>
			
			
			<description>During processing the spilled partitions, if there&amp;amp;apos;s any fatal error, such as Kryo exception, then we should exit early, instead of moving on to process the rest of spilled partitions.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-27 19:31:20" id="12742" opendate="2015-12-24 01:02:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NULL table comparison within CASE does not work as previous hive versions</summary>
			
			
			<description>drop table test_1; 
create table test_1 (id int, id2 int); 
insert into table test_1 values (123, NULL);
SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b 
FROM test_1; 
--NULL
But the output should be true (confirmed with postgres.)</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13942</link>
			
			
			<link description="relates to" type="Reference">12751</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-29 18:47:23" id="12744" opendate="2015-12-24 07:42:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>GROUPING__ID failed to be recognized in multiple insert</summary>
			
			
			<description>When using multiple insert with multiple group by, grouping__id will failed to be parse.
hive&amp;gt; create temporary table testtable3 (id string, name string);
OK
Time taken: 1.019 seconds
hive&amp;gt; create temporary table testtable2 (id string, name string);
OK
Time taken: 0.069 seconds
hive&amp;gt; create temporary table testtable1 (id string, name string);
OK
Time taken: 0.066 seconds
hive&amp;gt; insert into table testtable1 values (&quot;id&quot;, &quot;2333&quot;);
...
OK
Time taken: 32.515 seconds
hive&amp;gt; from testtable1
    &amp;gt; insert into table testtable2 select
    &amp;gt;     id, GROUPING__ID
    &amp;gt; group by id, name with cube;
...
OK
Time taken: 42.032 seconds
hive&amp;gt; from testtable1
    &amp;gt; insert into table testtable2 select
    &amp;gt;     id, GROUPING__ID
    &amp;gt; group by id, name with cube
    &amp;gt; insert into table testtable3 select
    &amp;gt;     id, name
    &amp;gt; group by id, name grouping sets ((id), (id, name));
FAILED: SemanticException [Error 10025]: Line 3:8 Expression not in GROUP BY key &amp;amp;apos;GROUPING__ID&amp;amp;apos;</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-12 18:30:10" id="12785" opendate="2016-01-05 21:55:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>View with union type and UDF to `cast` the struct is broken</summary>
			
			
			<description>Unfortunately HIVE-12156 is breaking the following use case:
I do have a table with a uniontype of struct s, such as:



CREATE TABLE `minimal_sample`(

  `record_type` string,

  `event` uniontype&amp;lt;struct&amp;lt;string_value:string&amp;gt;,struct&amp;lt;int_value:int&amp;gt;&amp;gt;)



In my case, the table comes from an Avro schema which looks like: 

  &amp;amp;apos;avro.schema.literal&amp;amp;apos;=&amp;amp;apos;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;Minimal\&quot;,\&quot;namespace\&quot;:\&quot;org.ver.vkanalas.minimalsamp\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;record_type\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;event\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;a\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;string_value\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;b\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;int_value\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}]}]}&amp;amp;apos;



I wrote custom UDF (source attached) to cast the union type to one of the struct to access nested elements, such as int_value in my example.



CREATE FUNCTION toSint AS &amp;amp;apos;org.ver.udf.minimal.StructFromUnionMinimalB&amp;amp;apos;;



A simple query with the UDF is working fine. But creating a view with the same select is failing when I&amp;amp;apos;m trying to query it:



CREATE OR REPLACE VIEW minimal_sample_viewB AS SELECT toSint(event).int_value FROM minimal_sample WHERE record_type = &amp;amp;apos;B&amp;amp;apos;;



SELECT * FROM minimal_sample_viewB;



The stack trace is posted below.
I did try to revert (or exclude) HIVE-12156 from the version I&amp;amp;apos;m running and this use case is working fine.



FAILED: SemanticException Line 0:-1 . Operator is only supported on struct or list of struct types &amp;amp;apos;int_value&amp;amp;apos; in definition of VIEW minimal_sample_viewb [

SELECT null.`int_value` FROM `default`.`minimal_sample` WHERE `minimal_sample`.`record_type` = &amp;amp;apos;B&amp;amp;apos;

] used as minimal_sample_viewb at Line 3:14

16/01/05 22:49:41 [main]: ERROR ql.Driver: FAILED: SemanticException Line 0:-1 . Operator is only supported on struct or list of struct types &amp;amp;apos;int_value&amp;amp;apos; in definition of VIEW minimal_sample_viewb [

SELECT null.`int_value` FROM `default`.`minimal_sample` WHERE `minimal_sample`.`record_type` = &amp;amp;apos;B&amp;amp;apos;

] used as minimal_sample_viewb at Line 3:14

org.apache.hadoop.hive.ql.parse.SemanticException: Line 0:-1 . Operator is only supported on struct or list of struct types &amp;amp;apos;int_value&amp;amp;apos; in definition of VIEW minimal_sample_viewb [

SELECT null.`int_value` FROM `default`.`minimal_sample` WHERE `minimal_sample`.`record_type` = &amp;amp;apos;B&amp;amp;apos;

] used as minimal_sample_viewb at Line 3:14

	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:893)

	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1321)

	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)

	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:209)

	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:153)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10500)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10455)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3822)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3601)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8943)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8898)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9743)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9623)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9650)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9636)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10109)

	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:329)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10120)

	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)

	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)

	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)

	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:314)

	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1164)

	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1212)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1091)

	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)

	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)

	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)

	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:314)

	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:412)

	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:428)

	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:717)

	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)

	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">12156</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-19 20:27:56" id="12682" opendate="2015-12-15 22:13:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Reducers in dynamic partitioning job spend a lot of time running hadoop.conf.Configuration.getOverlay</summary>
			
			
			<description>I tested this on Hive 1.2.1 but looks like it&amp;amp;apos;s still applicable to 2.0.
I ran this query:



create table flights (



)

PARTITIONED BY (Year int)

CLUSTERED BY (Month)

SORTED BY (DayofMonth) into 12 buckets

STORED AS ORC

TBLPROPERTIES(&quot;orc.bloom.filter.columns&quot;=&quot;*&quot;)

;



(Taken from here: https://github.com/t3rmin4t0r/all-airlines-data/blob/master/ddl/orc.sql)
I profiled just the reduce phase and noticed something odd, the attached graph shows where time was spent during the reducer phase.

Problem seems to relate to https://github.com/apache/hive/blob/branch-2.0/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L903
/cc Gopal V</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-25 18:36:08" id="12867" opendate="2016-01-13 21:28:40" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Semantic Exception Error Msg should be with in the range of &quot;10000 to 19999&quot;</summary>
			
			
			<description>At many places errors encountered during semantic exception is translated as generic error(GENERIC_ERROR, 40000) msg as opposed to semantic error msg.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-12 19:08:25" id="13017" opendate="2016-02-05 23:40:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Child process of HiveServer2 fails to get delegation token from non default FileSystem</summary>
			
			
			<description>The following query fails, when Azure Filesystem is used as default file system, and HDFS is used for intermediate data.

&amp;gt;&amp;gt;&amp;gt;  create temporary table s10k stored as orc as select * from studenttab10k;

&amp;gt;&amp;gt;&amp;gt;  create temporary table v10k as select * from votertab10k;

&amp;gt;&amp;gt;&amp;gt;  select registration 

from s10k s join v10k v 

on (s.name = v.name) join studentparttab30k p 

on (p.name = v.name) 

where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;

ERROR : Execution failed with exit status: 2

ERROR : Obtaining error information

ERROR : 

Task failed!

Task ID:

  Stage-5



Logs:



ERROR : /var/log/hive/hiveServer2.log

Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)

Aborting command set because &quot;force&quot; is false and command failed: &quot;select registration 

from s10k s join v10k v 

on (s.name = v.name) join studentparttab30k p 

on (p.name = v.name) 

where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;&quot;

Closing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice

hiveServer2.log shows:

2016-02-02 18:04:34,182 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,199 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,212 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie

2016-02-02 18:04:34,213 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:checkConcurrency(168)) - Concurrency mode is disabled, not creating a lock manager

2016-02-02 18:04:34,219 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal

2016-02-02 18:04:34,219 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,225 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1390)) - Setting caller context to query id hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0

2016-02-02 18:04:34,226 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1393)) - Starting command(queryId=hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0): select registration

from s10k s join v10k v

on (s.name = v.name) join studentparttab30k p

on (p.name = v.name)

where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25

2016-02-02 18:04:34,228 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:&amp;lt;init&amp;gt;(90)) - Created ATS Hook

2016-02-02 18:04:34,229 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,237 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa

2016-02-02 18:04:34,238 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436274229 end=1454436274238 duration=9 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,239 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,240 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook start=1454436274239 end=1454436274240 duration=1 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

Query ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0

2016-02-02 18:04:34,242 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Query ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0

Total jobs = 1

2016-02-02 18:04:34,243 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Total jobs = 1

2016-02-02 18:04:34,245 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=TimeToSubmit start=1454436274199 end=1454436274245 duration=46 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,246 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,247 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=task.MAPREDLOCAL.Stage-5 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:34,258 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:launchTask(1718)) - Starting task [Stage-5:MAPREDLOCAL] in serial mode

2016-02-02 18:04:34,280 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(158)) - Generating plan file file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml

2016-02-02 18:04:34,288 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;

2016-02-02 18:04:34,289 INFO  [HiveServer2-Background-Pool: Thread-517]: exec.Utilities (Utilities.java:serializePlan(1028)) - Serializing MapredLocalWork via kryo

2016-02-02 18:04:34,290 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPreHookEvent(158)) - Received pre-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0

2016-02-02 18:04:34,358 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=serializePlan start=1454436274288 end=1454436274358 duration=70 from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;

2016-02-02 18:04:34,737 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(287)) - Executing: /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop jar /usr/hdp/2.4.1.0-170/hive/lib/hive-common-1.2.1000.2.4.1.0-170.jar org.apache.hadoop.hive.ql.exec.mr.ExecDriver -localtask -plan file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml   -jobconffile file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10007/jobconf.xml

WARNING: Use &quot;yarn jar&quot; to launch YARN applications.

2016-02-02 18:04:37,450 INFO  [org.apache.ranger.audit.queue.AuditBatchQueue0]: provider.BaseAuditHandler (BaseAuditHandler.java:logStatus(312)) - Audit Status Log: name=hiveServer2.async.summary.batch.solr, interval=01:21.012 minutes, events=2, succcessCount=2, totalEvents=4, totalSuccessCount=4

Execution log at: /tmp/hive/hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0.log

2016-02-02 18:04:39,248 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie

2016-02-02 18:04:39,254 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal

2016-02-02 18:04:39,261 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa

2016-02-02 18:04:40     Starting to launch local task to process map join;      maximum memory = 477102080

Execution failed with exit status: 2

2016-02-02 18:04:43,728 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Execution failed with exit status: 2

Obtaining error information

2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Obtaining error information



Task failed!

Task ID:

  Stage-5



Logs:



2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) -

Task failed!

Task ID:

  Stage-5



Logs:



/var/log/hive/hiveServer2.log

2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - /var/log/hive/hiveServer2.log

2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(307)) - Execution failed with exit status: 2

2016-02-02 18:04:43,733 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:&amp;lt;init&amp;gt;(90)) - Created ATS Hook

2016-02-02 18:04:43,734 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:43,736 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436283734 end=1454436283736 duration=2 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:43,736 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPostHookEvent(193)) - Received post-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask

2016-02-02 18:04:43,757 ERROR [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printError(932)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask

2016-02-02 18:04:43,758 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1621)) - Resetting the caller context to HIVE_SSN_ID:916e3dbb-a10d-4888-a063-52fb058ea421

2016-02-02 18:04:43,759 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=Driver.execute start=1454436274219 end=1454436283759 duration=9540 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:43,760 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &amp;lt;PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:43,761 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &amp;lt;/PERFLOG method=releaseLocks start=1454436283760 end=1454436283761 duration=1 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:

org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask

        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)

        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)

        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)

        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:415)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)

        at java.util.concurrent.FutureTask.run(FutureTask.java:262)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        at java.lang.Thread.run(Thread.java:745)

hive Configs can be viewed from http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/artifacts/tmpModifyConfDir_1454394513592/

Attachments

Drop files to attach, or browse.

Add LinkIssue Links

relates to

Bug - A problem which impairs or prevents the functions of the product. HIVE-739 webhcat tests failing in HDInsight secure cluster throwing NullPointerException	 Blocker - Blocks development and/or testing work, production could not run. RESOLVED

Activity

All

Comments

Work Log

History

Activity

Ascending order - Click to sort in descending order

Permalink Edit Delete 

tsaito Takahiko Saito added a comment - 3 days ago

The test passes via hive CLI and explain shows:

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; explain select registration

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; from s10k s join v10k v

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (s.name = v.name) join studentparttab30k p

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (p.name = v.name)

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;

+--------------------------------------------------------------------------------------------------------------+--+

|                                                   Explain                                                    |

+--------------------------------------------------------------------------------------------------------------+--+

| STAGE DEPENDENCIES:                                                                                          |

|   Stage-5 is a root stage                                                                                    |

|   Stage-4 depends on stages: Stage-5                                                                         |

|   Stage-0 depends on stages: Stage-4                                                                         |

|                                                                                                              |

| STAGE PLANS:                                                                                                 |

|   Stage: Stage-5                                                                                             |

|     Map Reduce Local Work                                                                                    |

|       Alias -&amp;gt; Map Local Tables:                                                                             |

|         s                                                                                                    |

|           Fetch Operator                                                                                     |

|             limit: -1                                                                                        |

|         v                                                                                                    |

|           Fetch Operator                                                                                     |

|             limit: -1                                                                                        |

|       Alias -&amp;gt; Map Local Operator Tree:                                                                      |

|         s                                                                                                    |

|           TableScan                                                                                          |

|             alias: s                                                                                         |

|             filterExpr: (name is not null and (age &amp;lt; 25)) (type: boolean)                                    |

|             Statistics: Num rows: 459 Data size: 47777 Basic stats: COMPLETE Column stats: NONE              |

|             Filter Operator                                                                                  |

|               predicate: (name is not null and (age &amp;lt; 25)) (type: boolean)                                   |

|               Statistics: Num rows: 76 Data size: 7910 Basic stats: COMPLETE Column stats: NONE              |

|               HashTable Sink Operator                                                                        |

|                 keys:                                                                                        |

|                   0 name (type: string)                                                                      |

|                   1 name (type: string)                                                                      |

|                   2 name (type: string)                                                                      |

|         v                                                                                                    |

|           TableScan                                                                                          |

|             alias: v                                                                                         |

|             filterExpr: (name is not null and (age &amp;lt; 25)) (type: boolean)                                    |

|             Statistics: Num rows: 1653 Data size: 337233 Basic stats: COMPLETE Column stats: NONE            |

|             Filter Operator                                                                                  |

|               predicate: (name is not null and (age &amp;lt; 25)) (type: boolean)                                   |

|               Statistics: Num rows: 275 Data size: 56103 Basic stats: COMPLETE Column stats: NONE            |

|               HashTable Sink Operator                                                                        |

|                 keys:                                                                                        |

|                   0 name (type: string)                                                                      |

|                   1 name (type: string)                                                                      |

|                   2 name (type: string)                                                                      |

|                                                                                                              |

|   Stage: Stage-4                                                                                             |

|     Map Reduce                                                                                               |

|       Map Operator Tree:                                                                                     |

|           TableScan                                                                                          |

|             alias: p                                                                                         |

|             filterExpr: (name is not null and (age &amp;lt; 25)) (type: boolean)                                    |

|             Statistics: Num rows: 30000 Data size: 627520 Basic stats: COMPLETE Column stats: COMPLETE       |

|             Filter Operator                                                                                  |

|               predicate: (name is not null and (age &amp;lt; 25)) (type: boolean)                                   |

|               Statistics: Num rows: 10000 Data size: 1010000 Basic stats: COMPLETE Column stats: COMPLETE    |

|               Map Join Operator                                                                              |

|                 condition map:                                                                               |

|                      Inner Join 0 to 1                                                                       |

|                      Inner Join 1 to 2                                                                       |

|                 keys:                                                                                        |

|                   0 name (type: string)                                                                      |

|                   1 name (type: string)                                                                      |

|                   2 name (type: string)                                                                      |

|                 outputColumnNames: _col8                                                                     |

|                 Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE      |

|                 Select Operator                                                                              |

|                   expressions: _col8 (type: string)                                                          |

|                   outputColumnNames: _col0                                                                   |

|                   Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE    |

|                   File Output Operator                                                                       |

|                     compressed: false                                                                        |

|                     Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE  |

|                     table:                                                                                   |

|                         input format: org.apache.hadoop.mapred.TextInputFormat                               |

|                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat            |

|                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                            |

|       Local Work:                                                                                            |

|         Map Reduce Local Work                                                                                |

|                                                                                                              |

|   Stage: Stage-0                                                                                             |

|     Fetch Operator                                                                                           |

|       limit: -1                                                                                              |

|       Processor Tree:                                                                                        |

|         ListSink                                                                                             |

|                                                                                                              |

+--------------------------------------------------------------------------------------------------------------+--+

83 rows selected (2.473 seconds)

Permalink Edit Delete 

tsaito Takahiko Saito added a comment - 3 days ago

emptablemisc_8 also fails with the same error:

&amp;gt;&amp;gt;&amp;gt;  create temporary table temp1 as select * from votertab10k;

&amp;gt;&amp;gt;&amp;gt;  select * 

from studenttab10k s 

where s.name not in 

(select name from temp1);

INFO  : Number of reduce tasks determined at compile time: 1

INFO  : In order to change the average load for a reducer (in bytes):

INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;

INFO  : In order to limit the maximum number of reducers:

INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;

INFO  : In order to set a constant number of reducers:

INFO  :   set mapreduce.job.reduces=&amp;lt;number&amp;gt;

INFO  : number of splits:1

INFO  : Submitting tokens for job: job_1454394534358_0164

INFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 246 for hrt_qa)

INFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/

INFO  : Starting Job = job_1454394534358_0164, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/

INFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0164

INFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1

INFO  : 2016-02-02 10:11:02,367 Stage-4 map = 0%,  reduce = 0%

INFO  : 2016-02-02 10:11:26,060 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 5.48 sec

INFO  : 2016-02-02 10:11:39,024 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 11.18 sec

INFO  : MapReduce Total cumulative CPU time: 11 seconds 180 msec

INFO  : Ended Job = job_1454394534358_0164

INFO  : Stage-9 is selected by condition resolver.

INFO  : Stage-1 is filtered out by condition resolver.

ERROR : Execution failed with exit status: 2

ERROR : Obtaining error information

ERROR : 

Task failed!

Task ID:

  Stage-9



Logs:



ERROR : /var/log/hive/hiveServer2.log

INFO  : Number of reduce tasks determined at compile time: 1

INFO  : In order to change the average load for a reducer (in bytes):

INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;

INFO  : In order to limit the maximum number of reducers:

INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;

INFO  : In order to set a constant number of reducers:

INFO  :   set mapreduce.job.reduces=&amp;lt;number&amp;gt;

INFO  : number of splits:2

INFO  : Submitting tokens for job: job_1454394534358_0169

INFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 252 for hrt_qa)

INFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/

INFO  : Starting Job = job_1454394534358_0169, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/

INFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0169

INFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

INFO  : 2016-02-02 10:16:38,027 Stage-1 map = 0%,  reduce = 0%

INFO  : 2016-02-02 10:16:52,498 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 3.8 sec

INFO  : 2016-02-02 10:16:53,566 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.94 sec

INFO  : 2016-02-02 10:17:16,202 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.04 sec

INFO  : MapReduce Total cumulative CPU time: 17 seconds 40 msec

INFO  : Ended Job = job_1454394534358_0169

ERROR : Execution failed with exit status: 2

ERROR : Obtaining error information

ERROR : 

Task failed!

Task ID:

  Stage-8



Logs:



ERROR : /var/log/hive/hiveServer2.log

Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)

Aborting command set because &quot;force&quot; is false and command failed: &quot;select * 

from studenttab10k s 

where s.name not in 

(select name from temp1);&quot;

Closing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice

Its app log can be viewed at http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/app-logs/application_1454394534358_0169.log

Permalink Edit Delete 

tsaito Takahiko Saito added a comment - 3 days ago

The query works without &amp;amp;apos;temporary table&amp;amp;apos;:

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; select registration from studenttab10k s join votertab10k v

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (s.name = v.name) join studentparttab30k p

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; on (p.name = v.name)

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; where s.age &amp;lt; 25 and v.age &amp;lt; 25 and p.age &amp;lt; 25;

Permalink Edit Delete 

tsaito Takahiko Saito added a comment - 3 days ago

More info about temporary table:

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; describe formatted s10k;

+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+

|           col_name            |                                                                  data_type                                                                  |        comment        |

+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+

| # col_name                    | data_type                                                                                                                                   | comment               |

|                               | NULL                                                                                                                                        | NULL                  |

| name                          | string                                                                                                                                      |                       |

| age                           | int                                                                                                                                         |                       |

| gpa                           | double                                                                                                                                      |                       |

|                               | NULL                                                                                                                                        | NULL                  |

| # Detailed Table Information  | NULL                                                                                                                                        | NULL                  |

| Database:                     | default                                                                                                                                     | NULL                  |

| Owner:                        | hrt_qa                                                                                                                                      | NULL                  |

| CreateTime:                   | Tue Feb 02 23:02:31 UTC 2016                                                                                                                | NULL                  |

| LastAccessTime:               | UNKNOWN                                                                                                                                     | NULL                  |

| Protect Mode:                 | None                                                                                                                                        | NULL                  |

| Retention:                    | 0                                                                                                                                           | NULL                  |

| Location:                     | hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c  | NULL                  |

| Table Type:                   | MANAGED_TABLE                                                                                                                               | NULL                  |

|                               | NULL                                                                                                                                        | NULL                  |

| # Storage Information         | NULL                                                                                                                                        | NULL                  |

| SerDe Library:                | org.apache.hadoop.hive.ql.io.orc.OrcSerde                                                                                                   | NULL                  |

| InputFormat:                  | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat                                                                                             | NULL                  |

| OutputFormat:                 | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat                                                                                            | NULL                  |

| Compressed:                   | No                                                                                                                                          | NULL                  |

| Num Buckets:                  | -1                                                                                                                                          | NULL                  |

| Bucket Columns:               | []                                                                                                                                          | NULL                  |

| Sort Columns:                 | []                                                                                                                                          | NULL                  |

| Storage Desc Params:          | NULL                                                                                                                                        | NULL                  |

|                               | serialization.format                                                                                                                        | 1                     |

+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+

26 rows selected (0.22 seconds)

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; set hive.server2.enable.doAs;

+---------------------------------+--+

|               set               |

+---------------------------------+--+

| hive.server2.enable.doAs=false  |

+---------------------------------+--+

Permalink Edit Delete 

tsaito Takahiko Saito added a comment - 3 days ago

hdfs dir of temporary table is owned by hive as expected since hive.server2.enable.doAs=false:

hdfs@hn0-hs21-h:~$ hdfs dfs -ls hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c

Found 1 items

-rwx------   3 hive hdfs      47777 2016-02-02 23:02 hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c/000000_0

Not sure if it&amp;amp;apos;s related to the JIRA, but one thing I noticed was that dfs cmd throws error via beeline:

0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181&amp;gt; dfs -ls hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c;

Error: Error while processing statement: Permission denied: user [hrt_qa] does not have privilege for [DFS] command (state=,code=1)

Permalink Edit 

thejas Thejas Nair added a comment - 5 hours ago - edited

Takahiko Saito 

The above DFS error would be from Ranger of SQL standard authorization. It is not related to the test failure.

But you are right that it looks like a permission issue.

The issue seen in HIVE-739 also exists in this part of Hive. Hive in MR mode launches a new child process to process the small table and create a hash table. This child process needs credentials from HDFS. However, in this setup, azure fs is the default file system. We should also get delegation token from all FS URIs listed under mapreduce.job.hdfs-servers config.

There are hueristics around when map-join gets used. That is why you don&amp;amp;apos;t see it unless temp table is used (that uses ORC format and might have stats as well, while the original table is probaly text format).

https://github.com/hortonworks/hive/blob/2.4-maint/ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java needs change similar to webhcat change in HIVE-739

Permalink Edit 

sush Sushanth Sowmyan added a comment - 7 minutes ago

Takahiko Saito, we should file an apache jira for this as well. Would you like to do so, so the bug report has attribution to you?

Permalink Edit Delete 

tsaito Takahiko Saito added a comment - 4 minutes ago

Sushanth Sowmyan I will file one and update with that JIRA.

 Comment	

People

Assignee:	 sush Sushanth Sowmyan

Assign to me

Reporter:	 tsaito Takahiko Saito

QEAssignee:	Takahiko Saito

Votes:	0

Watchers:	3 Stop watching this issue 

Dates

Created:	3 days ago

Updated:	4 minutes ago

Who&amp;amp;apos;s Looking?



Agile

View on Board



hivesever2 log shows:

2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:

org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask

        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)

        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)

        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)

        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:415)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)

        at java.util.concurrent.FutureTask.run(FutureTask.java:262)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        at java.lang.Thread.run(Thread.java:745)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-16 16:26:48" id="13039" opendate="2016-02-10 21:06:05" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>BETWEEN predicate is not functioning correctly with predicate pushdown on Parquet table</summary>
			
			
			<description>BETWEEN becomes exclusive in parquet table when predicate pushdown is on (as it is by default in newer Hive versions). To reproduce(in a cluster, not local setup):
CREATE TABLE parquet_tbl(
  key int,
  ldate string)
 PARTITIONED BY (
 lyear string )
 ROW FORMAT SERDE
 &amp;amp;apos;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&amp;amp;apos;
 STORED AS INPUTFORMAT
 &amp;amp;apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&amp;amp;apos;
 OUTPUTFORMAT
 &amp;amp;apos;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&amp;amp;apos;;
insert overwrite table parquet_tbl partition (lyear=&amp;amp;apos;2016&amp;amp;apos;) select
  1,
  &amp;amp;apos;2016-02-03&amp;amp;apos; from src limit 1;
set hive.optimize.ppd.storage = true;
set hive.optimize.ppd = true;
select * from parquet_tbl where ldate between &amp;amp;apos;2016-02-03&amp;amp;apos; and &amp;amp;apos;2016-02-03&amp;amp;apos;;
No row will be returned in a cluster.
But if you turn off hive.optimize.ppd, one row will be returned.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.TestParquetRecordReaderWrapper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.FilterPredicateLeafBuilder.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12678</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-17 17:44:43" id="13056" opendate="2016-02-13 01:11:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>delegation tokens do not work with HS2 when used with http transport and kerberos</summary>
			
			
			<description>We&amp;amp;apos;re getting a HiveSQLException on secure windows clusters.



2016-02-08 13:48:09,535|beaver.machine|INFO|6114|140264674350912|MainThread|Job ID : 0000000-160208134528402-oozie-oozi-W

2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------

2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Workflow Name : hive2-wf

2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|App Path      : wasb://oozie1-hbs24@humbtestings5jp.blob.core.windows.net/user/hrt_qa/test_hiveserver2

2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Status        : KILLED

2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Run           : 0

2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|User          : hrt_qa

2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Group         : -

2016-02-08 13:48:09,547|beaver.machine|INFO|6114|140264674350912|MainThread|Created       : 2016-02-08 13:47 GMT

2016-02-08 13:48:09,548|beaver.machine|INFO|6114|140264674350912|MainThread|Started       : 2016-02-08 13:47 GMT

2016-02-08 13:48:09,552|beaver.machine|INFO|6114|140264674350912|MainThread|Last Modified : 2016-02-08 13:48 GMT

2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|Ended         : 2016-02-08 13:48 GMT

2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|CoordAction ID: -

2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|

2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|Actions

2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------

2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|ID                                                                            Status    Ext ID                 Ext Status Err Code

2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------

2016-02-08 13:48:09,571|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@:start:                                  OK        -                      OK         -

2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------

2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@hive-node                                ERROR     -                      ERROR      HiveSQLException

2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------

2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@fail                                     OK        -                      OK         E0729

2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13169</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-18 03:44:11" id="12981" opendate="2016-02-02 12:01:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ThriftCLIService uses incompatible getShortName() implementation</summary>
			
			
			<description>ThriftCLIService has a local implementation getShortName() that assumes a short name is always the part before &quot;@&quot; and &quot;/&quot;. This is not always the case as Kerberos Rules (from Hadoop&amp;amp;apos;s KerberosName) might actually transform a name to something else.
Considering a pending change to getShortName() (#HADOOP-12751) and the normal use of KerberosName in other parts of Hive it only seems logical to use the standard implementation.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 1.2.2, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12751</link>
			
			
			<link description="is related to" type="Reference">15174</link>
			
			
			<link description="breaks" type="Regression">13590</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-19 03:03:17" id="13075" opendate="2016-02-17 15:12:52" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Metastore shuts down when no delegation token is found in ZooKeeper</summary>
			
			
			<description>ZooKeeperTokenStore looks as follows:



@Override

public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {

  byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));

  try {

    return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);

  } catch (Exception ex) {

    throw new TokenStoreException(&quot;Failed to decode token&quot;, ex);

  }

}



which is slightly different from DBTokenStore implementation that is protected against tokenBytes==null because nullable tokenBytes causes NPE to be thrown in HiveDelegationTokenSupport#decodeDelegationTokenInformation
Furthermore, NPE thrown here causes TokenStoreDelegationTokenSecretManager.ExpiredTokenRemover to catch it and exits MetaStore.
null from zkGetData() is possible during ZooKeeper failure or (and that was our case) when another metastore instance removes tokens during ExpiredTokenRemover run. There were two solutions of this problem:

distributed lock in ZooKeeper acquired during one metastore instance&amp;amp;apos;s ExpiredTokenRemover run,
simple null check

I think null check is sufficient if it is in DBTokenStore.
Patch will be attached.
Sorry for an edit but I think worth mentioning is a fact that possible workaround for this issue is setting hive.cluster.delegation.key.update-interval, hive.cluster.delegation.token.renew-interval and hive.cluster.delegation.token.max-lifetime to one year as described here. But in my opinion it is not an engineer-way of doing things </description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13090</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-20 05:16:03" id="13021" opendate="2016-02-08 08:28:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>GenericUDAFEvaluator.isEstimable(agg) always returns false</summary>
			
			
			<description>GenericUDAFEvaluator.isEstimable(agg) always returns false, because annotation AggregationType has default RetentionPolicy.CLASS and cannot be retained by the VM at run time.
As result estimate method will never be executed.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13109</link>
			
			
			<link description="is related to" type="Reference">8188</link>
			
			
			<link description="supercedes" type="Supercedes">13109</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-23 17:31:32" id="13114" opendate="2016-02-22 20:39:11" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>parquet filter fails for type float when hive.optimize.index.filter=true</summary>
			
			
			<description>Hive fails when selecting from a table &amp;amp;apos;stored as parquet&amp;amp;apos; with a row filter based on a float column and hive.optimize.index.filter=true.
The following example fails in hive 1.2.1 (HDP 2.3.2), but works fine in hive 1.1.0 (CDH 5.5.0):



create table p(f float)stored as parquet;

insert into table p values (1), (2), (3);



select * from p where f &amp;gt;= 2;



set hive.optimize.index.filter=true;

select * from p where f &amp;gt;= 2;



The first select query works fine, the second fails with:



Failed with exception java.io.IOException:java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)

Valid types for this column are: [class java.lang.Float]



Here&amp;amp;apos;s the stack trace from log4j:



2016-02-22 12:18:30,691 ERROR [main]: CliDriver (SessionState.java:printError(960)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)

Valid types for this column are: [class java.lang.Float]

java.io.IOException: java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)

Valid types for this column are: [class java.lang.Float]

	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)

	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)

	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)

	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)

	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)

	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)

	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)

	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)

	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)

	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:601)

	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

Caused by: java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FullTypeDescriptor(PrimitiveType: FLOAT, OriginalType: null)

Valid types for this column are: [class java.lang.Float]

	at parquet.filter2.predicate.ValidTypeMap.assertTypeValid(ValidTypeMap.java:132)

	at parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:185)

	at parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:160)

	at parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:124)

	at parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:59)

	at parquet.filter2.predicate.Operators$GtEq.accept(Operators.java:248)

	at parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:64)

	at parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:59)

	at parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:40)

	at parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:126)

	at parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:46)

	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:275)

	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:99)

	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:85)

	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)

	at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)

	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:324)

	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:446)

	... 15 more



</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11504</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-24 19:52:27" id="12808" opendate="2016-01-07 23:42:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Logical PPD: Push filter clauses through PTF(Windowing) into TS</summary>
			
			
			<description>Simplified repro case of HCC #8880, with the slow query showing the push-down miss. 
And the manually rewritten query to indicate the expected one.
Part of the problem could be the window range not being split apart for PPD, but the FIL is not pushed down even if the rownum filter is removed.



create temporary table positions (regionid string, id bigint, deviceid string, ts string);



insert into positions values(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-01&amp;amp;apos;),

(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-01&amp;amp;apos;),

(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-02&amp;amp;apos;),

(&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos;, 1422792010, &amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;, &amp;amp;apos;2016-01-02&amp;amp;apos;);





-- slow query

explain

WITH t1 AS 

( 

         SELECT   *, 

                  Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownos

         FROM     positions ), 

latestposition as ( 

       SELECT * 

       FROM   t1 

       WHERE  rownos = 1) 

SELECT * 

FROM   latestposition 

WHERE  regionid=&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos; 

AND    id=1422792010 

AND    deviceid=&amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;;



-- fast query

explain

WITH t1 AS 

( 

         SELECT   *, 

                  Row_number() over ( PARTITION BY regionid, id, deviceid ORDER BY ts DESC) AS rownos

         FROM     positions 

         WHERE  regionid=&amp;amp;apos;1d6a0be1-6366-4692-9597-ebd5cd0f01d1&amp;amp;apos; 

         AND    id=1422792010 

         AND    deviceid=&amp;amp;apos;6c5d1a30-2331-448b-a726-a380d6b3a432&amp;amp;apos;

),latestposition as ( 

       SELECT * 

       FROM   t1 

       WHERE  rownos = 1) 

SELECT * 

FROM   latestposition 

;


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-01 14:30:23" id="13160" opendate="2016-02-26 00:02:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HS2 unable to load UDFs on startup when HMS is not ready</summary>
			
			
			<description>The error looks like this:



2016-02-18 14:43:54,251 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083

2016-02-18 14:48:54,692 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...

2016-02-18 14:48:54,692 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.

2016-02-18 14:48:55,692 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083

2016-02-18 14:53:55,800 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...

2016-02-18 14:53:55,800 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.

2016-02-18 14:53:56,801 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083

2016-02-18 14:58:56,967 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...

2016-02-18 14:58:56,967 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.

2016-02-18 14:58:57,994 WARN  hive.ql.metadata.Hive: [main]: Failed to register all functions.

java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient

        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)

        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:64)

        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)

        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)

.......

016-02-18 14:58:57,997 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083

2016-02-18 15:03:58,094 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...

2016-02-18 15:03:58,095 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.

2016-02-18 15:03:59,095 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083

2016-02-18 15:08:59,203 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...

2016-02-18 15:08:59,203 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.

2016-02-18 15:09:00,203 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083

2016-02-18 15:14:00,304 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...

2016-02-18 15:14:00,304 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.

2016-02-18 15:14:01,306 INFO  org.apache.hive.service.server.HiveServer2: [main]: Shutting down HiveServer2

2016-02-18 15:14:01,308 INFO  org.apache.hive.service.server.HiveServer2: [main]: Exception caught when calling stop of HiveServer2 before retrying start

java.lang.NullPointerException

        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)

        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)

        at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:69)

        at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:545)



And then none of the functions will be available for use as HS2 does not re-register them after HMS is up and ready.
This is not desired behaviour, we shouldn&amp;amp;apos;t allow HS2 to be in a servicing state if function list is not ready. Or, maybe instead of initialize the function list when HS2 starts, try to load the function list when each Hive session is created. Of course we can have a cache of function list somewhere for better performance, but we would better decouple it from class Hive.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-01 14:48:59" id="13146" opendate="2016-02-24 16:01:58" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>OrcFile table property values are case sensitive</summary>
			
			
			<description>In Hive v1.2.1.2.3, with Tez , create an external table with compression SNAPPY value marked as lower case.  Table is created successfully.  Insert data into table fails with no enum constant error.
CREATE EXTERNAL TABLE mydb.mytable 
(id int)
  PARTITIONED BY (business_date date)
STORED AS ORC
LOCATION
  &amp;amp;apos;/data/mydb/mytable&amp;amp;apos;
TBLPROPERTIES (
  &amp;amp;apos;orc.compress&amp;amp;apos;=&amp;amp;apos;snappy&amp;amp;apos;);
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE mydb.mytable PARTITION (business_date)
SELECT * from mydb.sourcetable;
Caused by: java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.hive.ql.io.orc.CompressionKind.snappy
	at java.lang.Enum.valueOf(Enum.java:238)
	at org.apache.hadoop.hive.ql.io.orc.CompressionKind.valueOf(CompressionKind.java:25)
Constant SNAPPY needs to be uppercase in definition to fix.  Case should be agnostic or throw error on creation of table.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			
			
			<file type="M">org.apache.orc.OrcFile.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-03 06:55:20" id="13108" opendate="2016-02-20 02:52:45" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Operators: SORT BY randomness is not safe with network partitions</summary>
			
			
			<description>SORT BY relies on a transient Random object, which is initialized once per deserialize operation.
This results in complications during a network partition and when Tez/Spark reuses a cached plan.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-04 20:51:38" id="13169" opendate="2016-02-26 06:56:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveServer2: Support delegation token based connection when using http transport</summary>
			
			
			<description>HIVE-5155 introduced support for delegation token based connection. However, it was intended for tcp transport mode. We need to have similar mechanisms for http transport.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">5155</link>
			
			
			<link description="is related to" type="Reference">13056</link>
			
			
			<link description="breaks" type="Regression">13209</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-10 18:19:41" id="13144" opendate="2016-02-24 14:04:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node</summary>
			
			
			<description>When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-24 08:39:20" id="13217" opendate="2016-03-07 18:50:59" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Replication for HoS mapjoin small file needs to respect dfs.replication.max</summary>
			
			
			<description>Currently Hive on Spark Mapjoin replicates small table file to a hard-coded value of 10.  See SparkHashTableSinkOperator.MIN_REPLICATION. 
When dfs.replication.max is less than 10, HoS query fails.  This constant should cap at dfs.replication.max.
Normally dfs.replication.max seems set at 512.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-25 02:23:23" id="12367" opendate="2015-11-09 04:52:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Lock/unlock database should add current database to inputs and outputs of authz hook</summary>
			
			
			<description/>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="breaks" type="Regression">12495</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-27 23:17:30" id="13115" opendate="2016-02-22 21:43:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>MetaStore Direct SQL getPartitions call fail when the columns schemas for a partition are null</summary>
			
			
			<description>We are seeing the following exception in our MetaStore logs

2016-02-11 00:00:19,002 DEBUG metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(602)) - Direct SQL query in 5.842372ms + 1.066728ms, the query is [select &quot;PARTITIONS&quot;.&quot;PART_ID&quot; from &quot;PARTITIONS&quot;  inner join &quot;TBLS&quot; on &quot;PART

ITIONS&quot;.&quot;TBL_ID&quot; = &quot;TBLS&quot;.&quot;TBL_ID&quot;     and &quot;TBLS&quot;.&quot;TBL_NAME&quot; = ?   inner join &quot;DBS&quot; on &quot;TBLS&quot;.&quot;DB_ID&quot; = &quot;DBS&quot;.&quot;DB_ID&quot;      and &quot;DBS&quot;.&quot;NAME&quot; = ?  order by &quot;PART_NAME&quot; asc]

2016-02-11 00:00:19,021 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2243)) - Direct SQL failed, falling back to ORM

MetaException(message:Unexpected null for one of the IDs, SD 6437, column null, serde 6437 for a non- view)

        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)

        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)

        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)

        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)

        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)

        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)

        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)

        at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:483)

        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)

        at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)

        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)

        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)

        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)

        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)

        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)

        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)

        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:422)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)

        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)

        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

        at java.lang.Thread.run(Thread.java:745)





This direct SQL call fails for every getPartitions call and then falls back to ORM.
The query which fails is



select 

  PARTITIONS.PART_ID, SDS.SD_ID, SDS.CD_ID,

  SERDES.SERDE_ID, PARTITIONS.CREATE_TIME,

  PARTITIONS.LAST_ACCESS_TIME, SDS.INPUT_FORMAT, SDS.IS_COMPRESSED,

  SDS.IS_STOREDASSUBDIRECTORIES, SDS.LOCATION, SDS.NUM_BUCKETS,

  SDS.OUTPUT_FORMAT, SERDES.NAME, SERDES.SLIB 

from PARTITIONS

  left outer join SDS on PARTITIONS.SD_ID = SDS.SD_ID 

  left outer join SERDES on SDS.SERDE_ID = SERDES.SERDE_ID 

  where PART_ID in (  ?  ) order by PART_NAME asc;



By looking at the source MetaStoreDirectSql.java, the third column in the query ( SDS.CD_ID), the column descriptor ID, is null, which triggers the exception. This exception is not thrown from the ORM layer since it is more forgiving to the null column descriptor. See ObjectStore.java:1197



 List&amp;lt;MFieldSchema&amp;gt; mFieldSchemas = msd.getCD() == null ? null : msd.getCD().getCols();



I verified that this exception gets triggered in the first place when we add a new partition without setting column level schemas for the partition, using the MetaStoreClient API. This exception does not occur when adding partitions using the CLI
I see two ways to solve the issue.
1. Make the MetaStoreClient API more strict and not allow creating partition without having column level schemas set. (This could break clients which use the MetaStoreclient API)
2. Make the Direct SQL code path and the ORM code path more consistent, where the Direct SQL does not fail on null column descriptor ID.
I feel 2 is more safer and easier to fix.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-28 18:54:10" id="12992" opendate="2016-02-03 19:23:12" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive on tez: Bucket map join plan is incorrect</summary>
			
			
			<description>TPCH Query 9 fails when bucket map join is enabled:



FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 5, vertexId=vertex_1450634494433_0007_2_06, diagnostics=[Exception in EdgeManager, vertex=vertex_1450634494433_0007_2_06 [Reducer 5], Fail to sendTezEventToDestinationTasks, event:DataMovementEvent [sourceIndex=0, targetIndex=-1, version=0], sourceInfo:{ producerConsumerType=OUTPUT, taskVertexName=Map 1, edgeVertexName=Reducer 5, taskAttemptId=attempt_1450634494433_0007_2_05_000000_0 }, destinationInfo:null, EdgeInfo: sourceVertexName=Map 1, destinationVertexName=Reducer 5, java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.routeDataMovementEventToDestination(CustomPartitionEdge.java:88)

	at org.apache.tez.dag.app.dag.impl.Edge.sendTezEventToDestinationTasks(Edge.java:458)

	at org.apache.tez.dag.app.dag.impl.Edge.handleCompositeDataMovementEvent(Edge.java:386)

	at org.apache.tez.dag.app.dag.impl.Edge.sendTezEventToDestinationTasks(Edge.java:439)

	at org.apache.tez.dag.app.dag.impl.VertexImpl.handleRoutedTezEvents(VertexImpl.java:4382)

	at org.apache.tez.dag.app.dag.impl.VertexImpl.access$4000(VertexImpl.java:202)

	at org.apache.tez.dag.app.dag.impl.VertexImpl$RouteEventTransition.transition(VertexImpl.java:4172)

	at org.apache.tez.dag.app.dag.impl.VertexImpl$RouteEventTransition.transition(VertexImpl.java:4164)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.2.2, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">13619</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-29 18:37:45" id="12937" opendate="2016-01-26 23:19:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>DbNotificationListener unable to clean up old notification events</summary>
			
			
			<description>There is a bug in ObjectStore, where we use pm.deletePersistent instead of pm.deletePersistentAll, which causes the persistenceManager to try and drop a org.datanucleus.store.rdbms.query.ForwardQueryResult instead of the appropriate associated org.apache.hadoop.hive.metastore.model.MNotificationLog.
This results in an error that looks like this:

Exception in thread &quot;CleanerThread&quot; org.datanucleus.api.jdo.exceptions.ClassNotPersistenceCapableException: The class &quot;org.datanucleus.store.rdbms.query.ForwardQueryResult&quot; is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.

at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:380)

at org.datanucleus.api.jdo.JDOPersistenceManager.jdoDeletePersistent(JDOPersistenceManager.java:807)

at org.datanucleus.api.jdo.JDOPersistenceManager.deletePersistent(JDOPersistenceManager.java:820)

at org.apache.hadoop.hive.metastore.ObjectStore.cleanNotificationEvents(ObjectStore.java:7149)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:606)

at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)

at com.sun.proxy.$Proxy0.cleanNotificationEvents(Unknown Source)

at org.apache.hive.hcatalog.listener.DbNotificationListener$CleanerThread.run(DbNotificationListener.java:277)

NestedThrowablesStackTrace:

The class &quot;org.datanucleus.store.rdbms.query.ForwardQueryResult&quot; is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.

org.datanucleus.exceptions.ClassNotPersistableException: The class &quot;org.datanucleus.store.rdbms.query.ForwardQueryResult&quot; is not persistable. This means that it either hasnt been enhanced, or that the enhanced version of the file is not in the CLASSPATH (or is hidden by an unenhanced version), or the Meta-Data/annotations for the class are not found.

at org.datanucleus.ExecutionContextImpl.assertClassPersistable(ExecutionContextImpl.java:5698)

at org.datanucleus.ExecutionContextImpl.deleteObjectInternal(ExecutionContextImpl.java:2495)

at org.datanucleus.ExecutionContextImpl.deleteObjectWork(ExecutionContextImpl.java:2466)

at org.datanucleus.ExecutionContextImpl.deleteObject(ExecutionContextImpl.java:2417)

at org.datanucleus.ExecutionContextThreadedImpl.deleteObject(ExecutionContextThreadedImpl.java:245)

at org.datanucleus.api.jdo.JDOPersistenceManager.jdoDeletePersistent(JDOPersistenceManager.java:802)

at org.datanucleus.api.jdo.JDOPersistenceManager.deletePersistent(JDOPersistenceManager.java:820)

at org.apache.hadoop.hive.metastore.ObjectStore.cleanNotificationEvents(ObjectStore.java:7149)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:606)

at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)

at com.sun.proxy.$Proxy0.cleanNotificationEvents(Unknown Source)

at org.apache.hive.hcatalog.listener.DbNotificationListener$CleanerThread.run(DbNotificationListener.java:277)



The end result of this bug is that users of DbNotificationListener will have an evergrowing number of notification events that are not cleaned up as they age. This is an easy enough fix, but shows that we have a lack of test coverage here.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-31 04:35:30" id="13372" opendate="2016-03-28 22:52:19" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive Macro overwritten when multiple macros are used in one column</summary>
			
			
			<description>When multiple macros are used in one column, results of the later ones are over written by that of the first.
For example:
Suppose we have created a table called macro_test with single column x in STRING type, and with data as:
&quot;a&quot;
&quot;bb&quot;
&quot;ccc&quot;
We also create three macros:



CREATE TEMPORARY MACRO STRING_LEN(x string) length(x);

CREATE TEMPORARY MACRO STRING_LEN_PLUS_ONE(x string) length(x)+1;

CREATE TEMPORARY MACRO STRING_LEN_PLUS_TWO(x string) length(x)+2;



When we ran the following query, 



SELECT

    CONCAT(STRING_LEN(x), &quot;:&quot;, STRING_LEN_PLUS_ONE(x), &quot;:&quot;, STRING_LEN_PLUS_TWO(x)) a

FROM macro_test

SORT BY a DESC;



We get result:
3:3:3
2:2:2
1:1:1
instead of expected:
3:4:5
2:3:4
1:2:3
Currently we are using Hive 1.2.1, and have applied both HIVE-11432 and HIVE-12277 patches.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">2655</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-05 18:47:01" id="13394" opendate="2016-03-31 00:54:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Analyze table fails in tez on empty partitions/files/tables</summary>
			
			
			<description>


        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352)

        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:237)

        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:252)

        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150)

        ... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException: 0

        at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:766)

        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343)

        ... 17 more

Caused by: java.lang.ArrayIndexOutOfBoundsException: 0

        at org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.deserialize(NumDistinctValueEstimator.java:219)

        at org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.&amp;lt;init&amp;gt;(NumDistinctValueEstimator.java:112)

        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFNumericStatsEvaluator.merge(GenericUDAFComputeStats.java:556)

        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:188)

        at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:612)

        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:851)

        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:695)

        at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:761)

        ... 18 more

]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1455918888034_27748_1_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.2.2, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-09 22:33:06" id="13320" opendate="2016-03-21 19:14:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Apply HIVE-11544 to explicit conversions as well as implicit ones</summary>
			
			
			<description>Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11544</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-12 00:56:24" id="13439" opendate="2016-04-06 21:20:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>JDBC: provide a way to retrieve GUID to query Yarn ATS</summary>
			
			
			<description>HIVE-9673 added support for passing base64 encoded operation handles to ATS. We should a method on client side to retrieve that.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.HiveStatement.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-22 18:19:19" id="13240" opendate="2016-03-09 02:29:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>GroupByOperator: Drop the hash aggregates when closing operator</summary>
			
			
			<description>GroupByOperator holds onto the Hash aggregates accumulated when the plan is cached.
Drop the hashAggregates in case of error during forwarding to the next operator.
Added for PTF, TopN and all GroupBy cases.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is depended upon by" type="dependent">13238</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-27 22:24:36" id="13440" opendate="2016-04-06 22:37:58" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>remove hiveserver1 scripts and thrift generated files</summary>
			
			
			<description>HIVE-6977 deleted hiveserver1, however the scripts remain under bin/ext/-
ls bin/ext/hiveserver.*
bin/ext/hiveserver.cmd bin/ext/hiveserver.sh
The should be removed as well.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="D">org.apache.hadoop.hive.service.HiveClusterStatus.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.service.ThriftHive.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.service.JobTrackerState.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.service.HiveServerException.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-02 20:36:36" id="13390" opendate="2016-03-30 22:07:27" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveServer2: Add more test to ZK service discovery using MiniHS2</summary>
			
			
			<description/>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 1.2.2, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestSSL.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-10 18:28:35" id="13264" opendate="2016-03-11 02:46:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>JDBC driver makes 2 Open Session Calls for every open session</summary>
			
			
			<description>When HTTP is used as the transport mode by the Hive JDBC driver, we noticed that there is an additional open/close session just to validate the connection. 
TCLIService.Iface client = new TCLIService.Client(new TBinaryProtocol(transport));
      TOpenSessionResp openResp = client.OpenSession(new TOpenSessionReq());
      if (openResp != null) 
{

        client.CloseSession(new TCloseSessionReq(openResp.getSessionHandle()));

      }

The open session call is a costly one and should not be used to test transport. </description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.minikdc.TestJdbcWithMiniKdc.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-22 22:54:53" id="13946" opendate="2016-06-04 00:18:06" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Decimal value need to be single-quoted when selecting where clause with that decimal value in order to get row</summary>
			
			
			<description>Create a table withe a column of decimal type(38,18) and insert &amp;amp;apos;4327269606205.029297&amp;amp;apos;. Then select with that value does not return anything.

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; drop table if exists test;

No rows affected (0.175 seconds)

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt;

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; create table test (dc decimal(38,18));

No rows affected (0.098 seconds)

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt;

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; insert into table test values (4327269606205.029297);

INFO  : Session is already open

INFO  : Dag name: insert into table tes...327269606205.029297)(Stage-1)

INFO  : Tez session was closed. Reopening...

INFO  : Session re-established.

INFO  :



INFO  : Status: Running (Executing on YARN cluster with App id application_1464727816747_0762)



INFO  : Map 1: -/-

INFO  : Map 1: 0/1

INFO  : Map 1: 0(+1)/1

INFO  : Map 1: 1/1

INFO  : Loading data to table default.test from hdfs://ts-0531-5.openstacklocal:8020/apps/hive/warehouse/test/.hive-staging_hive_2016-06-04_00-03-54_302_7708281807413586675-940/-ext-10000

INFO  : Table default.test stats: [numFiles=1, numRows=1, totalSize=21, rawDataSize=20]

No rows affected (13.821 seconds)

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt;

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; select * from test;

+-----------------------+--+

|        test.dc        |

+-----------------------+--+

| 4327269606205.029297  |

+-----------------------+--+

1 row selected (0.078 seconds)

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; select * from test where dc = 4327269606205.029297;

+----------+--+

| test.dc  |

+----------+--+

+----------+--+

No rows selected (0.224 seconds)



If you single quote that decimal value, a row is returned.

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; select * from test where dc = &amp;amp;apos;4327269606205.029297&amp;amp;apos;;

+-----------------------+--+

|        test.dc        |

+-----------------------+--+

| 4327269606205.029297  |

+-----------------------+--+

1 row selected (0.085 seconds)



explain shows:

0: jdbc:hive2://ts-0531-1.openstacklocal:2181&amp;gt; explain select * from test where dc = 4327269606205.029297;

+----------------------------------------------------------------------+--+

|                               Explain                                |

+----------------------------------------------------------------------+--+

| STAGE DEPENDENCIES:                                                  |

|   Stage-0 is a root stage                                            |

|                                                                      |

| STAGE PLANS:                                                         |

|   Stage: Stage-0                                                     |

|     Fetch Operator                                                   |

|       limit: -1                                                      |

|       Processor Tree:                                                |

|         TableScan                                                    |

|           alias: test                                                |

|           filterExpr: (dc = 4.3272696062050293E12) (type: boolean)   |

|           Filter Operator                                            |

|             predicate: (dc = 4.3272696062050293E12) (type: boolean)  |

|             Select Operator                                          |

|               expressions: dc (type: decimal(38,18))                 |

|               outputColumnNames: _col0                               |

|               ListSink                                               |

|                                                                      |

+----------------------------------------------------------------------+--+

18 rows selected (0.512 seconds)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.java</file>
			
			
			<file type="M">org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			
			
			<file type="M">org.apache.orc.impl.RecordReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13945</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-27 06:44:18" id="13725" opendate="2016-05-10 00:45:58" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ACID: Streaming API should synchronize calls when multiple threads use the same endpoint</summary>
			
			
			<description>Currently, the streaming endpoint creates a metastore client which gets used for RPC. The client itself is not internally thread safe. Therefore, the API methods should provide the relevant synchronization so that the methods can be called from different threads. A sample use case is as follows:
1. Thread 1 creates a streaming endpoint and opens a txn batch.
2. Thread 2 heartbeats the txn batch.
With the current impl, this can result in an &quot;out of sequence response&quot;, since the response of the calls in thread1 might end up going to thread2 and vice-versa.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-06 22:09:57" id="14174" opendate="2016-07-06 22:04:25" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Fix creating buckets without scheme information</summary>
			
			
			<description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">14175</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-07 00:20:21" id="14019" opendate="2016-06-15 18:08:02" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>HiveServer2: Enable Kerberos with SSL for TCP transport</summary>
			
			
			<description>Currently, there is a limitation where an HS2 user needs to use the auth-conf SASL qop value to achieve encryption when Kerberos is used as the authentication mechanism and transport is TCP. </description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10048</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-08 23:24:47" id="14178" opendate="2016-07-07 00:13:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive::needsToCopy should reuse FileUtils::equalsFileSystem</summary>
			
			
			<description>Clear bug triggered from missing FS checks in Hive.java



//Check if different FileSystems

if (!srcFs.getClass().equals(destFs.getClass()))

{ 

return true;

 }


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">9159</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-11 17:19:38" id="14027" opendate="2016-06-15 22:52:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NULL values produced by left outer join do not behave as NULL</summary>
			
			
			<description>Consider the following setup:



create table tbl (n bigint, t string); 



insert into tbl values (1, &amp;amp;apos;one&amp;amp;apos;); 

insert into tbl values(2, &amp;amp;apos;two&amp;amp;apos;);



select a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a  left outer join  (select * from tbl where 1 = 2) b on a.n = b.n;



1    one    false    true



The query should return true for isnull(b.n).
I&amp;amp;apos;ve tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case. </description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13977</link>
			
			
			<link description="relates to" type="Reference">14208</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-11 17:20:35" id="13977" opendate="2016-06-09 08:04:00" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>nvl funtion not working after left outer join </summary>
			
			
			<description>Recreating problem.
1).Create table with sample data.
create table tabletest (n bigint, t string); 
insert into tabletest values (1, &amp;amp;apos;one&amp;amp;apos;); 
insert into tabletest values(2, &amp;amp;apos;two&amp;amp;apos;); 
2) Run leftouter join query on single table.
select a.n as leftHandN 
, b.n as rightHandN 
, b.t as rightHandT 
, nvl(b.t,&quot;empty&quot;) as rightHandTnvl  Expected empty --&amp;gt; received empty
, nvl(b.n,-1) as rightHandNnvl  Expected -1 --&amp;gt; received 1 
from 
(
select *
from tabletest 
where n=1
) a
left outer join
(
select *
from tabletest 
where 1=2
) b
on a.n = b.n;
nvl(b.n,-1) should return -1 but returns 1.
I have found b.n always returning a.n value.if a.n is 1 ,b.n is returning 1 and if it is 2,same 2 will be returned.
More information:
length(b.n) --gives--&amp;gt;1
cast(b.n as string) -gives--&amp;gt;1
ascii(b.n) -gives---&amp;gt;49 i.e 1</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">14027</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-11 20:20:35" id="14175" opendate="2016-07-06 22:04:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix creating buckets without scheme information</summary>
			
			
			<description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14174</link>
			
			
			<link description="is related to" type="Reference">9928</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-13 04:37:59" id="14210" opendate="2016-07-11 23:58:12" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ExecDriver should call jobclient.close() to trigger cleanup</summary>
			
			
			<description>We found an issue in a customer environment where the HS2 crashed after a few days and the Java core dump contained several thousands of truststore reloader threads:
&quot;Truststore reloader thread&quot; #126 daemon prio=5 os_prio=0 tid=0x00007f680d2e3000 nid=0x98fd waiting on 
condition [0x00007f67e482c000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run
(ReloadingX509TrustManager.java:225)
        at java.lang.Thread.run(Thread.java:745)
We found the issue to be caused by a bug in Hadoop where the TimelineClientImpl is not destroying the SSLFactory if SSL is enabled in Hadoop and the timeline server is running. I opened YARN-5309 which has more details on the problem, and a patch was submitted a few days back.
In addition to the changes in Hadoop, there are a couple of Hive changes required:

ExecDriver needs to call jobclient.close() to trigger the clean-up of the resources after the submitted job is done/failed
Hive needs to pick up a newer release of Hadoop to pick up MAPREDUCE-6618 and MAPREDUCE-6621 that fixed issues with calling jobclient.close(). Both fixes are included in Hadoop 2.6.4.
However, since we also need to pick up YARN-5309, we need to wait for a new release of Hadoop.

</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 1.2.2, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">5309</link>
			
			
			<link description="is related to" type="Reference">12766</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-13 21:00:42" id="14164" opendate="2016-07-05 21:33:32" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>JDBC: Add retry in JDBC driver when reading config values from ZK</summary>
			
			
			<description>Sometimes ZK may intermittently experience network partitioning. During this time, clients trying to open a JDBC connection get an exception. To improve user experience, we should implement a retry logic and fail after retrying.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13400</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-14 22:23:03" id="14222" opendate="2016-07-13 00:23:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>PTF: Operator initialization does not clean state</summary>
			
			
			<description>PTFOperator::initializeOp() does not reset currentKeys to null.



      if (currentKeys != null &amp;amp;&amp;amp; !keysAreEqual) {

        ptfInvocation.finishPartition();

      }

....

      if (currentKeys == null) {

          currentKeys = newKeys.copyKey();

        } else {

          currentKeys.copyKey(newKeys);

        }


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1, 2.0.2</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-20 22:20:09" id="14282" opendate="2016-07-19 17:32:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HCatLoader ToDate() exception with hive partition table ,partitioned by column of DATE datatype</summary>
			
			
			<description>ToDate() function doesnt work with a partitioned table, partitioned by the column of DATE Datatype.
Below are the steps I followed to recreate the problem.
--&amp;gt;Sample input file to hive table :
hdfs@testhost ~$ cat test.log 
2012-06-13,16:11:17,574,140.134.127.109,SearchPage,Google.com,Win8,5,HTC
2012-06-13,16:11:17,466,43.176.108.158,Electronics,Google.com,Win8,3,iPhone
2012-06-13,16:11:17,501,97.73.102.79,Appliances,Google.com,Android,4,iPhone
2012-06-13,16:11:17,469,166.98.157.122,Recommendations,Google.com,Win8,5,HTC
2012-06-13,16:11:17,557,36.159.147.50,Sporting,Google.com,Win8,3,Samsung
2012-06-13,16:11:17,449,128.215.122.234,ShoppingCart,Google.com,Win8,5,HTC
2012-06-13,16:11:17,502,46.81.131.92,Electronics,Google.com,Android,5,Samsung
2012-06-13,16:11:17,554,120.187.105.127,Automotive,Google.com,Win8,5,HTC
2012-06-13,16:11:17,447,127.94.64.59,DetailPage,Google.com,Win8,3,Samsung
2012-06-13,16:11:17,490,132.54.25.75,ShoppingCart,Google.com,Win8,3,iPhone
2012-06-13,16:11:17,578,79.201.53.179,Automotive,Google.com,Win8,5,Samsung
2012-06-13,16:11:17,435,158.106.164.38,HomePage,Google.com,Web,5,Chrome
2012-06-13,16:11:17,523,17.131.82.171,Recommendations,Google.com,Web,3,IE9
2012-06-13,16:11:17,575,178.95.126.105,Appliances,Google.com,iOS,3,iPhone
2012-06-13,16:11:17,468,225.143.39.176,SearchPage,Google.com,iOS,5,HTC
2012-06-13,16:11:17,511,43.103.102.147,ShoppingCart,Google.com,iOS,5,Samsung
--&amp;gt; Copied to hdfs directory:
hdfs@testhost ~$ hdfs dfs -put -f test.log /user/hdfs/
--&amp;gt;Create partitoned table (partitioned with date data type column) in hive:
0: jdbc:hive2://hdp2.raghav.com:10000/default&amp;gt; create table mytable(Dt DATE,Time STRING,Number INT,IPAddr STRING,Type STRING,Site STRING,OSType STRING,Visit INT,PhModel STRING) row format delimited fields terminated by &amp;amp;apos;,&amp;amp;apos; stored as textfile;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; load data inpath &amp;amp;apos;/user/hdfs/test.log&amp;amp;apos; overwrite into table mytable;
0: jdbc:hive2://testhost..com:10000/default&amp;gt; SET hive.exec.dynamic.partition = true;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; SET hive.exec.dynamic.partition.mode = nonstrict;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; create table partmytable(Number INT,IPAddr STRING,Type STRING,Site STRING,OSType STRING,Visit INT,PhModel STRING) partitioned by (Dt DATE,Time STRING) row format delimited fields terminated by &amp;amp;apos;,&amp;amp;apos; stored as textfile;
0: jdbc:hive2://testhost.com:10000/default&amp;gt; insert overwrite table partmytable partition(Dt,Time) select Number,IPAddr,Type,Site,OSType,Visit,PhModel,Dt,Time from mytable;
0: jdbc:hive2://hdp2.raghav.com:10000/default&amp;gt; describe partmytable;
--&amp;gt; Try to filter with ToDate function which fails with error:
hdfs@testhost ~$ pig -useHCatalog
grunt&amp;gt;
grunt&amp;gt; temp = LOAD &amp;amp;apos;partmytable&amp;amp;apos; using org.apache.hive.hcatalog.pig.HCatLoader();
grunt&amp;gt; temp1 = FILTER temp by dt == ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;,&amp;amp;apos;yyyy-MM-dd&amp;amp;apos;);
grunt&amp;gt; dump temp1;
--&amp;gt;Try to filter the normal table with same statement works;
grunt&amp;gt;
grunt&amp;gt; temp = LOAD &amp;amp;apos;mytable&amp;amp;apos; using org.apache.hive.hcatalog.pig.HCatLoader();
grunt&amp;gt; temp1 = FILTER temp by dt == ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;,&amp;amp;apos;yyyy-MM-dd&amp;amp;apos;);
grunt&amp;gt; dump temp1;
Workaround :
Use below statement instead of direct ToDate();
grunt&amp;gt;temp1 = FILTER temp5 by DaysBetween(dt,(datetime)ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;, &amp;amp;apos;yyyy-MM-dd&amp;amp;apos;)) &amp;gt;=(long)0 AND DaysBetween(dt,(datetime)ToDate(&amp;amp;apos;2012-06-13&amp;amp;apos;, &amp;amp;apos;yyyy-MM-dd&amp;amp;apos;)) &amp;lt;=(long)0;</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-28 21:09:12" id="14349" opendate="2016-07-26 23:53:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: LIKE should anchor the regexes</summary>
			
			
			<description>RLIKE works like contains() and LIKE works like matches().
The UDFLike LIKE -&amp;gt; Regex conversion returns unanchored regexes making the vectorized LIKE behave like RLIKE.



create temporary table x (a string) stored as orc;

insert into x values(&amp;amp;apos;XYZa&amp;amp;apos;), (&amp;amp;apos;badXYZa&amp;amp;apos;);



select * from x where a LIKE &amp;amp;apos;XYZ%a%&amp;amp;apos; order by 1;

OK

XYZa

badXYZa

Time taken: 4.029 seconds, Fetched: 2 row(s)


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14318</link>
			
			
			<link description="requires" type="Required">13196</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-29 15:26:57" id="14294" opendate="2016-07-20 04:28:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveSchemaConverter for Parquet doesn&amp;apos;t translate TINYINT and SMALLINT into proper Parquet types</summary>
			
			
			<description>To reproduce this issue, run the following DDL:



CREATE TABLE foo STORED AS PARQUET AS SELECT CAST(1 AS TINYINT);



And then check the schema of the written Parquet file:

$ parquet-schema $WAREHOUSE_PATH/foo/000000_0

message hive_schema {

  optional int32 _c0;

}



When translating Hive types into Parquet types, TINYINT and SMALLINT should be translated into the int32 (INT_8) and int32 (INT_16) respectively. However, HiveSchemaConverter converts all of TINYINT, SMALLINT, and INT into Parquet int32. This causes problem when accessing Parquet files generated by Hive in other systems since type information gets wrong.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">16632</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-08-06 23:04:29" id="14390" opendate="2016-07-30 08:18:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Wrong Table alias when CBO is on</summary>
			
			
			<description>There are 5 web_sales references in query95 of tpcds ,with alias ws1-ws5.
But the query plan only has ws1 when CBO is on.
query95 :

SELECT count(distinct ws1.ws_order_number) as order_count,

               sum(ws1.ws_ext_ship_cost) as total_shipping_cost,

               sum(ws1.ws_net_profit) as total_net_profit

FROM web_sales ws1

JOIN customer_address ca ON (ws1.ws_ship_addr_sk = ca.ca_address_sk)

JOIN web_site s ON (ws1.ws_web_site_sk = s.web_site_sk)

JOIN date_dim d ON (ws1.ws_ship_date_sk = d.d_date_sk)

LEFT SEMI JOIN (SELECT ws2.ws_order_number as ws_order_number

                               FROM web_sales ws2 JOIN web_sales ws3

                               ON (ws2.ws_order_number = ws3.ws_order_number)

                               WHERE ws2.ws_warehouse_sk &amp;lt;&amp;gt; ws3.ws_warehouse_sk

                        ) ws_wh1

ON (ws1.ws_order_number = ws_wh1.ws_order_number)

LEFT SEMI JOIN (SELECT wr_order_number

                               FROM web_returns wr

                               JOIN (SELECT ws4.ws_order_number as ws_order_number

                                          FROM web_sales ws4 JOIN web_sales ws5

                                          ON (ws4.ws_order_number = ws5.ws_order_number)

                                         WHERE ws4.ws_warehouse_sk &amp;lt;&amp;gt; ws5.ws_warehouse_sk

                                ) ws_wh2

                               ON (wr.wr_order_number = ws_wh2.ws_order_number)) tmp1

ON (ws1.ws_order_number = tmp1.wr_order_number)

WHERE d.d_date between &amp;amp;apos;2002-05-01&amp;amp;apos; and &amp;amp;apos;2002-06-30&amp;amp;apos; and

               ca.ca_state = &amp;amp;apos;GA&amp;amp;apos; and

               s.web_company_name = &amp;amp;apos;pri&amp;amp;apos;;


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-10 05:58:30" id="14436" opendate="2016-08-05 11:03:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive 1.2.1/Hitting &quot;ql.Driver: FAILED: IllegalArgumentException Error: , expected at the end of &amp;apos;decimal(9&amp;apos;&quot; after enabling hive.optimize.skewjoin and with MR engine</summary>
			
			
			<description>PROBLEM:
The following Query run with MapReduce engine with &quot;hive.optimize.skewjoin = true&quot; fails with error:
&quot;FAILED: IllegalArgumentException Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;&quot; 
&amp;gt; SELECT a.col1 FROM db.tableA a  INNER JOIN  db.tableB b  ON b.key=a.key limit 5;
FAILED: IllegalArgumentException Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;
16/08/04 12:47:50 [main]: ERROR ql.Driver: FAILED: IllegalArgumentException Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;
java.lang.IllegalArgumentException: Error: , expected at the end of &amp;amp;apos;decimal(9&amp;amp;apos;
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:336)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseParams(TypeInfoUtils.java:378)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parsePrimitiveParts(TypeInfoUtils.java:518)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.parsePrimitiveParts(TypeInfoUtils.java:533)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.createPrimitiveTypeInfo(TypeInfoFactory.java:136)
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo(TypeInfoFactory.java:109)
	at org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.processSkewJoin(GenMRSkewJoinProcessor.java:214)
	at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory$SkewJoinJoinProcessor.process(SkewJoinProcFactory.java:60)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver$SkewJoinTaskDispatcher.dispatch(SkewJoinResolver.java:100)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.resolve(SkewJoinResolver.java:55)
	at org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize(PhysicalOptimizer.java:107)
	at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.optimizeTaskPlan(MapReduceCompiler.java:270)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10219)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:459)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:316)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1189)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1126)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:168)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:379)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:624)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
--------------
2) However same query works fine when we set &quot;hive.optimize.skewjoin = false&quot; .  And we dont find this issue using Tez execution engine.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14968</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-08-10 16:56:35" id="13756" opendate="2016-05-13 09:20:22" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Map failure attempts to delete reducer _temporary directory on multi-query pig query</summary>
			
			
			<description>A pig script, executed with multi-query enabled, that reads the source data and writes it as-is into TABLE_A as well as performing a group-by operation on the data which is written into TABLE_B can produce erroneous results if any map fails. This results in a single MR job that writes the map output to a scratch directory relative to TABLE_A and the reducer output to a scratch directory relative to TABLE_B.
If one or more maps fail it will delete the attempt data relative to TABLE_A, but it also deletes the _temporary directory relative to TABLE_B. This has the unintended side-effect of preventing subsequent maps from committing their data. This means that any maps which successfully completed before the first map failure will have its data committed as expected, other maps not, resulting in an incomplete result set.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-10 17:13:08" id="13754" opendate="2016-05-13 01:17:59" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix resource leak in HiveClientCache</summary>
			
			
			<description>Found that the users reference count can go into negative values, which prevents tearDownIfUnused from closing the client connection when called.
This leads to a build up of clients which have been evicted from the cache, are no longer in use, but have not been shutdown.
GC will eventually call finalize, which forcibly closes the connection and cleans up the client, but I have seen as many as several hundred open client connections as a result.
The main resource for this is caused by RetryingMetaStoreClient, which will call reconnect on acquire, which calls close. This will decrement users to -1 on the reconnect, then acquire will increase this to 0 while using it, and back to -1 when it releases it.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.common.HCatUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.common.TestHiveClientCache.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.common.HiveClientCache.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.common.HCatConstants.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-09 09:44:39" id="14591" opendate="2016-08-19 23:40:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HS2 is shut down unexpectedly during the startup time</summary>
			
			
			<description>If there is issue with Zookeeper (e.g. connection issues), then it takes HS2 some time to connect. During this time, Ambari could issue health checks against HS2 and the CloseSession call will trigger the shutdown of HS2, which is not expected. That triggering should happen only when the HS2 has been deregistered with Zookeeper, not during the startup time when HS2 is not registered with ZK yet.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
			
			
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-09 11:05:43" id="14686" opendate="2016-09-01 09:33:56" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Get unexpected command type when execute query &quot;CREATE TABLE IF NOT EXISTS ... AS&quot;</summary>
			
			
			<description>See the query: 



create table if not exists DST as select * from SRC;



if the table DST doesn&amp;amp;apos;t exist, SessionState.get().getHiveOperation() will return HiveOperation.CREATETABLE_AS_SELECT;
But if the table DST already exists, it will return HiveOperation.CREATETABLE;
It really makes some trouble for those who judge operation type by SessionState.get().getHiveOperation().
The reason I find out is that the function analyzeCreateTable in SemanticAnalyzer.java will return null and won&amp;amp;apos;t set the correct command type if the table already exists.
Here is the related code:



// check for existence of table

    if (ifNotExists) {

      try {

        Table table = getTable(qualifiedTabName, false);

        if (table != null) { // table exists

          return null;

        }

      } catch (HiveException e) {

        // should not occur since second parameter to getTableWithQN is false

        throw new IllegalStateException(&quot;Unxpected Exception thrown: &quot; + e.getMessage(), e);

      }

    }


</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-28 16:09:02" id="12222" opendate="2015-10-21 17:29:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Define port range in property for RPCServer</summary>
			
			
			<description>Creating this JIRA after discussin with Xuefu on the dev mailing list. Would need some help to review and update the fields in this JIRA ticket, thanks.
I notice that in 
./spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java
The port number is assigned with 0 which means it will be a random port every time when the RPC Server is created to talk to Spark in the same session.
Because of this, this is causing problems to configure firewall between the 
HiveCLI RPC Server and Spark due to unpredictable port numbers here. In other word, users need to open all hive ports range 
from Data Node =&amp;gt; HiveCLI (edge node).



 this.channel = new ServerBootstrap()

      .group(group)

      .channel(NioServerSocketChannel.class)

      .childHandler(new ChannelInitializer&amp;lt;SocketChannel&amp;gt;() {

          @Override

          public void initChannel(SocketChannel ch) throws Exception {

            SaslServerHandler saslHandler = new SaslServerHandler(config);

            final Rpc newRpc = Rpc.createServer(saslHandler, config, ch, group);

            saslHandler.rpc = newRpc;



            Runnable cancelTask = new Runnable() {

                @Override

                public void run() {

                  LOG.warn(&quot;Timed out waiting for hello from client.&quot;);

                  newRpc.close();

                }

            };

            saslHandler.cancelTask = group.schedule(cancelTask,

                RpcServer.this.config.getServerConnectTimeoutMs(),

                TimeUnit.MILLISECONDS);



          }

      })



2 Main reasons.

Most users (what I see and encounter) use HiveCLI as a command line tool, and in order to use that, they need to login to the edge node (via SSH). Now, here comes the interesting part.
Could be true or not, but this is what I observe and encounter from time to time. Most users will abuse the resource on that edge node (increasing HADOOP_HEAPSIZE, dumping output to local disk, running huge python workflow, etc), this may cause the HS2 process to run into OOME, choke and die, etc. various resource issues including others like login, etc.


Analyst connects to Hive via HS2 + ODBC. So HS2 needs to be highly available. This makes sense to run it on the gateway node or a service node and separated from the HiveCLI.
The logs are located in different location, monitoring and auditing is easier to run HS2 with a daemon user account, etc. so we don&amp;amp;apos;t want users to run HiveCLI where HS2 is running.
It&amp;amp;apos;s better to isolate the resource this way to avoid any memory, file handlers, disk space, issues.

From a security standpoint, 

Since users can login to edge node (via SSH), the security on the edge node needs to be fortified and enhanced. Therefore, all the FW comes in and auditing.


Regulation/compliance for auditing is another requirement to monitor all traffic, specifying ports and locking down the ports makes it easier since we can focus
on a range to monitor and audit.

</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.spark.client.rpc.TestRpc.java</file>
			
			
			<file type="M">org.apache.hive.spark.client.rpc.RpcServer.java</file>
			
			
			<file type="M">org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14327</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-10-15 07:46:53" id="14966" opendate="2016-10-14 20:25:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>JDBC: Make cookie-auth work in HTTP mode</summary>
			
			
			<description>HiveServer2 cookie-auth is non-functional and forces authentication to be repeated for the status check loop, row fetch loop and the get logs loop.
The repeated auth in the fetch-loop is a performance issue, but is also causing occasional DoS responses from the remote auth-backend if this is not using local /etc/passwd.
The HTTP-Cookie auth once made functional will behave similarly to the binary protocol, authenticating exactly once per JDBC session and not causing further load on the authentication backend irrespective how many rows are returned from the JDBC request.
This due to the fact that the cookies are not sent out with matching flags for SSL usage.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
			
			
			<file type="M">org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-10-18 18:25:12" id="15004" opendate="2016-10-18 18:22:41" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Query for parquet tables failing with java.lang.IllegalArgumentException: FilterPredicate column: f&amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. </summary>
			
			
			<description>Queries involving float data type, run against parquet tables failing
hive&amp;gt; desc extended all100k;
OK
t tinyint
sismallint
i int
b bigint
f float
d double
s string
dcdecimal(38,18)
boboolean
v varchar(25)
c char(25)
tstimestamp
dtdate
Detailed Table InformationTable(tableName:all100k, dbName:default, owner:hrt_qa, createTime:1476765150, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:t, type:tinyint, comment:null), FieldSchema(name:si, type:smallint, comment:null), FieldSchema(name:i, type:int, comment:null), FieldSchema(name:b, type:bigint, comment:null), FieldSchema(name:f, type:float, comment:null), FieldSchema(name:d, type:double, comment:null), FieldSchema(name:s, type:string, comment:null), FieldSchema(name:dc, type:decimal(38,18), comment:null), FieldSchema(name:bo, type:boolean, comment:null), FieldSchema(name:v, type:varchar(25), comment:null), FieldSchema(name:c, type:char(25), comment:null), FieldSchema(name:ts, type:timestamp, comment:null), FieldSchema(name:dt, type:date, comment:null)], location:hdfs://ctr-e45-1475874954070-9012-01-000008.hwx.site:8020/apps/hive/warehouse/all100k, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:
{serialization.format=1}
), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=1, transient_lastDdlTime=1476765184, COLUMN_STATS_ACCURATE={&quot;COLUMN_STATS&quot;:
{&quot;t&quot;:&quot;true&quot;,&quot;si&quot;:&quot;true&quot;,&quot;i&quot;:&quot;true&quot;,&quot;b&quot;:&quot;true&quot;,&quot;f&quot;:&quot;true&quot;,&quot;d&quot;:&quot;true&quot;,&quot;s&quot;:&quot;true&quot;,&quot;dc&quot;:&quot;true&quot;,&quot;bo&quot;:&quot;true&quot;,&quot;v&quot;:&quot;true&quot;,&quot;c&quot;:&quot;true&quot;,&quot;ts&quot;:&quot;true&quot;}
,&quot;BASIC_STATS&quot;:&quot;true&quot;}, totalSize=6564143, numRows=100000, rawDataSize=1300000}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
Time taken: 0.54 seconds, Fetched: 15 row(s)
hive&amp;gt; select t from all100k
&amp;gt; where t&amp;lt;&amp;gt;0 and s&amp;lt;&amp;gt;0 and b&amp;lt;&amp;gt;0 and (f&amp;lt;&amp;gt;0 or d&amp;lt;&amp;gt;0);
OK
SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Failed with exception java.io.IOException:java.lang.IllegalArgumentException: FilterPredicate column: f&amp;amp;apos;s declared type (java.lang.Double) does not match the schema found in file metadata. Column f is of type: FLOAT
Valid types for this column are: [class java.lang.Float]
Time taken: 0.919 seconds</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.2.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.read.TestParquetFilterPredicate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11504</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-11-01 00:36:28" id="15099" opendate="2016-10-31 18:07:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>PTFOperator.PTFInvocation didn&amp;apos;t properly reset the input partition</summary>
			
			
			<description>There is an issue with PTFOperator.PTFInvocation where the inputPart is not reset properly. The inputPart has been closed and its content (member variables) has been cleaned up, but since itself is not nullified, it&amp;amp;apos;s reused in the next round and caused NPE issue.</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-11-07 14:17:59" id="13947" opendate="2016-06-04 09:58:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HoS print wrong number for hash table size in map join scenario</summary>
			
			
			<description>In sparkHashTableSinkOperator, when flushToFile, before close output stream, it try to get the file length, and will get 0 for it,  take hashTableSinkOperator for ref, it should get length after output stream closed</description>
			
			
			<version>1.2.1</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
</bugrepository>
