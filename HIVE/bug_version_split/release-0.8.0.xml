<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="HIVE">
	<bug fixdate="2011-02-25 00:25:18" id="2001" opendate="2011-02-22 23:52:13" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add inputs and outputs to authorization DDL commands</summary>
			
			
			<description>When permissions are changed for a table/partition, the respective object should be present in the read/write entities for hooks to act on.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-03-17 19:26:40" id="1959" opendate="2011-02-04 13:58:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Potential memory leak when same connection used for long time. TaskInfo and QueryInfo objects are getting accumulated on executing more queries on the same connection.</summary>
			
			
			<description>org.apache.hadoop.hive.ql.history.HiveHistory$TaskInfo and org.apache.hadoop.hive.ql.history.HiveHistory$QueryInfo these two objects are getting accumulated on executing more number of queries on the same connection. These objects are getting released only when the connection is closed.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-03-21 00:41:01" id="2060" opendate="2011-03-17 18:28:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CLI local mode hit NPE when exiting by ^D</summary>
			
			
			<description>CLI gets an NPE when running in local mode and hit an ^D to exit it. </description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.cli.CliSessionState.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-03-23 17:57:02" id="2069" opendate="2011-03-22 20:32:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NullPointerException on getSchemas</summary>
			
			
			<description>Calling getSchemas will cause a nullpointerexception</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">8030</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2011-04-05 18:10:25" id="2054" opendate="2011-03-15 12:47:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Exception on windows when using the jdbc driver. &quot;IOException: The system cannot find the path specified&quot;</summary>
			
			
			<description>It seems something recently changed on the jdbc driver which causes this IOException on windows.
java.lang.RuntimeException: java.io.IOException: The system cannot find the path specified
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:237)
	at org.apache.hadoop.hive.jdbc.HiveConnection.&amp;lt;init&amp;gt;(HiveConnection.java:73)
	at org.apache.hadoop.hive.jdbc.HiveDriver.connect(HiveDriver.java:110)</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.7.1, 0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcSessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-05-20 17:33:24" id="2096" opendate="2011-04-06 00:07:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>throw a error if the input is larger than a threshold for index input format</summary>
			
			
			<description>This can hang for ever.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndexResult.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-06-23 21:16:30" id="2237" opendate="2011-06-23 20:05:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hive fails to build in eclipse due to syntax error in BitmapIndexHandler.java</summary>
			
			
			<description>I see the following error in helios eclipse with the latest trunk (although build on the command line is fine):
Syntax error on token &quot;;&quot;, delete this token
seems to have been introduced by this change in HIVE-2036
+import org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;;
I have a patch forthcoming.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-07-14 18:08:09" id="2204" opendate="2011-06-07 13:21:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>unable to get column names for a specific table that has &amp;apos;_&amp;apos; as part of its table name</summary>
			
			
			<description>I have a table age_group and I am trying to get list of columns for this table name. As underscore and &amp;amp;apos;%&amp;amp;apos; have special meaning in table search pattern according to JDBC searchPattern string specification, I escape the &amp;amp;apos;_&amp;amp;apos; in my table name when I call getColumns for this single table. But HIVE does not return any columns. My call to getColumns is as follows
catalog	&amp;lt;null&amp;gt;
schemaPattern	&quot;%&quot;
tableNamePattern  &quot;age_group&quot;
columnNamePattern  &quot;%&quot;
If I don&amp;amp;apos;t escape the &amp;amp;apos;_&amp;amp;apos; in my tableNamePattern, I am able to get the list of columns.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-07-22 20:21:24" id="2296" opendate="2011-07-20 23:13:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>bad compressed file names from insert into</summary>
			
			
			<description>When INSERT INTO is run on a table with compressed output (hive.exec.compress.output=true) and existing files in the table, it may copy the new files in bad file names:
Before INSERT INTO:
000000_0.gz
After INSERT INTO:
000000_0.gz
000000_0.gz_copy_1
This causes corrupted output when doing a SELECT * on the table.
Correct behavior should be to pick a valid filename such as:
000000_0_copy_1.gz</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-08-08 19:07:51" id="2138" opendate="2011-05-01 00:19:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Exception when no splits returned from index</summary>
			
			
			<description>Running a query that uses indexing but doesn&amp;amp;apos;t return any results give an exception.

 java.lang.IllegalArgumentException: Can not create a Path from an empty string

at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)

at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:90)

at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:224)

at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:282)

at org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.getSplits(HiveIndexedInputFormat.java:123) 

This could potentially be fixed by creating a new empty file to use for the splits.
Once this is fixed, the index_auto_test_if_used.q can be used.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">1644</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2011-08-11 20:08:56" id="2344" opendate="2011-08-03 21:44:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>filter is removed due to regression of HIVE-1538</summary>
			
			
			<description> select * from 
 (
 select type_bucket,randum123
 from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) a
 where randum123 &amp;lt;=0.1)s where s.randum123&amp;gt;0.1 limit 20;
This is returning results...
and 
 explain
 select type_bucket,randum123
 from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) a
 where randum123 &amp;lt;=0.1
shows that there is no filter.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">1538</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2011-08-15 07:23:48" id="2358" opendate="2011-08-08 21:43:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>JDBC DatabaseMetaData and ResultSetMetaData need to match for particular types</summary>
			
			
			<description>My patch for HIVE-1631 did not ensure the following (from comment on 1631):
-------------
Mythili Gopalakrishnan added a comment - 08/Aug/11 08:42
Just tested this fix and does NOT work correctly. Here are my findings on a FLOAT column
Without Patch on a FLOAT Column
--------------------------------
DatabaseMetaData.getColumns () COLUMN_SIZE returns 12
DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0
ResultSetMetaData.getPrecision() returns 0
ResultSetMetaData.getScale() returns 0
With Patch on a FLOAT Column
----------------------------
DatabaseMetaData.getColumns () COLUMN_SIZE returns 24
DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0
ResultSetMetaData.getPrecision() returns 7
ResultSetMetaData.getScale() returns 7
Also both DatabaseMetadata and ResultSetMetaData must return the same information for Precision and Scale for FLOAT,DOUBLE types.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">1631</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2011-09-08 23:38:09" id="2402" opendate="2011-08-24 13:55:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Function like with empty string is throwing null pointer exception</summary>
			
			
			<description>select emp.ename from emp where ename like &amp;amp;apos;&amp;amp;apos;
This query is throwing null pointer exception</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFLike.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-09-23 17:21:29" id="2181" opendate="2011-05-24 09:47:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary> Clean up the scratch.dir (tmp/hive-root) while restarting Hive server. </summary>
			
			
			<description>Now queries leaves the map outputs under scratch.dir after execution. If the hive server is stopped we need not keep the stopped server&amp;amp;apos;s map oputputs. So whle starting the server we can clear the scratch.dir. This can help in improved disk usage.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.service.TestHiveServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.service.HiveServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">10415</link>
			
			
			<link description="is related to" type="Reference">13429</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2011-11-03 19:17:15" id="2214" opendate="2011-06-10 21:00:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CommandNeedRetryException.java is missing ASF header</summary>
			
			
			<description>Please add one.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.CommandNeedRetryException.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2011-11-15 17:59:47" id="2196" opendate="2011-06-04 07:33:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Ensure HiveConf includes all properties defined in hive-default.xml</summary>
			
			
			<description>There are a bunch of properties that are defined in hive-default.xml but not in HiveConf.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">1984</link>
			
			
			<link description="incorporates" type="Incorporates">1571</link>
			
			
			<link description="relates to" type="Reference">2596</link>
			
			
			<link description="relates to" type="Reference">6037</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2011-12-16 21:17:13" id="2631" opendate="2011-12-07 01:14:05" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Make Hive work with Hadoop 1.0.0</summary>
			
			
			<description>With Hadoop 1.0.0 around the corner ( http://mail-archives.apache.org/mod_mbox/hadoop-general/201111.mbox/%3C9D6B6144-F4E0-4A31-883F-2AC504727A1F%40hortonworks.com%3E ), it will be useful to make Hive work with it.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.1, 0.9.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2012-01-26 02:38:24" id="2746" opendate="2012-01-24 22:49:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Metastore client doesn&amp;apos;t log properly in case of connection failure to server</summary>
			
			
			<description>LOG.error(e.getStackTrace()) in current code prints memory location of StackTraceElement[] instead of message.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.9.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is depended upon by" type="dependent">231</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2012-02-21 07:43:36" id="2792" opendate="2012-02-09 02:25:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>SUBSTR(CAST(&lt;string&gt; AS BINARY)) produces unexpected results</summary>
			
			
			<description/>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.9.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2012-03-15 19:30:30" id="2856" opendate="2012-03-09 00:19:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix TestCliDriver escape1.q failure on MR2</summary>
			
			
			<description>Additional &amp;amp;apos;^&amp;amp;apos; in escape test:
[junit] Begin query: escape1.q
[junit] Copying file: file:/home/cloudera/Code/hive/data/files/escapetest.txt
[junit] 12/01/23 15:22:15 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir
[junit] 12/01/23 15:22:15 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[junit] diff -a -I file: -I pfile: -I hdfs: -I /tmp/ -I invalidscheme: -I lastUpdateTime -I lastAccessTime -I [Oo]wner -I CreateTime -I LastAccessTime -I Location -I LOCATION &amp;amp;apos; -I transient_lastDdlTime -I last_modified_ -I java.lang.RuntimeException -I at org -I at sun -I at java -I at junit -I Caused by: -I LOCK_QUERYID: -I LOCK_TIME: -I grantTime -I [.][.][.] [0-9]* more -I job_[0-9]_[0-9] -I USING &amp;amp;apos;java -cp /home/cloudera/Code/hive/build/ql/test/logs/clientpositive/escape1.q.out /home/cloudera/Code/hive/ql/src/test/results/clientpositive/escape1.q.out
[junit] 893d892
[junit] &amp;lt; 1	1	^
[junit] junit.framework.AssertionFailedError: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try &quot;ant test ... -Dtest.silent=false&quot; to get more logs.
[junit] at junit.framework.Assert.fail(Assert.java:50)
[junit] at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape1(TestCliDriver.java:131)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] at java.lang.reflect.Method.invoke(Method.java:616)
[junit] at junit.framework.TestCase.runTest(TestCase.java:168)
[junit] at junit.framework.TestCase.runBare(TestCase.java:134)
[junit] at junit.framework.TestResult$1.protect(TestResult.java:110)
[junit] at junit.framework.TestResult.runProtected(TestResult.java:128)
[junit] at junit.framework.TestResult.run(TestResult.java:113)
[junit] at junit.framework.TestCase.run(TestCase.java:124)
[junit] at junit.framework.TestSuite.runTest(TestSuite.java:243)
[junit] at junit.framework.TestSuite.run(TestSuite.java:238)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
[junit] Exception: Client execution results failed with error code = 1
[junit] See build/ql/tmp/hive.log, or try &quot;ant test ... -Dtest.silent=false&quot; to get more logs.
[junit] See build/ql/tmp/hive.log, or try &quot;ant test ... -Dtest.silent=false&quot; to get more logs.)</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.9.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2012-04-23 23:23:18" id="2803" opendate="2012-02-12 11:50:03" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>utc_from_timestamp and utc_to_timestamp returns incorrect results.</summary>
			
			
			<description>How to reproduce:

$ echo &quot;2011-12-25 09:00:00.123456&quot; &amp;gt; /tmp/data5.txt

hive&amp;gt; create table ts1(t1 timestamp);

hive&amp;gt; load data local inpath &amp;amp;apos;/tmp/data5.txt&amp;amp;apos; overwrite into table ts1;

hive&amp;gt; select t1, from_utc_timestamp(t1, &amp;amp;apos;JST&amp;amp;apos;), from_utc_timestamp(t1, &amp;amp;apos;JST&amp;amp;apos;) from ts1 limit 1;



The following result is expected:

 2011-12-25 09:00:00.123456      2011-12-25 18:00:00.123456      2011-12-25 18:00:00.123456



However, the above query return incorrect result like this:

 2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456      2011-12-26 03:00:00.492456



This is because GenericUDFFromUtcTimestamp.applyOffset() does setTime() improperly.
On evaluating query, timestamp argument always returns the same instance.
GenericUDFFromUtcTimestamp.applyOffset() does setTime() on the instance.
That means it adds all offsets in the query.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.10.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2012-05-10 21:31:31" id="2757" opendate="2012-01-27 02:21:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hive can&amp;apos;t find hadoop executor scripts without HADOOP_HOME set</summary>
			
			
			<description>The trouble is that in Hadoop 0.23 HADOOP_HOME has been deprecated. I think it would be really nice if bin/hive can be modified to capture the which hadoop
and pass that as a property into the JVM.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.10.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="breaks" type="Regression">3014</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2012-06-16 06:42:02" id="3062" opendate="2012-05-29 08:15:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Insert into table overwrites existing table if table name contains uppercase character</summary>
			
			
			<description>&quot;Insert into table &amp;lt;table-name&amp;gt; ~~&quot; is expected to append query result into the table. But when the table name contains uppercase character, it overwrite existing table.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.10.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">3939</link>
			
			
			<link description="is duplicated by" type="Duplicate">3064</link>
			
			
			<link description="relates to" type="Reference">3465</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2012-07-05 19:53:58" id="2498" opendate="2011-10-12 05:20:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Group by operator does not estimate size of Timestamp &amp; Binary data correctly</summary>
			
			
			<description>It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.10.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2012-07-10 23:03:37" id="3232" opendate="2012-07-05 19:45:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Resource Leak: Fix the File handle leak in EximUtil.java</summary>
			
			
			<description>1) Not closing the file handle EximUtil after reading the metadata from the file.
2) Nit: Get the path from URI to handle the Windows paths.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.10.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is required by" type="Required">2998</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2012-07-14 12:14:08" id="3064" opendate="2012-05-30 09:43:53" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>in &quot;insert into tablename&quot; statement,if the &quot;tablename&quot; contains uppercase characters this statement will overwrite the table</summary>
			
			
			<description>in &quot;insert into tablename&quot; statement,
if the &quot;tablename&quot; contains uppercase characters this statement will overwrite the table.
For Example:
hive&amp;gt; desc dual;
OK
dummy   string  
Time taken: 1.856 seconds
hive&amp;gt; select * from dual;
OK
dummy
Time taken: 3.133 seconds
drop table if exists tmp_test_1 ;
create EXTERNAL table tmp_test_1 (dummy string) partitioned by (dt string, hr string);
insert into table tmp_test_1 partition (dt=&amp;amp;apos;1&amp;amp;apos;, hr=&amp;amp;apos;1&amp;amp;apos;)  select * from dual;
insert into table tmp_TEST_1 partition (dt=&amp;amp;apos;1&amp;amp;apos;, hr=&amp;amp;apos;1&amp;amp;apos;) select count from dual;
select * from tmp_test_1;
Result :
OK
1       1       1
Time taken: 0.121 seconds</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">3062</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2012-11-08 09:53:26" id="2715" opendate="2012-01-12 19:41:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Upgrade Thrift dependency to 0.9.0</summary>
			
			
			<description>I work on HCatalog (0.2). Recently, we ran into HCat_server running out of memory every few days, and it boiled down to a bug in thrift, (THRIFT-1468, recently fixed).
HCat-0.2-branch depends on Hive-0.8, which in turn depends on thrift-0.5.0. (The bug also exists on 0.7.0.)
May I please enquire if Hive can&amp;amp;apos;t depend on a more current version of thrift? (Does it break the metastore?) I&amp;amp;apos;m afraid I&amp;amp;apos;m not privy to the reasoning behind Hive&amp;amp;apos;s dependency on a slightly dated thrift-lib. </description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.10.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ArchiveUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Order.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidObjectException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.IntString.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.UnknownDBException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.Complex.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.FieldSchema.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.NoSuchObjectException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.StringColumnStatsData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.SerDeInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.PrincipalType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.service.HiveClusterStatus.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.service.TestHiveServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFEvaluateNPE.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.QueryPlan.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.UnknownPartitionException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinUtil.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.metastore.api.Constants.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.PartitionEventType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.TestRCFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Schema.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaTool.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Stage.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Database.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.service.JobTrackerState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.AlreadyExistsException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.PrivilegeBag.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Adjacency.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.service.HiveServerException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.MetaException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Operator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectRef.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Type.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Role.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Partition.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.SkewedInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Index.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.AdjacencyType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.service.ThriftHive.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Query.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.UnknownTableException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.serde.Constants.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde.test.InnerStruct.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidInputException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Task.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidPartitionException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.NodeType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.RegexSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.MyEnum.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.Version.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.InvalidOperationException.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde.test.ThriftTestObj.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.TestStatsSerde.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.Graph.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.contrib.serde2.TestRegexSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.api.TaskType.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">183</link>
			
			
			<link description="is related to" type="Reference">1468</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2012-11-29 04:52:35" id="3596" opendate="2012-10-18 12:11:46" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Regression - HiveConf static variable causes issues in long running JVM instances with /tmp/ data</summary>
			
			
			<description>With Hive 0.8.x, HiveConf was changed to utilize the private, static member &quot;confVarURL&quot; which points to /tmp/hive-&amp;lt;user&amp;gt;-&amp;lt;tmp_number&amp;gt;.xml for job configuration settings. 
During long running JVMs, such as a Beeswax server, which creates multiple HiveConf objects over time this variable does not properly get updated between jobs and can cause job failure if the OS cleans /tmp/ during a cron job. </description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.8.1, 0.9.0, 0.10.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">3709</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2013-07-02 01:45:24" id="2517" opendate="2011-10-20 04:40:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Support group by on struct type</summary>
			
			
			<description>Currently group by on struct and union types are not supported. This issue will enable support for those.</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.12.0</fixedVersion>
			
			
			<type>New Feature</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">4491</link>
			
			
			<link description="relates to" type="Reference">2390</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2013-08-09 06:54:01" id="3191" opendate="2012-06-24 15:33:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>timestamp - timestamp causes null pointer exception</summary>
			
			
			<description>select tts.rnum, tts.cts - tts.cts from cert.tts tts
Error: Query returned non-zero code: 12, cause: FAILED: Hive Internal Error: java.lang.NullPointerException(null)
SQLState:  42000
ErrorCode: 12
create table if not exists CERT.TTS ( RNUM int , CTS timestamp) 
stored as sequencefile;</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.12.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">5021</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2013-08-14 16:33:43" id="3189" opendate="2012-06-24 15:21:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>cast ( &lt;string type&gt; as bigint) returning null values</summary>
			
			
			<description>select rnum, c1, cast(c1 as bigint) from cert.tsdchar tsdchar where rnum in (0,1,2)
create table if not exists CERT.TSDCHAR ( RNUM int , C1 string)
row format sequencefile
rnum	c1	_c2
0	-1                         	&amp;lt;null&amp;gt;
1	0                          	&amp;lt;null&amp;gt;
2	10                         	&amp;lt;null&amp;gt;</description>
			
			
			<version>0.8.0</version>
			
			
			<fixedVersion>0.12.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.TestUDFDateAdd.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
</bugrepository>
