<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="HIVE">
	<bug fixdate="2015-04-13 20:34:56" id="10322" opendate="2015-04-13 19:07:10" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>TestJdbcWithMiniHS2.testNewConnectionConfiguration fails</summary>
			
			
			<description>Fix test org.apache.hive.jdbc.TestJdbcWithMiniHS2.testNewConnectionConfiguration failed with following error:



org.apache.hive.service.cli.HiveSQLException: Failed to open new session: org.apache.hive.service.cli.HiveSQLException: java.lang.IllegalArgumentException: hive configuration hive.server2.thrift.http.max.worker.threads does not exists.

	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:243)

	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:234)

	at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:513)

	at org.apache.hive.jdbc.HiveConnection.&amp;lt;init&amp;gt;(HiveConnection.java:188)

	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)

	at java.sql.DriverManager.getConnection(DriverManager.java:571)

	at java.sql.DriverManager.getConnection(DriverManager.java:233)

	at org.apache.hive.jdbc.TestJdbcWithMiniHS2.testNewConnectionConfiguration(TestJdbcWithMiniHS2.java:275)

Caused by: org.apache.hive.service.cli.HiveSQLException: Failed to open new session: org.apache.hive.service.cli.HiveSQLException: java.lang.IllegalArgumentException: hive configuration hive.server2.thrift.http.max.worker.threads does not exists.



It seems related to HIVE-10271(remove hive.server2.thrift.http.min/max.worker.threads properties) </description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10309</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-04-13 20:52:39" id="10309" opendate="2015-04-11 08:23:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestJdbcWithMiniHS2.java broken because of the removal of hive.server2.thrift.http.max.worker.threads </summary>
			
			
			<description>HIVE-10271 removed hive.server2.thrift.http.min/max.worker.threads properties, however these properties are used in a few more places in hive code. For example, TestJdbcWithMiniHS2.java . We need to fix these as well.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10322</link>
			
			
			<link description="is broken by" type="Regression">10271</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-04-27 23:23:01" id="10493" opendate="2015-04-27 06:16:41" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Merge multiple joins when join keys are the same</summary>
			
			
			<description>CBO return path: auto_join3.q is joined on the same key from 3 sources. It is translated into 2 map joins. Need to merge them into a single one.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10071</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-04-28 19:34:51" id="10523" opendate="2015-04-28 19:23:14" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hive HCatalog Core 1.2.0 can not be built with hadoop-1 profile</summary>
			
			
			<description>I tried to built Hive 1.2.0 with hadoop-1 profile and got the following error in HCatalog Core



$ git status

On branch branch-1.2

Your branch is up-to-date with &amp;amp;apos;origin/branch-1.2&amp;amp;apos;



$ mvn clean install -DskipTests -Phadoop-1

...

[ERROR] COMPILATION ERROR : 

[INFO] -------------------------------------------------------------

[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[515,19] cannot find symbol

  symbol:   method isFile()

  location: variable fileStatus of type org.apache.hadoop.fs.FileStatus

[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[545,26] cannot find symbol

  symbol:   method isDirectory()

  location: variable fileStatus of type org.apache.hadoop.fs.FileStatus

[INFO] 2 errors 

[INFO] -------------------------------------------------------------

[INFO] ------------------------------------------------------------------------

[INFO] Reactor Summary:

[INFO] 

[INFO] Hive ............................................... SUCCESS [  3.181 s]

[INFO] Hive Shims Common .................................. SUCCESS [  4.292 s]

[INFO] Hive Shims 0.20S ................................... SUCCESS [  1.035 s]

[INFO] Hive Shims 0.23 .................................... SUCCESS [  5.692 s]

[INFO] Hive Shims Scheduler ............................... SUCCESS [  1.681 s]

[INFO] Hive Shims ......................................... SUCCESS [  1.302 s]

[INFO] Hive Common ........................................ SUCCESS [  4.787 s]

[INFO] Hive Serde ......................................... SUCCESS [  5.501 s]

[INFO] Hive Metastore ..................................... SUCCESS [ 15.634 s]

[INFO] Hive Ant Utilities ................................. SUCCESS [  0.695 s]

[INFO] Spark Remote Client ................................ SUCCESS [  9.376 s]

[INFO] Hive Query Language ................................ SUCCESS [01:19 min]

[INFO] Hive Service ....................................... SUCCESS [  5.310 s]

[INFO] Hive Accumulo Handler .............................. SUCCESS [  2.462 s]

[INFO] Hive JDBC .......................................... SUCCESS [  8.817 s]

[INFO] Hive Beeline ....................................... SUCCESS [  1.636 s]

[INFO] Hive CLI ........................................... SUCCESS [  4.843 s]

[INFO] Hive Contrib ....................................... SUCCESS [  1.501 s]

[INFO] Hive HBase Handler ................................. SUCCESS [ 11.925 s]

[INFO] Hive HCatalog ...................................... SUCCESS [  0.265 s]

[INFO] Hive HCatalog Core ................................. FAILURE [  1.003 s]

[INFO] Hive HCatalog Pig Adapter .......................... SKIPPED

[INFO] Hive HCatalog Server Extensions .................... SKIPPED

[INFO] Hive HCatalog Webhcat Java Client .................. SKIPPED

[INFO] Hive HCatalog Webhcat .............................. SKIPPED

[INFO] Hive HCatalog Streaming ............................ SKIPPED

[INFO] Hive HWI ........................................... SKIPPED

[INFO] Hive ODBC .......................................... SKIPPED

[INFO] Hive Shims Aggregator .............................. SKIPPED

[INFO] Hive TestUtils ..................................... SKIPPED

[INFO] Hive Packaging ..................................... SKIPPED

[INFO] ------------------------------------------------------------------------

[INFO] BUILD FAILURE

[INFO] ------------------------------------------------------------------------

[INFO] Total time: 02:51 min

[INFO] Finished at: 2015-04-28T12:20:09-07:00

[INFO] Final Memory: 179M/649M

[INFO] ------------------------------------------------------------------------

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-hcatalog-core: Compilation failure: Compilation failure:

[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[515,19] cannot find symbol

[ERROR] symbol:   method isFile()

[ERROR] location: variable fileStatus of type org.apache.hadoop.fs.FileStatus

[ERROR] /workhive/hive/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java:[545,26] cannot find symbol

[ERROR] symbol:   method isDirectory()

[ERROR] location: variable fileStatus of type org.apache.hadoop.fs.FileStatus

[ERROR] -&amp;gt; [Help 1]

[ERROR] 

[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.

[ERROR] Re-run Maven using the -X switch to enable full debug logging.

[ERROR] 

[ERROR] For more information about the errors and possible solutions, please read the following articles:

[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException

[ERROR] 

[ERROR] After correcting the problems, you can resume the build with the command

[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-hcatalog-core


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10444</link>
			
			
			<link description="relates to" type="Reference">10370</link>
			
			
			<link description="relates to" type="Reference">10442</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-04-30 06:25:41" id="10071" opendate="2015-03-24 18:01:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO (Calcite Return Path): Join to MultiJoin rule</summary>
			
			
			<description>CBO return path: auto_join3.q can be used to reproduce the problem.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10493</link>
			
			
			<link description="is related to" type="Reference">10533</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-05-02 00:40:47" id="10444" opendate="2015-04-22 18:56:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HIVE-10223 breaks hadoop-1 build</summary>
			
			
			<description>FileStatus.isFile() and FileStatus.isDirectory() methods added in HIVE-10223 are not present in hadoop 1.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">10443</link>
			
			
			<link description="is duplicated by" type="Duplicate">10523</link>
			
			
			<link description="relates to" type="Reference">10370</link>
			
			
			<link description="is related to" type="Reference">10066</link>
			
			
			<link description="is related to" type="Reference">10151</link>
			
			
			<link description="is broken by" type="Regression">10223</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-01 15:32:10" id="10879" opendate="2015-06-01 15:23:02" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>The bucket number is not respected in insert overwrite.</summary>
			
			
			<description>When hive.enforce.bucketing is true, the bucket number defined in the table is no longer respected in current master and 1.2. This is a regression.
Reproduce:

CREATE TABLE IF NOT EXISTS buckettestinput( 

data string 

) 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

CREATE TABLE IF NOT EXISTS buckettestoutput1( 

data string 

)CLUSTERED BY(data) 

INTO 2 BUCKETS 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

CREATE TABLE IF NOT EXISTS buckettestoutput2( 

data string 

)CLUSTERED BY(data) 

INTO 2 BUCKETS 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

Then I inserted the following data into the &quot;buckettestinput&quot; table

firstinsert1 

firstinsert2 

firstinsert3 

firstinsert4 

firstinsert5 

firstinsert6 

firstinsert7 

firstinsert8 

secondinsert1 

secondinsert2 

secondinsert3 

secondinsert4 

secondinsert5 

secondinsert6 

secondinsert7 

secondinsert8

set hive.enforce.bucketing = true; 

set hive.enforce.sorting=true;

insert overwrite table buckettestoutput1 

select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;;

set hive.auto.convert.sortmerge.join=true; 

set hive.optimize.bucketmapjoin = true; 

set hive.optimize.bucketmapjoin.sortedmerge = true; 

select * from buckettestoutput1 a join buckettestoutput2 b on (a.data=b.data);



Error: Error while compiling statement: FAILED: SemanticException [Error 10141]: Bucketed table metadata is not correct. Fix the metadata or don&amp;amp;apos;t use bucketed mapjoin, by setting hive.enforce.bucketmapjoin to false. The number of buckets for table buckettestoutput1 is 2, whereas the number of files is 1 (state=42000,code=10141)



The related debug information related to insert overwrite:

0: jdbc:hive2://localhost:10000&amp;gt; insert overwrite table buckettestoutput1 

select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;insert overwrite table buckettestoutput1 

0: jdbc:hive2://localhost:10000&amp;gt; ;

select * from buckettestinput where data like &amp;amp;apos; 

first%&amp;amp;apos;;

INFO  : Number of reduce tasks determined at compile time: 2

INFO  : In order to change the average load for a reducer (in bytes):

INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;

INFO  : In order to limit the maximum number of reducers:

INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;

INFO  : In order to set a constant number of reducers:

INFO  :   set mapred.reduce.tasks=&amp;lt;number&amp;gt;

INFO  : Job running in-process (local Hadoop)

INFO  : 2015-06-01 11:09:29,650 Stage-1 map = 86%,  reduce = 100%

INFO  : Ended Job = job_local107155352_0001

INFO  : Loading data to table default.buckettestoutput1 from file:/user/hive/warehouse/buckettestoutput1/.hive-staging_hive_2015-06-01_11-09-28_166_3109203968904090801-1/-ext-10000

INFO  : Table default.buckettestoutput1 stats: [numFiles=1, numRows=4, totalSize=52, rawDataSize=48]

No rows affected (1.692 seconds)


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10880</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-06 14:25:12" id="10881" opendate="2015-06-01 15:27:11" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>The bucket number is not respected in insert overwrite.</summary>
			
			
			<description>When hive.enforce.bucketing is true, the bucket number defined in the table is no longer respected in current master and 1.2. This is a regression.
Reproduce:

CREATE TABLE IF NOT EXISTS buckettestinput( 

data string 

) 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

CREATE TABLE IF NOT EXISTS buckettestoutput1( 

data string 

)CLUSTERED BY(data) 

INTO 2 BUCKETS 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

CREATE TABLE IF NOT EXISTS buckettestoutput2( 

data string 

)CLUSTERED BY(data) 

INTO 2 BUCKETS 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;

Then I inserted the following data into the &quot;buckettestinput&quot; table

firstinsert1 

firstinsert2 

firstinsert3 

firstinsert4 

firstinsert5 

firstinsert6 

firstinsert7 

firstinsert8 

secondinsert1 

secondinsert2 

secondinsert3 

secondinsert4 

secondinsert5 

secondinsert6 

secondinsert7 

secondinsert8

set hive.enforce.bucketing = true; 

set hive.enforce.sorting=true;

insert overwrite table buckettestoutput1 

select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;;

set hive.auto.convert.sortmerge.join=true; 

set hive.optimize.bucketmapjoin = true; 

set hive.optimize.bucketmapjoin.sortedmerge = true; 

select * from buckettestoutput1 a join buckettestoutput2 b on (a.data=b.data);



Error: Error while compiling statement: FAILED: SemanticException [Error 10141]: Bucketed table metadata is not correct. Fix the metadata or don&amp;amp;apos;t use bucketed mapjoin, by setting hive.enforce.bucketmapjoin to false. The number of buckets for table buckettestoutput1 is 2, whereas the number of files is 1 (state=42000,code=10141)



The related debug information related to insert overwrite:

0: jdbc:hive2://localhost:10000&amp;gt; insert overwrite table buckettestoutput1 

select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;insert overwrite table buckettestoutput1 

0: jdbc:hive2://localhost:10000&amp;gt; ;

select * from buckettestinput where data like &amp;amp;apos; 

first%&amp;amp;apos;;

INFO  : Number of reduce tasks determined at compile time: 2

INFO  : In order to change the average load for a reducer (in bytes):

INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;

INFO  : In order to limit the maximum number of reducers:

INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;

INFO  : In order to set a constant number of reducers:

INFO  :   set mapred.reduce.tasks=&amp;lt;number&amp;gt;

INFO  : Job running in-process (local Hadoop)

INFO  : 2015-06-01 11:09:29,650 Stage-1 map = 86%,  reduce = 100%

INFO  : Ended Job = job_local107155352_0001

INFO  : Loading data to table default.buckettestoutput1 from file:/user/hive/warehouse/buckettestoutput1/.hive-staging_hive_2015-06-01_11-09-28_166_3109203968904090801-1/-ext-10000

INFO  : Table default.buckettestoutput1 stats: [numFiles=1, numRows=4, totalSize=52, rawDataSize=48]

No rows affected (1.692 seconds)


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">10866</link>
			
			
			<link description="duplicates" type="Duplicate">10880</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-10 07:09:42" id="10971" opendate="2015-06-09 08:52:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>count(*) with count(distinct) gives wrong results when hive.groupby.skewindata=true</summary>
			
			
			<description>When hive.groupby.skewindata=true, the following query based on TPC-H gives wrong results:



set hive.groupby.skewindata=true;



select l_returnflag, count(*), count(distinct l_linestatus)

from lineitem

group by l_returnflag

limit 10;



The query plan shows that it generates only one MapReduce job instead of two theoretically, which is dictated by hive.groupby.skewindata=true.
The problem arises only when 

count(*)

 and 

count(distinct)

 exist together.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.2.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">7261</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-22 07:39:32" id="11069" opendate="2015-06-22 07:31:54" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>ColumnStatsTask doesn&amp;apos;t work with hive.exec.parallel</summary>
			
			
			<description>Try a simple query:



hive&amp;gt; set hive.exec.parallel=true;

hive&amp;gt; analyze table src compute statistics for columns;



It fails with errors similar to:



FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.ColumnStatsTask

hive&amp;gt; java.lang.RuntimeException: Error caching map.xml: java.io.IOException: java.lang.InterruptedException

	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:747)

	at org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Utilities.java:682)

	at org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Utilities.java:674)

	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:375)

	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)

	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)

	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)

	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)

Caused by: java.io.IOException: java.lang.InterruptedException

	at org.apache.hadoop.ipc.Client.call(Client.java:1450)

	at org.apache.hadoop.ipc.Client.call(Client.java:1402)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)

	at com.sun.proxy.$Proxy14.mkdirs(Unknown Source)

	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:539)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:606)

	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)

	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)

	at com.sun.proxy.$Proxy15.mkdirs(Unknown Source)

	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2758)

	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2729)

	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:870)

	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:866)

	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)

	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:866)

	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:859)

	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1954)

	at org.apache.hadoop.hive.ql.exec.Utilities.setPlanPath(Utilities.java:765)

	at org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Utilities.java:691)

	... 7 more

Caused by: java.lang.InterruptedException

	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)

	at java.util.concurrent.FutureTask.get(FutureTask.java:187)

	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1049)

	at org.apache.hadoop.ipc.Client.call(Client.java:1444)

	... 28 more

Job Submission failed with exception &amp;amp;apos;java.lang.RuntimeException(Error caching map.xml: java.io.IOException: java.lang.InterruptedException)&amp;amp;apos;



The problem is the Column Stats Task doesn&amp;amp;apos;t depend on the root task which causes errors. Here&amp;amp;apos;s the explain output:



hive&amp;gt; explain analyze table src compute statistics for columns;

OK

STAGE DEPENDENCIES:

  Stage-0 is a root stage

  Stage-1 is a root stage



STAGE PLANS:

  Stage: Stage-0

    Map Reduce

      Map Operator Tree:

          TableScan

            alias: src

            Select Operator

              expressions: key (type: string), value (type: string)

              outputColumnNames: key, value

              Group By Operator

                aggregations: compute_stats(key, 16), compute_stats(value, 16)

                mode: hash

                outputColumnNames: _col0, _col1

                Reduce Output Operator

                  sort order:

                  value expressions: _col0 (type: struct&amp;lt;columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int&amp;gt;), _col1 (type: struct&amp;lt;columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int&amp;gt;)

      Reduce Operator Tree:

        Group By Operator

          aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1)

          mode: mergepartial

          outputColumnNames: _col0, _col1

          File Output Operator

            compressed: false

            table:

                input format: org.apache.hadoop.mapred.TextInputFormat

                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe



  Stage: Stage-1

    Column Stats Work

      Column Stats Desc:

          Columns: key, value

          Column Types: string, string

          Table: default.src



Time taken: 0.761 seconds, Fetched: 39 row(s)



For reference, here&amp;amp;apos;s the corresponding output in Hive 0.13:



hive&amp;gt; explain analyze table orders compute statistics for columns;

OK

STAGE DEPENDENCIES:

  Stage-0 is a root stage

  Stage-1 depends on stages: Stage-0



STAGE PLANS:

  Stage: Stage-0

    Map Reduce

      Map Operator Tree:

          TableScan

            alias: orders

            Statistics: Num rows: 0 Data size: 13310103552 Basic stats: PARTIAL Column stats: COMPLETE



  Stage: Stage-1

    Stats-Aggr Operator



Time taken: 2.142 seconds, Fetched: 15 row(s)


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10677</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-29 14:23:36" id="11112" opendate="2015-06-25 17:18:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ISO-8859-1 text output has fragments of previous longer rows appended</summary>
			
			
			<description>If a LazySimpleSerDe table is created using ISO 8859-1 encoding, query results for a string column are incorrect for any row that was preceded by a row containing a longer string.
Example steps to reproduce:
1. Create a table using ISO 8859-1 encoding:



CREATE TABLE person_lat1 (name STRING)

ROW FORMAT SERDE &amp;amp;apos;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&amp;amp;apos; WITH SERDEPROPERTIES (&amp;amp;apos;serialization.encoding&amp;amp;apos;=&amp;amp;apos;ISO8859_1&amp;amp;apos;);



2. Copy an ISO-8859-1 encoded text file into the appropriate warehouse folder in HDFS. I&amp;amp;apos;ll attach an example file containing the following text: 

Mller,Thomas

Jrgensen,Jrgen

Pea,Andrs

Nm,Fk



3. Execute SELECT * FROM person_lat1
Result - The following output appears:

+-------------------+--+

| person_lat1.name |

+-------------------+--+

| Mller,Thomas |

| Jrgensen,Jrgen |

| Pea,Andrsrgen |

| Nm,Fkdrsrgen |

+-------------------+--+


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10983</link>
			
			
			<link description="is part of" type="Incorporates">10983</link>
			
			
			<link description="relates to" type="Reference">11095</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-08-07 13:20:36" id="10880" opendate="2015-06-01 15:24:59" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>The bucket number is not respected in insert overwrite.</summary>
			
			
			<description>When hive.enforce.bucketing is true, the bucket number defined in the table is no longer respected in current master and 1.2. 
Reproduce:



CREATE TABLE IF NOT EXISTS buckettestinput( 

data string 

) 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;



CREATE TABLE IF NOT EXISTS buckettestoutput1( 

data string 

)CLUSTERED BY(data) 

INTO 2 BUCKETS 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;



CREATE TABLE IF NOT EXISTS buckettestoutput2( 

data string 

)CLUSTERED BY(data) 

INTO 2 BUCKETS 

ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;;



Then I inserted the following data into the &quot;buckettestinput&quot; table:

firstinsert1 

firstinsert2 

firstinsert3 

firstinsert4 

firstinsert5 

firstinsert6 

firstinsert7 

firstinsert8 

secondinsert1 

secondinsert2 

secondinsert3 

secondinsert4 

secondinsert5 

secondinsert6 

secondinsert7 

secondinsert8






set hive.enforce.bucketing = true; 

set hive.enforce.sorting=true;

insert overwrite table buckettestoutput1 

select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;;

set hive.auto.convert.sortmerge.join=true; 

set hive.optimize.bucketmapjoin = true; 

set hive.optimize.bucketmapjoin.sortedmerge = true; 

select * from buckettestoutput1 a join buckettestoutput2 b on (a.data=b.data);




Error: Error while compiling statement: FAILED: SemanticException [Error 10141]: Bucketed table metadata is not correct. Fix the metadata or don&amp;amp;apos;t use bucketed mapjoin, by setting hive.enforce.bucketmapjoin to false. The number of buckets for table buckettestoutput1 is 2, whereas the number of files is 1 (state=42000,code=10141)



The related debug information related to insert overwrite:

0: jdbc:hive2://localhost:10000&amp;gt; insert overwrite table buckettestoutput1 

select * from buckettestinput where data like &amp;amp;apos;first%&amp;amp;apos;insert overwrite table buckettestoutput1 

0: jdbc:hive2://localhost:10000&amp;gt; ;

select * from buckettestinput where data like &amp;amp;apos; 

first%&amp;amp;apos;;

INFO  : Number of reduce tasks determined at compile time: 2

INFO  : In order to change the average load for a reducer (in bytes):

INFO  :   set hive.exec.reducers.bytes.per.reducer=&amp;lt;number&amp;gt;

INFO  : In order to limit the maximum number of reducers:

INFO  :   set hive.exec.reducers.max=&amp;lt;number&amp;gt;

INFO  : In order to set a constant number of reducers:

INFO  :   set mapred.reduce.tasks=&amp;lt;number&amp;gt;

INFO  : Job running in-process (local Hadoop)

INFO  : 2015-06-01 11:09:29,650 Stage-1 map = 86%,  reduce = 100%

INFO  : Ended Job = job_local107155352_0001

INFO  : Loading data to table default.buckettestoutput1 from file:/user/hive/warehouse/buckettestoutput1/.hive-staging_hive_2015-06-01_11-09-28_166_3109203968904090801-1/-ext-10000

INFO  : Table default.buckettestoutput1 stats: [numFiles=1, numRows=4, totalSize=52, rawDataSize=48]

No rows affected (1.692 seconds)



Insert use dynamic partition does not have the issue. </description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10879</link>
			
			
			<link description="is duplicated by" type="Duplicate">10881</link>
			
			
			<link description="relates to" type="Reference">11360</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-08-11 03:31:28" id="11498" opendate="2015-08-07 05:03:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HIVE Authorization v2 should not check permission for dummy entity</summary>
			
			
			<description>The queries like SELECT 1+1;, The target table and database will set to _dummy_database _dummy_table, authorization should skip these kinds of databases or tables.
For authz v1. it has skip them.
eg1. Source code at github

for (WriteEntity write : outputs) {

        if (write.isDummy() || write.isPathType()) {

          continue;

        }



eg2. Source code at github

for (ReadEntity read : inputs) {

        if (read.isDummy() || read.isPathType()) {

          continue;

        }

       ...

        }



...
This patch will fix authz v2.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 1.2.1, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10625</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-08-20 01:23:26" id="11502" opendate="2015-08-08 21:34:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Map side aggregation is extremely slow</summary>
			
			
			<description>For the query as following:

create table tbl2 as 

select col1, max(col2) as col2 

from tbl1 group by col1;



If the column for group by has many different values (for example 400000) and it is in type double, the map side aggregation is very slow. I ran the query which took more than 3 hours , after 3 hours, I have to kill the query.
The same query can finish in 7 seconds, if I turn off map side aggregation by:

set hive.map.aggr = false;


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">9495</link>
			
			
			<link description="is duplicated by" type="Duplicate">12093</link>
			
			
			<link description="relates to" type="Reference">11761</link>
			
			
			<link description="is related to" type="Reference">12217</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-08-27 18:16:26" id="11123" opendate="2015-06-26 04:14:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix how to confirm the RDBMS product name at Metastore.</summary>
			
			
			<description>I use PostgreSQL to Hive Metastore. And I saw the following message at PostgreSQL log.



&amp;lt; 2015-06-26 10:58:15.488 JST &amp;gt;ERROR:  syntax error at or near &quot;@@&quot; at character 5

&amp;lt; 2015-06-26 10:58:15.488 JST &amp;gt;STATEMENT:  SET @@session.sql_mode=ANSI_QUOTES

&amp;lt; 2015-06-26 10:58:15.489 JST &amp;gt;ERROR:  relation &quot;v$instance&quot; does not exist at character 21

&amp;lt; 2015-06-26 10:58:15.489 JST &amp;gt;STATEMENT:  SELECT version FROM v$instance

&amp;lt; 2015-06-26 10:58:15.490 JST &amp;gt;ERROR:  column &quot;version&quot; does not exist at character 10

&amp;lt; 2015-06-26 10:58:15.490 JST &amp;gt;STATEMENT:  SELECT @@version



When Hive CLI and Beeline embedded mode are carried out, this message is output to PostgreSQL log.
These queries are called from MetaStoreDirectSql#determineDbType. And if we use MetaStoreDirectSql#getProductName, we need not to call these queries.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11859</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-01 23:09:09" id="11566" opendate="2015-08-14 20:28:02" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hybrid grace hash join should only allocate write buffer for a hash partition when first write happens</summary>
			
			
			<description>Currently it&amp;amp;apos;s allocating one write buffer for a number of hash partitions up front, which can cause GC pause.
It&amp;amp;apos;s better to do the write buffer allocation on demand.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11587</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-10 19:02:28" id="11587" opendate="2015-08-17 21:09:30" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix memory estimates for mapjoin hashtable</summary>
			
			
			<description>Due to the legacy in in-memory mapjoin and conservative planning, the memory estimation code for mapjoin hashtable is currently not very good. It allocates the probe erring on the side of more memory, not taking data into account because unlike the probe, it&amp;amp;apos;s free to resize, so it&amp;amp;apos;s better for perf to allocate big probe and hope for the best with regard to future data size. It is not true for hybrid case.
There&amp;amp;apos;s code to cap the initial allocation based on memory available (memUsage argument), but due to some code rot, the memory estimates from planning are not even passed to hashtable anymore (there used to be two config settings, hashjoin size fraction by itself, or hashjoin size fraction for group by case), so it never caps the memory anymore below 1 Gb. 
Initial capacity is estimated from input key count, and in hybrid join cache can exceed Java memory due to number of segments.
There needs to be a review and fix of all this code.
Suggested improvements:
1) Make sure &quot;initialCapacity&quot; argument from Hybrid case is correct given the number of segments. See how it&amp;amp;apos;s calculated from keys for regular case; it needs to be adjusted accordingly for hybrid case if not done already.
1.5) Note that, knowing the number of rows, the maximum capacity one will ever need for probe size (in longs) is row count (assuming key per row, i.e. maximum possible number of keys) divided by load factor, plus some very small number to round up. That is for flat case. For hybrid case it may be more complex due to skew, but that is still a good upper bound for the total probe capacity of all segments.
2) Rename memUsage to maxProbeSize, or something, make sure it&amp;amp;apos;s passed correctly based on estimates that take into account both probe and data size, esp. in hybrid case.
3) Make sure that memory estimation for hybrid case also doesn&amp;amp;apos;t come up with numbers that are too small, like 1-byte hashtable. I am not very familiar with that code but it has happened in the past.
Other issues we have seen:
4) Cap single write buffer size to 8-16Mb. The whole point of WBs is that you should not allocate large array in advance. Even if some estimate passes 500Mb or 40Mb or whatever, it doesn&amp;amp;apos;t make sense to allocate that.
5) For hybrid, don&amp;amp;apos;t pre-allocate WBs - only allocate on write.
6) Change everywhere rounding up to power of two is used to rounding down, at least for hybrid case 
I wanted to put all of these items in single JIRA so we could keep track of fixing all of them.
I think there are JIRAs for some of these already, feel free to link them to this one.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11566</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-17 18:42:14" id="11859" opendate="2015-09-17 03:42:01" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Bunch of SQL error reported for Hive with SQL Server as Metastore</summary>
			
			
			<description>We are getting a lot of SQL errors reported at SQL Server end. Our set up is a Two Node Hive cluster using SQL Server as MetaStore.
Here is a snippet of SQL Server errors
Invalid object name &amp;amp;apos;v$instance&amp;amp;apos;. Caused by: SELECT version FROM v$instance
Must declare the scalar variable &quot;@@session&quot;.Caused by: SET @@session.sql_mode=ANSI_QUOTES
This is filling our logs at a very rapid rate, which we are forced to purge. 
Please note these queries are for MySQL / Oracle.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11123</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-30 18:45:29" id="11920" opendate="2015-09-22 20:39:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ADD JAR failing with URL schemes other than file/ivy/hdfs</summary>
			
			
			<description>Example stack trace below. It looks like this was introduced by HIVE-9664.

015-09-16 19:53:16,502 ERROR [main]: SessionState (SessionState.java:printError(960)) - invalid url: wasb:///tmp/hive-udfs-0.1.jar, expecting ( file | hdfs | ivy)  as url scheme.

java.lang.RuntimeException: invalid url: wasb:///tmp/hive-udfs-0.1.jar, expecting ( file | hdfs | ivy)  as url scheme.

        at org.apache.hadoop.hive.ql.session.SessionState.getURLType(SessionState.java:1230)

        at org.apache.hadoop.hive.ql.session.SessionState.resolveAndDownload(SessionState.java:1237)

        at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1163)

        at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1149)

        at org.apache.hadoop.hive.ql.exec.FunctionTask.addFunctionResources(FunctionTask.java:301)

        at org.apache.hadoop.hive.ql.exec.Registry.registerToSessionRegistry(Registry.java:453)

        at org.apache.hadoop.hive.ql.exec.Registry.registerPermanentFunction(Registry.java:200)

        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerPermanentFunction(FunctionRegistry.java:1495)

        at org.apache.hadoop.hive.ql.exec.FunctionTask.createPermanentFunction(FunctionTask.java:136)

        at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:75)

        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)

        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)

        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)

        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)

        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)

        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)

        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)

        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)

        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)

        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)

        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:606)

        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11850</link>
			
			
			<link description="is related to" type="Reference">9664</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-14 20:50:52" id="10755" opendate="2015-05-19 18:32:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Rework on HIVE-5193 to enhance the column oriented table access</summary>
			
			
			<description>Add the support of column pruning for column oriented table access which was done in HIVE-5193 but was reverted due to the join issue in HIVE-10720.
In 1.3.0, the patch posted by Viray didn&amp;amp;apos;t work, probably due to some jar reference. That seems to get fixed and that patch works in 2.0.0 now.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.pig.HCatLoader.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12006</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-29 17:24:24" id="12275" opendate="2015-10-27 17:52:20" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Query results on macro_duplicate.q are in different order on some environments</summary>
			
			
			<description>On my Linux VM, the order is different from the golden file. Seems to work on Mac as well as on the Pre-Commit tests. We can add an order-by to make the results deterministic across environments.

55d54

&amp;lt; 16    25      24      120     8       10      6

56a56

&amp;gt; 16    25      24      120     8       10      6

Exception: Client Execution results failed with error code = 1


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12277</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-30 17:18:19" id="12277" opendate="2015-10-27 18:39:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive macro results on macro_duplicate.q different after adding ORDER BY</summary>
			
			
			<description>Added an order-by to the query in macro_duplicate.q:

-select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing;

\ No newline at end of file

+select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing order by int(c);



And the results from math_add() changed unexpectedly:

-1      4       1       2       2       4       3

-16     25      24      120     8       10      6

+1      4       1       2       1       4       3

+16     25      24      120     16      25      6


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12275</link>
			
			
			<link description="relates to" type="Reference">2655</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-03 04:10:57" id="7428" opendate="2014-07-16 17:44:21" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>OrcSplit fails to account for columnar projections in its size estimates</summary>
			
			
			<description>Currently, ORC generates splits based on stripe offset + stripe length.
This means that the splits for all columnar projections are exactly the same size, despite reading the footer which gives the estimated sizes for each column.
This is a hold-out from FileSplit which uses getLen() as the I/O cost of reading a file in a map-task.
RCFile didn&amp;amp;apos;t have a footer with column statistics information, but for ORC this would be extremely useful to reduce task overheads when processing extremely wide tables with highly selective column projections.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.ql.io.ColumnarSplit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">10497</link>
			
			
			<link description="is duplicated by" type="Duplicate">10397</link>
			
			
			<link description="relates to" type="Reference">11546</link>
			
			
			<link description="relates to" type="Reference">1993</link>
			
			
			<link description="supercedes" type="Supercedes">10397</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-20 07:25:49" id="12450" opendate="2015-11-18 01:08:12" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>OrcFileMergeOperator does not use correct compression buffer size</summary>
			
			
			<description>OrcFileMergeOperator checks for compatibility before merging orc files. This compatibility check include checking compression buffer size. But the output file that is created does not honor the compression buffer size and always defaults to 256KB. This will not be a problem when reading the orc file but can create unwanted memory pressure because of wasted space within compression buffer.
This issue also can make the merged file unreadable under certain cases. For example, if the original compression buffer size is 8KB and if  hive.exec.orc.default.buffer.size is set to 4KB. The merge file operator will use 4KB instead of actual 8KB which can result in hanging of ORC reader (more specifically ZlibCodec will wait for more compression buffers). 
jstack output for hanging issue


&quot;main&quot; prio=5 tid=0x00007fc073000000 nid=0x1703 runnable [0x0000700000218000]

   java.lang.Thread.State: RUNNABLE

	at java.util.zip.Inflater.inflateBytes(Native Method)

	at java.util.zip.Inflater.inflate(Inflater.java:259)

	- locked &amp;lt;0x00000007f5d5fdc8&amp;gt; (a java.util.zip.ZStreamRef)

	at org.apache.hadoop.hive.ql.io.orc.ZlibCodec.decompress(ZlibCodec.java:94)

	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:238)

	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:262)

	at java.io.InputStream.read(InputStream.java:101)

	at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)

	at com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)

	at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)

	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter.&amp;lt;init&amp;gt;(OrcProto.java:10661)

	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter.&amp;lt;init&amp;gt;(OrcProto.java:10625)

	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter$1.parsePartialFrom(OrcProto.java:10730)

	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter$1.parsePartialFrom(OrcProto.java:10725)

	at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:89)

	at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:95)

	at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)

	at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeFooter.parseFrom(OrcProto.java:10958)

	at org.apache.hadoop.hive.ql.io.orc.MetadataReaderImpl.readStripeFooter(MetadataReaderImpl.java:114)

	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripeFooter(RecordReaderImpl.java:240)

	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.beginReadStripe(RecordReaderImpl.java:847)

	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:818)

	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1033)

	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1068)

	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.&amp;lt;init&amp;gt;(RecordReaderImpl.java:217)

	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:638)

	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(ReaderImpl.java:625)

	at org.apache.hadoop.hive.ql.io.orc.FileDump.printMetaData(FileDump.java:162)

	at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:110)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:606)

	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-23 20:47:35" id="12406" opendate="2015-11-13 07:39:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HIVE-9500 introduced incompatible change to LazySimpleSerDe public interface</summary>
			
			
			<description>In the process of fixing HIVE-9500, an incompatibility was introduced that will break 3rd party code that relies on LazySimpleSerde. In HIVE-9500, the nested class SerDeParamaters was removed and the method LazySimpleSerDe.initSerdeParms was also removed. They were replaced by a standalone class LazySerDeParameters.
Since this has already been released, I don&amp;amp;apos;t think we should revert the change since that would mean breaking compatibility again. Instead, the best approach would be to support both interfaces, if possible. </description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">9500</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-05 00:01:40" id="12563" opendate="2015-12-02 08:28:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NullPointerException with 3-way Tez merge join</summary>
			
			
			<description>Issue occurs in Tez merge joins with 3 way join (A join B join C), where A is the big table, and A and C end up having 0 rows after table filters.
Was able to repro this issue with using the sample tables in the Hive unit tests:

select

  a.key, b.value, c.value

from

  src a,

  src1 b,

  src c

where

  a.key = b.key and a.key = c.key

  and a.value &amp;gt; &amp;amp;apos;wal_6789&amp;amp;apos;

  and c.value &amp;gt; &amp;amp;apos;wal_6789&amp;amp;apos;




], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:&quot;146&quot;},&quot;value&quot;:{&quot;_col0&quot;:&quot;val_146&quot;}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:744)

Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:&quot;146&quot;},&quot;value&quot;:{&quot;_col0&quot;:&quot;val_146&quot;}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:310)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:&quot;146&quot;},&quot;value&quot;:{&quot;_col0&quot;:&quot;val_146&quot;}}

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:312)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:293)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:501)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:416)

	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:617)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:287)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:&quot;146&quot;},&quot;value&quot;:{&quot;_col0&quot;:&quot;val_146&quot;}}

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchOneRow(CommonMergeJoinOperator.java:439)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchNextGroup(CommonMergeJoinOperator.java:407)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:310)

	... 19 more

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:&quot;146&quot;},&quot;value&quot;:{&quot;_col0&quot;:&quot;val_146&quot;}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:302)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchOneRow(CommonMergeJoinOperator.java:431)

	... 21 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=1) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:&quot;146&quot;},&quot;value&quot;:{&quot;_col0&quot;:&quot;val_146&quot;}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:370)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:292)

	... 22 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.process(CommonMergeJoinOperator.java:288)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:361)

	... 23 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1449041190339_0001_1_03 [Reducer 2] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:0


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-04 18:45:45" id="12951" opendate="2016-01-28 00:45:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Reduce Spark executor prewarm timeout to 5s</summary>
			
			
			<description>Currently it&amp;amp;apos;s set to 30s, which tends to be longer than needed. Reduce it to 5s, only considering jvm startup time. (Eventually, we may want to make this configurable.)</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11363</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-09 02:20:24" id="12999" opendate="2016-02-04 03:29:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Tez: Vertex creation reduce NN IPCs</summary>
			
			
			<description>Tez vertex building has a decidedly slow path in the code, which is not related to the DAG plan at all.
The total number of RPC calls is not related to the total number of operators, due to a bug in the DagUtils inner loops.



	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1877)

	at org.apache.hadoop.hive.ql.exec.Utilities.createTmpDirs(Utilities.java:3207)

	at org.apache.hadoop.hive.ql.exec.Utilities.createTmpDirs(Utilities.java:3170)

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:548)

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1151)

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:388)

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-11 23:55:00" id="13020" opendate="2016-02-08 04:48:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive Metastore and HiveServer2 to Zookeeper fails with IBM JDK</summary>
			
			
			<description>HiveServer2 and Hive Metastore Zookeeper component is hardcoded to only support the Oracle/Open JDK. I was performing testing of Hadoop running on the IBM JDK and discovered this issue and have since drawn up the attached patch. This looks to resolve the issue in a similar manner as how the Hadoop core folks handle the IBM JDK.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Utils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-25 07:01:02" id="13147" opendate="2016-02-24 18:31:37" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>COLUMN_STATS_ACCURATE is not accurate</summary>
			
			
			<description>Often we see on a described table:



Table Parameters:	NULL	NULL

18		COLUMN_STATS_ACCURATE	true                

19		numFiles            	1                   

20		numRows             	0                   

21		rawDataSize         	0                   

22		totalSize           	46069               

23		transient_lastDdlTime	1448930216          



Notice



20		numRows             	0                   

21		rawDataSize         	0       



are wrong.
After doing an analyze we get:



Table Parameters:	NULL	NULL

18		COLUMN_STATS_ACCURATE	true                

19		numFiles            	1                   

20		numRows             	823                 

21		rawDataSize         	45246               

22		totalSize           	46069               

23		transient_lastDdlTime	1456338426 


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.StatsWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.StatsSetupConst.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12661</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-07 02:41:18" id="13063" opendate="2016-02-16 00:13:40" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Create UDFs for CHR and REPLACE </summary>
			
			
			<description>Create UDFS for these functions.
CHR: convert n where n : [0, 256) into the ascii equivalent as a varchar. If n is less than 0 or greater than 255, return the empty string. If n is 0, return null.
REPLACE: replace all substrings of &amp;amp;apos;str&amp;amp;apos; that match &amp;amp;apos;search&amp;amp;apos; with &amp;amp;apos;rep&amp;amp;apos;.
Example. SELECT REPLACE(&amp;amp;apos;Hack and Hue&amp;amp;apos;, &amp;amp;apos;H&amp;amp;apos;, &amp;amp;apos;BL&amp;amp;apos;);
Equals &amp;amp;apos;BLack and BLue&amp;amp;apos;&quot;</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">14581</link>
			
			
			<link description="is duplicated by" type="Duplicate">14581</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-09 19:17:33" id="13216" opendate="2016-03-07 05:51:37" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ORC Reader will leave file open until GC when opening a malformed ORC file</summary>
			
			
			<description>In ORC extractMetaInfoFromFooter method of ReaderImpl.java:
A new input stream is open without try-catch-finally to enforce closing.
Once the footer parse has some exception, the stream close will miss. 
Until GC happen to close the stream.
private static FileMetaInfo extractMetaInfoFromFooter(FileSystem fs,
                                                        Path path,
                                                        long maxFileLength
                                                        ) throws IOException 
{

    FSDataInputStream file = fs.open(path);



    ...

    file.close();



    return new FileMetaInfo(

        ps.getCompression().toString(),

        (int) ps.getCompressionBlockSize(),

        (int) ps.getMetadataLength(),

        buffer,

        ps.getVersionList(),

        writerVersion

        );

  }</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-21 15:02:06" id="13141" opendate="2016-02-24 07:54:22" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive on Spark over HBase should accept parameters starting with &quot;zookeeper.znode&quot;</summary>
			
			
			<description>HBase related paramters has been added by HIVE-12708.
Following the same way,parameters starting with &quot;zookeeper.znode&quot; should be add too,which are also HBase related paramters .
Refering to http://blog.cloudera.com/blog/2013/10/what-are-hbase-znodes/
I have seen a failure with Hive on Spark over HBase  due to customize zookeeper.znode.parent.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-15 11:50:35" id="13415" opendate="2016-04-04 09:15:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Decouple Sessions from thrift binary transport</summary>
			
			
			<description>Current behaviour is:

Open a thrift binary transport
create a session
close the transport

Then the session gets closed. Consequently, all the operations running in the session also get killed.
Whereas, if you open an HTTP transport, and close, the enclosing sessions are not closed. 
This seems like a bad design, having transport and sessions tightly coupled. I&amp;amp;apos;d like to fix this. 
The issue that introduced it is HIVE-9601 Relevant discussions at here, here and mentioned links on those comments. 
Another thing that seems like a slightly bad design is this line of code in ThriftBinaryCLIService:

server.setServerEventHandler(serverEventHandler);



Whereas serverEventHandler is defined by the base class, with no users except one sub-class(ThriftBinaryCLIService), violating the separation of concerns. </description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.TestRetryingThriftCLIServiceClient.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">518</link>
			
			
			<link description="relates to" type="Reference">9601</link>
			
			
			<link description="is related to" type="Reference">14227</link>
			
			
			<link description="is depended upon by" type="dependent">1150</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-27 23:28:12" id="13493" opendate="2016-04-12 18:54:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix TransactionBatchImpl.getCurrentTxnId() and mis logging fixes</summary>
			
			
			<description>sort list of transaction IDs deleted by performTimeouts
sort list of &quot;empty aborted&quot;
log the list of lock id removed due to timeout
fix TransactionBatchImpl.getCurrentTxnId() not to look past end of array (see HIVE-13489)
beginNextTransactionImpl()
if ( currentTxnIndex &amp;gt;= txnIds.size() )//todo: this condition is bogus should check currentTxnIndex + 1</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13489</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-05 22:29:55" id="13395" opendate="2016-03-31 00:57:49" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Lost Update problem in ACID</summary>
			
			
			<description>ACID users can run into Lost Update problem.
In Hive 1.2, Driver.recordValidTxns() (which records the snapshot to use for the query) is called in Driver.compile().
Now suppose to concurrent &quot;update T set x = x + 1&quot; are executed.  (for simplicity assume there is exactly 1 row in T)
What can happen is that both compile at the same time (more precisely before acquireLocksAndOpenTxn() in runInternal() is called) and thus will lock in the same snapshot, say the value of x = 7 in this snapshot.
Now 1 will get the lock on the row, the second will block.  
Now 1, makes x = 8 and commits.
Now 2 proceeds and makes x = 8 again since in it&amp;amp;apos;s snapshot x is still 7.
This specific issue is solved in Hive 1.3/2.0 (HIVE-11077 which is a large patch that deals with multi-statement txns) by moving recordValidTxns() after locks are acquired which reduces the likelihood of this but doesn&amp;amp;apos;t eliminate the problem.

Even in 1.3 version of the code, you could have the same issue.  Assume the same 2 queries:
Both start a txn, say txnid 9 and 10.  Say 10 gets the lock first, 9 blocks.
10 updates the row (so x = 8) and thus ReaderKey.currentTransactionId=10.
10 commits.
Now 9 can proceed and it will get a snapshot that includes 10, i.e. it will see x = 8 and it will write x = 9, but it will set ReaderKey.currentTransactionId = 9.  Thus when merge logic runs, it will see x = 8 is the later version of this row, i.e. lost update.
The problem is that locks alone are insufficient for MVCC architecture.  

At lower level Row ID has (originalTransactionId, rowid, bucket id, currentTransactionId) and since on update/delete we do a table scan, we could check that we are about to write a row with currentTransactionId &amp;lt; (currentTransactionId of row we&amp;amp;apos;ve read) and fail the query.  Currently, currentTransactionId is not surfaced at higher level where this check can be made.
This would not work (efficiently) longer term where we want to support fast update on user defined PK vis streaming ingest.
Also, this would not work with multi statement txns since in that case we&amp;amp;apos;d lock in the snapshot at the start of the txn, but then 2nd, 3rd etc queries would use the same snapshot and the locks for these queries would be acquired after the snapshot is locked in so this would be the same situation as pre HIVE-11077.

A more robust solution (commonly used with MVCC) is to keep track of start and commit time (logical counter) or each transaction to detect if two txns overlap.  The 2nd part is to keep track of write-set, i.e. which data (rows, partitions, whatever appropriate level of granularity is) were modified by any txn and if 2 txns overlap in time and wrote the same element, abort later one.  This is called first-committer-wins rule.  This requires a MS DB schema change
It would be most convenient to use the same sequence for txnId, start and commit time (in which case txnid=start time).  In this case we&amp;amp;apos;d need to add 1 filed to TXNS table.  The complication here is that we&amp;amp;apos;ll be using elements of the sequence faster and they are used as part of file name of delta and base dir and currently limited to 7 digits which can be exceeded.  So this would require some thought to handling upgrade/migration.
Also, write-set tracking requires either additional metastore table or keeping info in HIVE_LOCKS around longer with new state.

In the short term, on SQL side of things we could (in auto commit mode only)
acquire the locks first and then open the txn AND update these locks with txn id.
This implies another Thrift change to pass in lockId to openTxn.
The same would not work for Streaming API since it opens several txns at once and then acquires locks for each.
(Not sure if that&amp;amp;apos;s is an issue or not since Streaming only does Insert).
Either way this feels hacky.

Here is one simple example why we need Write-Set tracking for multi-statement txns
Consider transactions T 1 and T 2:
T 1: r 1[x] -&amp;gt; w 1[y] -&amp;gt; c 1 
T 2: w 2[x] -&amp;gt; w 2[y] -&amp;gt; c 2 
Suppose the order of operations is r 1[x] w 2[x].... then a conventional R/W lock manager w/o MVCSS will block the write from T 2 
With MVCC we don&amp;amp;apos;t want readers to interfere with writers and so the following schedule is possible (because Hive&amp;amp;apos;s semi-shared (write) don&amp;amp;apos;t conflict with shared (read) locks) in Hive&amp;amp;apos;s current implementation.
r 1[x] w 2[x] w 2[y] c 2 w 1[y] c 1
By the time w 1[y] happens, T 2 has committed and released it&amp;amp;apos;s locks.  But this is a lost update if c 1 is allowed to commit.  That&amp;amp;apos;s where write-set tracking comes in.</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.TestAcidUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">13622</link>
			
			
			<link description="blocks" type="Blocker">13354</link>
			
			
			<link description="is blocked by" type="Blocker">13664</link>
			
			
			<link description="relates to" type="Reference">14047</link>
			
			
			<link description="is related to" type="Reference">11077</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-10 16:49:07" id="13676" opendate="2016-05-03 08:15:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Tests failing because metastore doesn&amp;apos;t come up</summary>
			
			
			<description>In 5-6 test classes, metastore is required to be up for tests to run. The metastore is started in setup Phase asynchronously. But there&amp;amp;apos;s no logic to wait till the metastore comes up. Hence, sometimes tests run even when metastore isn&amp;amp;apos;t up and fail. </description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.api.TestHCatClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.TestHiveRemote.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-27 07:24:48" id="13561" opendate="2016-04-20 21:41:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used</summary>
			
			
			<description>I can repo this on branch-1.2 and branch-2.0.
It looks to be the same issues as: HIVE-11408
The patch from HIVE-11408 looks to fix the issue as well.
I&amp;amp;apos;ve updated the patch from HIVE-11408 to be aligned with branch-1.2 and master
</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">11408</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-10 17:28:16" id="14113" opendate="2016-06-28 03:10:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Create function failed but function in show function list</summary>
			
			
			<description>1. create function with invalid hdfs path, /udf/udf-test.jar does not exists

create function my_lower as &amp;amp;apos;com.tang.UDFLower&amp;amp;apos; using jar &amp;amp;apos;hdfs:///udf/udf-test.jar&amp;amp;apos;;
Failed with following exception:

0: jdbc:hive2://189.39.151.44:10000/&amp;gt; create function my_lower as &amp;amp;apos;com.tang.UDFLower&amp;amp;apos; using jar &amp;amp;apos;hdfs:///udf/udf-test.jar&amp;amp;apos;;
INFO  : converting to local hdfs:///udf/udf-test.jar
ERROR : Failed to read external resource hdfs:///udf/udf-test.jar
java.lang.RuntimeException: Failed to read external resource hdfs:///udf/udf-test.jar
	at org.apache.hadoop.hive.ql.session.SessionState.downloadResource(SessionState.java:1384)
	at org.apache.hadoop.hive.ql.session.SessionState.resolveAndDownload(SessionState.java:1340)
	at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1264)
	at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1250)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.addFunctionResources(FunctionTask.java:306)
	at org.apache.hadoop.hive.ql.exec.Registry.registerToSessionRegistry(Registry.java:466)
	at org.apache.hadoop.hive.ql.exec.Registry.registerPermanentFunction(Registry.java:206)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerPermanentFunction(FunctionRegistry.java:1551)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.createPermanentFunction(FunctionTask.java:136)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:75)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:158)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:101)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1965)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1723)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1475)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1283)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1278)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:167)
	at org.apache.hive.service.cli.operation.SQLOperation.access$200(SQLOperation.java:75)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:245)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:258)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: File does not exist: hdfs:/udf/udf-test.jar
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1391)
	at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1383)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1383)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:340)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:292)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2034)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2003)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1979)
	at org.apache.hadoop.hive.ql.session.SessionState.downloadResource(SessionState.java:1370)
	... 28 more
ERROR : Failed to register default.my_lower using class com.tang.UDFLower
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask (state=08S01,code=1)
2. Execute show functions, the failed function my_lower is in the function list

0: jdbc:hive2://189.39.151.44:21066/&amp;gt; show functions;
-------------------------+


        tab_name         


-------------------------+


 day                     


 dayofmonth              


 decode                  


 default.my_lower       


 degrees                 


 dense_rank              


0: jdbc:hive2://189.39.151.44:10000/&amp;gt; select my_lower(name) from stu;
Error: Error while compiling statement: FAILED: SemanticException [Error 10011]: Invalid function my_lower (state=42000,code=10011)

</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-06 14:51:19" id="14581" opendate="2016-08-19 01:13:42" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Add chr udf</summary>
			
			
			<description>http://docs.aws.amazon.com/redshift/latest/dg/r_CHR.html</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13063</link>
			
			
			<link description="is duplicated by" type="Duplicate">13063</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-10-05 17:44:18" id="14773" opendate="2016-09-16 12:51:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NPE aggregating column statistics for date column in partitioned table</summary>
			
			
			<description>Hive runs into a NPE when the query has a filter on a date column and the partitioned column 
eg: 



create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc;

set hive.exec.dynamic.partition.mode=nonstrict;

insert into date_dim partition(d_date_sk=2416945) values(&amp;amp;apos;1905-04-09&amp;amp;apos;);

insert into date_dim partition(d_date_sk=2416946) values(&amp;amp;apos;1905-04-10&amp;amp;apos;);

insert into date_dim partition(d_date_sk=2416947) values(&amp;amp;apos;1905-04-11&amp;amp;apos;);

analyze table date_dim partition(d_date_sk) compute statistics for columns;



explain select count(*) from date_dim where d_date &amp;gt; date &quot;1900-01-02&quot; and d_date_sk= 2416945;



Here d_date_sk is a partition column and d_date is of type date.



2016-09-16T08:27:06,510 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.AggregateStatsCache: No aggregate stats cached for database:default, table:date_dim, column:d_date

2016-09-16T08:27:06,512 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: Direct SQL query in 1.302231ms + 0.00653ms, the query is [select &quot;COLUMN_NAME&quot;, &quot;COLUMN_TYPE&quot;, min(&quot;LONG_LOW_VALUE&quot;), max(&quot;LONG_HIGH_VALUE&quot;), min(&quot;DOUBLE_LOW_VALUE&quot;), max(&quot;DOUBLE_HIGH_VALUE&quot;), min(cast(&quot;BIG_DECIMAL_LOW_VALUE&quot; as decimal)), max(cast(&quot;BIG_DECIMAL_HIGH_VALUE&quot; as decimal)), sum(&quot;NUM_NULLS&quot;), max(&quot;NUM_DISTINCTS&quot;), max(&quot;AVG_COL_LEN&quot;), max(&quot;MAX_COL_LEN&quot;), sum(&quot;NUM_TRUES&quot;), sum(&quot;NUM_FALSES&quot;), avg((&quot;LONG_HIGH_VALUE&quot;-&quot;LONG_LOW_VALUE&quot;)/cast(&quot;NUM_DISTINCTS&quot; as decimal)),avg((&quot;DOUBLE_HIGH_VALUE&quot;-&quot;DOUBLE_LOW_VALUE&quot;)/&quot;NUM_DISTINCTS&quot;),avg((cast(&quot;BIG_DECIMAL_HIGH_VALUE&quot; as decimal)-cast(&quot;BIG_DECIMAL_LOW_VALUE&quot; as decimal))/&quot;NUM_DISTINCTS&quot;),sum(&quot;NUM_DISTINCTS&quot;) from &quot;PART_COL_STATS&quot; where &quot;DB_NAME&quot; = ? and &quot;TABLE_NAME&quot; = ?  and &quot;COLUMN_NAME&quot; in (?) and &quot;PARTITION_NAME&quot; in (?) group by &quot;COLUMN_NAME&quot;, &quot;COLUMN_TYPE&quot;]

2016-09-16T08:27:06,526  INFO [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: useDensityFunctionForNDVEstimation = false

partsFound = 1

ColumnStatisticsObj = [ColumnStatisticsObj(colName:d_date, colType:date, statsData:&amp;lt;ColumnStatisticsData &amp;gt;)]

2016-09-16T08:27:06,526 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: Commit transaction: count = 0, isactive true at:

        org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2827)

2016-09-16T08:27:06,531 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: null retrieved using SQL in 43.425925ms

2016-09-16T08:27:06,545 ERROR [90d4780f-77e4-4704-9907-4860ce11a206 main] ql.Driver: FAILED: NullPointerException null

java.lang.NullPointerException

        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:451)

        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDateStats(ColumnStatisticsData.java:574)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:304)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)

        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:126)

        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)

        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)

        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)

        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)

        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)

        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)

        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)

        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)

        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10928)

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:255)

        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:251)

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:467)

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)

        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1235)

        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1355)

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143)

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131)

        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)

        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)

        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)

        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777)

        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)

        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:498)

        at org.apache.hadoop.util.RunJar.run(RunJar.java:233)

        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)


</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">10226</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-10-28 18:02:22" id="14883" opendate="2016-10-03 21:59:14" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Checks for Acid operation/bucket table write are in the wrong place</summary>
			
			
			<description>The following code in 
 in SemanticAnalyzer.getMetaData(QB qb, ReadEntity parentInput) 

      // Disallow INSERT INTO on bucketized tables

      boolean isAcid = AcidUtils.isAcidTable(tab);

      boolean isTableWrittenTo = qb.getParseInfo().isInsertIntoTable(tab.getDbName(), tab.getTableName());

      if (isTableWrittenTo &amp;amp;&amp;amp;

          tab.getNumBuckets() &amp;gt; 0 &amp;amp;&amp;amp; !isAcid) {

        throw new SemanticException(ErrorMsg.INSERT_INTO_BUCKETIZED_TABLE.

            getMsg(&quot;Table: &quot; + tabName));

      }

      // Disallow update and delete on non-acid tables

      if ((updating() || deleting()) &amp;amp;&amp;amp; !isAcid &amp;amp;&amp;amp; isTableWrittenTo) {

        //isTableWrittenTo: delete from acidTbl where a in (select id from nonAcidTable)

        //so only assert this if we are actually writing to this table

        // Whether we are using an acid compliant transaction manager has already been caught in

        // UpdateDeleteSemanticAnalyzer, so if we are updating or deleting and getting nonAcid

        // here, it means the table itself doesn&amp;amp;apos;t support it.

        throw new SemanticException(ErrorMsg.ACID_OP_ON_NONACID_TABLE, tabName);

      }



is done in the loop &quot;    for (String alias : tabAliases) {&quot; which is over tables being read.
Should be done in &quot;    for (String name : qbp.getClauseNamesForDest()) {&quot; loop</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.LockComponentBuilder.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">10924</link>
			
			
			<link description="duplicates" type="Duplicate">14943</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-11-08 13:34:03" id="14984" opendate="2016-10-17 08:48:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive-WebUI access results in Request is a replay (34) attack</summary>
			
			
			<description>When trying to access kerberized webui of HS2, The following error is received
GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))
While this is not happening for RM webui (checked if kerberos webui is enabled)
To reproduce the issue 
Try running
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&amp;lt;hostname&amp;gt;:10002/
from any cluster nodes
or 
Try accessing the URL from a VM with windows machine and firefox browser to replicate the issue
The following workaround helped, but need a permanent solution for the bug
Workaround:
=========
First access the index.html directly and then actual URL of webui
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&amp;lt;hostname&amp;gt;:10002/index.html
curl --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://&amp;lt;hostname&amp;gt;:10002
In browser:
First access
http://&amp;lt;hostname&amp;gt;:10002/index.html
then
http://&amp;lt;hostname&amp;gt;:10002</description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.http.HttpServer.java</file>
			
			
			<file type="M">org.apache.hive.service.server.TestHS2HttpServer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="breaks" type="Regression">15196</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-11-12 20:26:41" id="14943" opendate="2016-10-13 17:43:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Base Implementation</summary>
			
			
			<description>Create the 1st pass functional implementation of MERGE
This should run e2e and produce correct results.  </description>
			
			
			<version>1.2.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.LockComponentBuilder.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">14993</link>
			
			
			<link description="is blocked by" type="Blocker">15010</link>
			
			
			<link description="is duplicated by" type="Duplicate">14948</link>
			
			
			<link description="is duplicated by" type="Duplicate">14944</link>
			
			
			<link description="is duplicated by" type="Duplicate">14945</link>
			
			
			<link description="is duplicated by" type="Duplicate">14952</link>
			
			
			<link description="is duplicated by" type="Duplicate">15011</link>
			
			
			<link description="is duplicated by" type="Duplicate">14883</link>
			
			
			<link description="is depended upon by" type="dependent">13795</link>
			
		
		</links>
		
	
	</bug>
</bugrepository>
