<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="HIVE">
	<bug fixdate="2014-05-01 01:20:11" id="6741" opendate="2014-03-25 04:56:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveServer2 startup fails in secure (kerberos) mode due to backward incompatible hadoop change</summary>
			
			
			<description> HADOOP-10211 made a backward incompatible change due to which the following hive call returns a null map (HiveAuthFactory-old):



Map&amp;lt;String, String&amp;gt; hadoopSaslProps =  ShimLoader.getHadoopThriftAuthBridge().

        getHadoopSaslProperties(conf); 

SaslQOP hadoopSaslQOP = SaslQOP.fromString(hadoopSaslProps.get(Sasl.QOP));

if(hadoopSaslQOP.ordinal() &amp;gt; saslQOP.ordinal()) {

LOG.warn(MessageFormat.format(&quot;\&quot;hadoop.rpc.protection\&quot; is set to higher security level &quot; +

          &quot;{0} then {1} which is set to {2}&quot;, hadoopSaslQOP.toString(),

          ConfVars.HIVE_SERVER2_THRIFT_SASL_QOP.varname, saslQOP.toString()));

}



Since this code path is only used for logging hadoop sasl qop values in case hadoop&amp;amp;apos;s qop &amp;gt; hive&amp;amp;apos;s qop, we can do away with this and add a general log message.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.auth.HiveAuthFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">6960</link>
			
			
			<link description="is duplicated by" type="Duplicate">8154</link>
			
			
			<link description="relates to" type="Reference">6987</link>
			
			
			<link description="is related to" type="Reference">7620</link>
			
			
			<link description="is related to" type="Reference">10451</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-06-13 00:51:55" id="6824" opendate="2014-04-03 01:13:59" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hive HBase query fails on Tez due to missing jars - part 2</summary>
			
			
			<description>Follow-up from HIVE-6739. We cannot wait for Tez 0.4 (or even be sure that it will have TEZ-1004 and TEZ-1005), so I will split the patch into two. Original jira will have the straightforward (but less efficient) fix. This jira will use new relocalize APIs. Depending on relative timing of Tez 0.4 release and Hive 0.13 release, this will go into 0.13 or 0.14 blocked on Tez 0.5</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">6739</link>
			
			
			<link description="is duplicated by" type="Duplicate">7212</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-06-16 18:30:39" id="7212" opendate="2014-06-11 04:06:19" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Use resource re-localization instead of restarting sessions in Tez</summary>
			
			
			<description>scriptfile1.q is failing on Tez because of a recent breakage in localization. On top of that we&amp;amp;apos;re currently restarting sessions if the resources have changed. (add file/add jar/etc). Instead of doing this we should just have tez relocalize these new resources. This way no session/AM restart is required.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TestTezTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">6824</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-08-10 04:31:10" id="7539" opendate="2014-07-29 02:03:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>streaming windowing UDAF seems to be broken without Partition Spec</summary>
			
			
			<description>


select  avg(c_int) over(rows between 1 PRECEDING and current row) from t1



results in 

:1}}

	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271)

	... 9 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage$GenericUDAFAverageEvaluatorDouble$1.getNextResult(GenericUDAFAverage.java:180)

	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage$GenericUDAFAverageEvaluatorDouble$1.getNextResult(GenericUDAFAverage.java:166)

	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator$SumAvgEnhancer.iterate(GenericUDAFStreamingEvaluator.java:166)

	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:185)

	at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.processRow(WindowingTableFunction.java:348)

	at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.processRow(PTFOperator.java:318)

	at org.apache.hadoop.hive.ql.exec.PTFOperator.processOp(PTFOperator.java:131)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800)

	at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)

	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:262)

	... 9 more


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">7306</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-08-22 08:59:37" id="6987" opendate="2014-04-29 19:46:41" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Metastore qop settings won&amp;apos;t work with Hadoop-2.4</summary>
			
			
			<description> HADOOP-10211 made a backward incompatible change due to which the following hive call returns a null map:



Map&amp;lt;String, String&amp;gt; hadoopSaslProps =  ShimLoader.getHadoopThriftAuthBridge().

        getHadoopSaslProperties(conf); 



Metastore uses the underlying hadoop.rpc.protection values to set the qop between metastore client/server. </description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.ShimLoader.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">7620</link>
			
			
			<link description="is related to" type="Reference">6741</link>
			
			
			<link description="is broken by" type="Regression">10211</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-08-26 18:46:34" id="7764" opendate="2014-08-18 10:00:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Support all JDBC-HiveServer2 authentication modes on a secure cluster</summary>
			
			
			<description>Currently, HiveServer2 logs in with its keytab only if hive.server2.authentication is set to KERBEROS. However, hive.server2.authentication is config that determines the auth type an end user will use while authenticating with HiveServer2. There is a valid use case of user authenticating with HiveServer2 using LDAP for example, while HiveServer2 runs the query on a kerberized cluster.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">5447</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-02 16:15:22" id="7937" opendate="2014-09-02 15:32:34" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Compilation error on hadoop-1 profile</summary>
			
			
			<description>Currently while building with the hadoop-1 profile, it seems like we have a compilation error.

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-common: Compilation failure

[ERROR] /Users/sk018283/git-repo/hive/common/src/java/org/apache/hadoop/hive/common/FileUtils.java:[664,35] cannot find symbol

[ERROR] symbol  : method getStickyBit()

[ERROR] location: class org.apache.hadoop.fs.permission.FsPermission

[ERROR] -&amp;gt; [Help 1]

[ERROR] 

[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.

[ERROR] Re-run Maven using the -X switch to enable full debug logging.



Build works fine with hadoop-2.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">7927</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-03 20:52:47" id="7944" opendate="2014-09-02 22:30:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>current update stats for columns of a partition of a table is not correct</summary>
			
			
			<description>We worked hard towards faster update stats for columns of a partition of a table previously 
https://issues.apache.org/jira/browse/HIVE-7736
and
https://issues.apache.org/jira/browse/HIVE-7876
Although there is some improvement, it is only correct in the first run. There will be duplicate column stats later. Thanks to Eugene Koifman &amp;amp;apos;s comments</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RawStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">7811</link>
			
			
			<link description="is duplicated by" type="Duplicate">7982</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-04 20:27:14" id="6847" opendate="2014-04-04 23:56:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Improve / fix bugs in Hive scratch dir setup</summary>
			
			
			<description>Currently, the hive server creates scratch directory and changes permission to 777 however, this is not great with respect to security. We need to create user specific scratch directories instead. Also refer to HIVE-6782 1st iteration of the patch for approach.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="D">org.apache.hadoop.hive.ql.TestUtilitiesDfs.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionBase.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.ServerUtils.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="D">org.apache.hive.service.cli.TestScratchDir.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">8606</link>
			
			
			<link description="relates to" type="Reference">4487</link>
			
			
			<link description="relates to" type="Reference">6602</link>
			
			
			<link description="relates to" type="Reference">8143</link>
			
			
			<link description="is related to" type="Reference">8643</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-04 22:05:41" id="7982" opendate="2014-09-04 14:01:22" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Regression in explain with CBO enabled due to issuing query per K,V for the stats</summary>
			
			
			<description>
Now explain for Q17 is back in the 12 second range, I checked the queries issues to MySQL and they are very different than before 
on August 15 explain was completing in under 5 seconds and we issued the following queries : 



select &quot;COLUMN_NAME&quot;, &quot;COLUMN_TYPE&quot;, min(&quot;LONG_LOW_VALUE&quot;), max(&quot;LONG_HIGH_VALUE&quot;), min(&quot;DOUBLE_LOW_VALUE&quot;), max(&quot;DOUBLE_HIGH_VALUE&quot;), min(&quot;BIG_DECIMAL_LOW_VALUE&quot;), max(&quot;BIG_DECIMAL_HIGH_VALUE&quot;), sum(&quot;NUM_NULLS&quot;), max(&quot;NUM_DISTINCTS&quot;), max(&quot;AVG_COL_LEN&quot;), max(&quot;MAX_COL_LEN&quot;), sum(&quot;NUM_TRUES&quot;), sum(&quot;NUM_FALSES&quot;) from &quot;PART_COL_STATS&quot; where &quot;DB_NAME&quot; = &amp;amp;apos;tpcds_bin_partitioned_orc_30000&amp;amp;apos; and &quot;TABLE_NAME&quot; = &amp;amp;apos;store_returns&amp;amp;apos; and &quot;COLUMN_NAME&quot; in (&amp;amp;apos;sr_item_sk&amp;amp;apos;,&amp;amp;apos;sr_customer_sk&amp;amp;apos;,&amp;amp;apos;sr_ticket_number&amp;amp;apos;) AND &quot;PARTITION_NAME&quot; in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=1998-01-07&amp;amp;apos;,..&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by &quot;COLUMN_NAME&quot;, &quot;COLUMN_TYPE&quot;;



select &quot;COLUMN_NAME&quot;, &quot;COLUMN_TYPE&quot;, min(&quot;LONG_LOW_VALUE&quot;), max(&quot;LONG_HIGH_VALUE&quot;), min(&quot;DOUBLE_LOW_VALUE&quot;), max(&quot;DOUBLE_HIGH_VALUE&quot;), min(&quot;BIG_DECIMAL_LOW_VALUE&quot;), max(&quot;BIG_DECIMAL_HIGH_VALUE&quot;), sum(&quot;NUM_NULLS&quot;), max(&quot;NUM_DISTINCTS&quot;), max(&quot;AVG_COL_LEN&quot;), max(&quot;MAX_COL_LEN&quot;), sum(&quot;NUM_TRUES&quot;), sum(&quot;NUM_FALSES&quot;) from &quot;PART_COL_STATS&quot; where &quot;DB_NAME&quot; = &amp;amp;apos;tpcds_bin_partitioned_orc_30000&amp;amp;apos; and &quot;TABLE_NAME&quot; = &amp;amp;apos;store_returns&amp;amp;apos; and &quot;COLUMN_NAME&quot; in (&amp;amp;apos;sr_returned_date_sk&amp;amp;apos;) AND &quot;PARTITION_NAME&quot; in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;..&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by &quot;COLUMN_NAME&quot;, &quot;COLUMN_TYPE&quot;



Currently explain Q17 takes 11 seconds and the queries sent to MySQL are very inefficient because 
1) They no longer do the aggregation on MySQL and get a row per partition 
2) There is a query per stats K,V pair so the number of queries is up by 9x



		select COLUMN_NAME, COLUMN_TYPE, count(PARTITION_NAME)  from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos;  and COLUMN_NAME in (&amp;amp;apos;sr_item_sk&amp;amp;apos;,&amp;amp;apos;sr_customer_sk&amp;amp;apos;,&amp;amp;apos;sr_ticket_number&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by COLUMN_NAME, COLUMN_TYPE

		select COLUMN_NAME, sum(NUM_NULLS), sum(NUM_TRUES), sum(NUM_FALSES) from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos;  and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;,&amp;amp;apos;sr_item_sk&amp;amp;apos;,&amp;amp;apos;sr_ticket_number&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) group by COLUMN_NAME

		select LONG_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_LOW_VALUE&amp;amp;apos;

		select LONG_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_HIGH_VALUE&amp;amp;apos;

		select DOUBLE_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;DOUBLE_LOW_VALUE&amp;amp;apos;

		select DOUBLE_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;DOUBLE_HIGH_VALUE&amp;amp;apos;

		select BIG_DECIMAL_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;BIG_DECIMAL_LOW_VALUE&amp;amp;apos;

		select BIG_DECIMAL_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;BIG_DECIMAL_HIGH_VALUE&amp;amp;apos;

		select NUM_DISTINCTS,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;NUM_DISTINCTS&amp;amp;apos;

		select AVG_COL_LEN,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;AVG_COL_LEN&amp;amp;apos;

		select MAX_COL_LEN,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_customer_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;MAX_COL_LEN&amp;amp;apos;

		select LONG_LOW_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_item_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_LOW_VALUE&amp;amp;apos;

		select LONG_HIGH_VALUE,PARTITION_NAME from PART_COL_STATS where DB_NAME = &amp;amp;apos;tpcds_bin_partitioned_orc_200&amp;amp;apos; and TABLE_NAME = &amp;amp;apos;store_returns&amp;amp;apos; and COLUMN_NAME in (&amp;amp;apos;sr_item_sk&amp;amp;apos;) and PARTITION_NAME in (&amp;amp;apos;sr_returned_date=1998-01-06&amp;amp;apos;,&amp;amp;apos;sr_returned_date=2003-07-01&amp;amp;apos;) order by &amp;amp;apos;LONG_HIGH_VALUE&amp;amp;apos;

 
</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RawStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">7944</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-05 21:47:08" id="7927" opendate="2014-09-01 06:04:53" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Checking sticky bit needs shim</summary>
			
			
			<description>Hive cannot be built on hadoop-1 after HIVE-7895, which checks sticky bit in FsPermission.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.shims.HadoopShims.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">7937</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-16 19:41:34" id="7935" opendate="2014-09-02 07:20:49" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Support dynamic service discovery for HiveServer2</summary>
			
			
			<description>To support Rolling Upgrade / HA, we need a mechanism by which a JDBC client can dynamically resolve an HiveServer2 to connect to.
High Level Design: 
Whether, dynamic service discovery is supported or not, can be configured by setting HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY. ZooKeeper is used to support this.

When an instance of HiveServer2 comes up, it adds itself as a znode to ZooKeeper under a configurable namespace (HIVE_SERVER2_ZOOKEEPER_NAMESPACE).
A JDBC/ODBC client now specifies the ZooKeeper ensemble in its connection string, instead of pointing to a specific HiveServer2 instance. The JDBC driver, uses the ZooKeeper ensemble to pick an instance of HiveServer2 to connect for the entire session.
When an instance is removed from ZooKeeper, the existing client sessions continue till completion. When the last client session completes, the instance shuts down.
All new client connection pick one of the available HiveServer2 uris from ZooKeeper.

</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
			
			
			<file type="M">org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.operation.OperationManager.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
			
			
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.CLIService.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.HiveDriver.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">6363</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-26 09:50:35" id="8264" opendate="2014-09-26 00:23:16" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Math UDFs in Reducer-with-vectorization fail with ArrayIndexOutOfBoundsException</summary>
			
			
			<description>Following queries are representative of the exceptions we are seeing with trunk. These queries pass if vectorization is disabled (or if limit is removed, which means no reducer).
select name, log2(0) from (select name from mytable limit 1) t;
select name, rand() from (select name from mytable limit 1) t;
.. similar patterns with other Math UDFs&amp;amp;apos;.
Exception:
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:177)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:180)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:172)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:167)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:254)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:167)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:154)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:360)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:242)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating null
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:127)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:801)
	at org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.processOp(VectorLimitOperator.java:47)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:801)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:139)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:347)
	... 17 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateLong(ConstantVectorExpression.java:102)
	at org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(ConstantVectorExpression.java:150)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(VectorSelectOperator.java:125)
	... 22 more</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">8171</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-09-26 22:31:21" id="8171" opendate="2014-09-18 00:35:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Tez and Vectorized Reduce doesn&amp;apos;t create scratch columns</summary>
			
			
			<description>This query fails with ArrayIndexOutofBound exception in the reducer.



create table varchar_3 (

  field varchar(25)

) stored as orc;



insert into table varchar_3 select cint from alltypesorc limit 10;


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">8264</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-01 21:20:16" id="8290" opendate="2014-09-29 17:02:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>With DbTxnManager configured, all ORC tables forced to be transactional</summary>
			
			
			<description>Currently, once a user configures DbTxnManager to the be transaction manager, all tables that use ORC are expected to be transactional.  This means they all have to have buckets.  This most likely won&amp;amp;apos;t be what users want.
We need to add a specific mark to a table so that users can indicate it should be treated in a transactional way.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">8231</link>
			
			
			<link description="is related to" type="Reference">8323</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-02 08:26:47" id="8231" opendate="2014-09-23 10:08:57" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Error when insert into empty table with ACID</summary>
			
			
			<description>Steps to show the bug :
1. create table 



create table encaissement_1b_64m like encaissement_1b;



2. check table 



desc encaissement_1b_64m;

dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m;



everything is ok:

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; desc encaissement_1b_64m;                                                                                                              +------------+------------+----------+--+

|  col_name  | data_type  | comment  |

+------------+------------+----------+--+

| id         | int        |          |

| idmagasin  | int        |          |

| zibzin     | string     |          |

| cheque     | int        |          |

| montant    | double     |          |

| date       | timestamp  |          |

| col_6      | string     |          |

| col_7      | string     |          |

| col_8      | string     |          |

+------------+------------+----------+--+

9 rows selected (0.158 seconds)

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;

+-------------+--+

| DFS Output  |

+-------------+--+

+-------------+--+

No rows selected (0.01 seconds)





3. Insert values into the new table

insert into table encaissement_1b_64m VALUES (1, 1, &amp;amp;apos;800000000909000000000000&amp;amp;apos;, 1, 12.5, &amp;amp;apos;12/05/2014&amp;amp;apos;, &amp;amp;apos;&amp;amp;apos;,&amp;amp;apos;&amp;amp;apos;,&amp;amp;apos;&amp;amp;apos;);



4. Check

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; select id from encaissement_1b_64m;

+-----+--+

| id  |

+-----+--+

+-----+--+

No rows selected (0.091 seconds)



There are already a pb. I don&amp;amp;apos;t see the inserted row.
5. When I&amp;amp;apos;m checking HDFS directory, I see delta_0000421_0000421 folder

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;

+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+

|                                                                     DFS Output                                                                      |

+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+

| Found 1 items                                                                                                                                       |

| drwxr-xr-x   - hduser supergroup          0 2014-09-23 12:17 hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/delta_0000421_0000421  |

+-----------------------------------------------------------------------------------------------------------------------------------------------------+--+

2 rows selected (0.014 seconds)



6. Doing a major compaction solves the bug

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; alter table encaissement_1b_64m compact &amp;amp;apos;major&amp;amp;apos;;

No rows affected (0.046 seconds)

0: jdbc:hive2://nc-h04:10000/casino&amp;gt; dfs -ls hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/;

+--------------------------------------------------------------------------------------------------------------------------------------------+--+

|                                                                 DFS Output                                                                 |

+--------------------------------------------------------------------------------------------------------------------------------------------+--+

| Found 1 items                                                                                                                              |

| drwxr-xr-x   - hduser supergroup          0 2014-09-23 12:21 hdfs://nc-h04/user/hive/warehouse/casino.db/encaissement_1b_64m/base_0000421  |

+--------------------------------------------------------------------------------------------------------------------------------------------+--+

2 rows selected (0.02 seconds)



</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Table.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">8290</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-08 05:15:16" id="8316" opendate="2014-09-30 21:56:32" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>CBO : cardinality estimation for filters is much lower than actual row count</summary>
			
			
			<description>CBO underestimates selectivity from filter which consequently results in under estimation throughout the plan.



8 rows, 0.0 cpu, 0.0 io}, id = 7808

                                            HiveJoinRel(condition=[=($0, $12)], joinType=[inner]): rowcount = 11459.928208333333, cumulative cost = {5.50076555E8 rows, 0.0 cpu, 0.0 io}, id = 7426

                                              HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_cdemo_sk=[$3], ss_hdemo_sk=[$4], ss_addr_sk=[$5], ss_store_sk=[$6], ss_promo_sk=[$7], ss_ticket_number=[$8], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$18], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 893

                                                HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 55

                                              HiveProjectRel(i_item_sk=[$0], i_current_price=[$5], i_color=[$17], i_product_name=[$21]): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1163

                                                HiveFilterRel(condition=[AND(in($17, &amp;amp;apos;maroon&amp;amp;apos;, &amp;amp;apos;burnished&amp;amp;apos;, &amp;amp;apos;dim&amp;amp;apos;, &amp;amp;apos;steel&amp;amp;apos;, &amp;amp;apos;navajo&amp;amp;apos;, &amp;amp;apos;chocolate&amp;amp;apos;), between(false, $5, 35, +(35, 10)), between(false, $5, +(35, 1), +(35, 15)))]): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1161

                                                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.item]]): rowcount = 48000.0, cumulative cost = {0}, id = 68






select count(*) from item where  i_color in (&amp;amp;apos;maroon&amp;amp;apos;,&amp;amp;apos;burnished&amp;amp;apos;,&amp;amp;apos;dim&amp;amp;apos;,&amp;amp;apos;steel&amp;amp;apos;,&amp;amp;apos;navajo&amp;amp;apos;,&amp;amp;apos;chocolate&amp;amp;apos;) and

         i_current_price between 35 and 35 + 10 and

         i_current_price between 35 + 1 and 35 + 15;


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">8315</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-08 15:29:00" id="8315" opendate="2014-09-30 21:47:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO : Negate condition underestimates selectivity which results in an in-efficient plan</summary>
			
			
			<description>For TPC-DS Q64 the predicate cd1.cd_marital_status &amp;lt;&amp;gt; cd2.cd_marital_status under estimate the join selectivity by a huge margin and results in in-efficient join order.
This is a subset of the logical plan showing that item was joined very last



                                HiveJoinRel(condition=[=($0, $37)], joinType=[inner]): rowcount = 1.0, cumulative cost = {6.386017602518958E8 rows, 0.0 cpu, 0.0 io}, id = 3790

                                  HiveJoinRel(condition=[=($0, $33)], joinType=[inner]): rowcount = 1.0, cumulative cost = {6.386017582518958E8 rows, 0.0 cpu, 0.0 io}, id = 3067

                                    HiveFilterRel(condition=[&amp;lt;&amp;gt;($30, $32)]): rowcount = 1.8252236387887635, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 1153

                                      HiveProjectRel(ss_item_sk=[$2], ss_customer_sk=[$3], ss_cdemo_sk=[$4], ss_hdemo_sk=[$5], ss_addr_sk=[$6], ss_store_sk=[$7], ss_promo_sk=[$8], ss_ticket_number=[$9], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$12], ss_sold_date_sk=[$13], sr_item_sk=[$0], sr_ticket_number=[$1], c_customer_sk=[$23], c_current_cdemo_sk=[$24], c_current_hdemo_sk=[$25], c_current_addr_sk=[$26], c_first_shipto_date_sk=[$27], c_first_sales_date_sk=[$28], d_date_sk=[$14], d_year=[$15], d_date_sk0=[$29], d_year0=[$30], d_date_sk1=[$31], d_year1=[$32], s_store_sk=[$18], s_store_name=[$19], s_zip=[$20], cd_demo_sk=[$16], cd_marital_status=[$17], cd_demo_sk0=[$21], cd_marital_status0=[$22]): rowcount = 3.6246005783468924E7, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 2312

                                        HiveJoinRel(condition=[AND(=($2, $0), =($9, $1))], joinType=[inner]): rowcount = 3.6246005783468924E7, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 2310

                                          HiveProjectRel(sr_item_sk=[$1], sr_ticket_number=[$8]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 912

                                            HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 62

                                          HiveJoinRel(condition=[=($1, $21)], joinType=[inner]): rowcount = 1.2950939439433252E7, cumulative cost = {5.700728109872389E8 rows, 0.0 cpu, 0.0 io}, id = 2308

                                            HiveJoinRel(condition=[=($5, $16)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.629812800658973E8 rows, 0.0 cpu, 0.0 io}, id = 2301

                                              HiveJoinRel(condition=[=($2, $14)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.574895371445558E8 rows, 0.0 cpu, 0.0 io}, id = 2299

                                                HiveJoinRel(condition=[=($11, $12)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.500772062232143E8 rows, 0.0 cpu, 0.0 io}, id = 1898

                                                  HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_cdemo_sk=[$3], ss_hdemo_sk=[$4], ss_addr_sk=[$5], ss_store_sk=[$6], ss_promo_sk=[$7], ss_ticket_number=[$8], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$18], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 909

                                                    HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 55

Query 



select cs1.product_name ,cs1.store_name ,cs1.store_zip ,cs1.b_street_number ,cs1.b_streen_name ,cs1.b_city

     ,cs1.b_zip ,cs1.c_street_number ,cs1.c_street_name ,cs1.c_city ,cs1.c_zip ,cs1.syear ,cs1.cnt

     ,cs1.s1 ,cs1.s2 ,cs1.s3

     ,cs2.s1 ,cs2.s2 ,cs2.s3 ,cs2.syear ,cs2.cnt

from

(select i_product_name as product_name ,i_item_sk as item_sk ,s_store_name as store_name

     ,s_zip as store_zip ,ad1.ca_street_number as b_street_number ,ad1.ca_street_name as b_streen_name

     ,ad1.ca_city as b_city ,ad1.ca_zip as b_zip ,ad2.ca_street_number as c_street_number

     ,ad2.ca_street_name as c_street_name ,ad2.ca_city as c_city ,ad2.ca_zip as c_zip

     ,d1.d_year as syear ,d2.d_year as fsyear ,d3.d_year as s2year ,count(*) as cnt

     ,sum(ss_wholesale_cost) as s1 ,sum(ss_list_price) as s2 ,sum(ss_coupon_amt) as s3

  FROM   store_sales

        JOIN store_returns ON store_sales.ss_item_sk = store_returns.sr_item_sk and store_sales.ss_ticket_number = store_returns.sr_ticket_number

        JOIN customer ON store_sales.ss_customer_sk = customer.c_customer_sk

        JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk

        JOIN date_dim d2 ON customer.c_first_sales_date_sk = d2.d_date_sk 

        JOIN date_dim d3 ON customer.c_first_shipto_date_sk = d3.d_date_sk

        JOIN store ON store_sales.ss_store_sk = store.s_store_sk

        JOIN customer_demographics cd1 ON store_sales.ss_cdemo_sk= cd1.cd_demo_sk

        JOIN customer_demographics cd2 ON customer.c_current_cdemo_sk = cd2.cd_demo_sk

        JOIN promotion ON store_sales.ss_promo_sk = promotion.p_promo_sk

        JOIN household_demographics hd1 ON store_sales.ss_hdemo_sk = hd1.hd_demo_sk

        JOIN household_demographics hd2 ON customer.c_current_hdemo_sk = hd2.hd_demo_sk

        JOIN customer_address ad1 ON store_sales.ss_addr_sk = ad1.ca_address_sk

        JOIN customer_address ad2 ON customer.c_current_addr_sk = ad2.ca_address_sk

        JOIN income_band ib1 ON hd1.hd_income_band_sk = ib1.ib_income_band_sk

        JOIN income_band ib2 ON hd2.hd_income_band_sk = ib2.ib_income_band_sk

        JOIN item ON store_sales.ss_item_sk = item.i_item_sk

        JOIN

 (select cs_item_sk

        ,sum(cs_ext_list_price) as sale,sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit) as refund

  from catalog_sales JOIN catalog_returns

  ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk

    and catalog_sales.cs_order_number = catalog_returns.cr_order_number

  group by cs_item_sk

  having sum(cs_ext_list_price)&amp;gt;2*sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit)) cs_ui

ON store_sales.ss_item_sk = cs_ui.cs_item_sk

  WHERE  

         cd1.cd_marital_status &amp;lt;&amp;gt; cd2.cd_marital_status and

         i_color in (&amp;amp;apos;maroon&amp;amp;apos;,&amp;amp;apos;burnished&amp;amp;apos;,&amp;amp;apos;dim&amp;amp;apos;,&amp;amp;apos;steel&amp;amp;apos;,&amp;amp;apos;navajo&amp;amp;apos;,&amp;amp;apos;chocolate&amp;amp;apos;) and

         i_current_price between 35 and 35 + 10 and

         i_current_price between 35 + 1 and 35 + 15

group by i_product_name ,i_item_sk ,s_store_name ,s_zip ,ad1.ca_street_number

       ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,ad2.ca_street_number

       ,ad2.ca_street_name ,ad2.ca_city ,ad2.ca_zip ,d1.d_year ,d2.d_year ,d3.d_year

) cs1

JOIN

(select i_product_name as product_name ,i_item_sk as item_sk ,s_store_name as store_name

     ,s_zip as store_zip ,ad1.ca_street_number as b_street_number ,ad1.ca_street_name as b_streen_name

     ,ad1.ca_city as b_city ,ad1.ca_zip as b_zip ,ad2.ca_street_number as c_street_number

     ,ad2.ca_street_name as c_street_name ,ad2.ca_city as c_city ,ad2.ca_zip as c_zip

     ,d1.d_year as syear ,d2.d_year as fsyear ,d3.d_year as s2year ,count(*) as cnt

     ,sum(ss_wholesale_cost) as s1 ,sum(ss_list_price) as s2 ,sum(ss_coupon_amt) as s3

  FROM   store_sales

        JOIN store_returns ON store_sales.ss_item_sk = store_returns.sr_item_sk and store_sales.ss_ticket_number = store_returns.sr_ticket_number

        JOIN customer ON store_sales.ss_customer_sk = customer.c_customer_sk

        JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk

        JOIN date_dim d2 ON customer.c_first_sales_date_sk = d2.d_date_sk 

        JOIN date_dim d3 ON customer.c_first_shipto_date_sk = d3.d_date_sk

        JOIN store ON store_sales.ss_store_sk = store.s_store_sk

        JOIN customer_demographics cd1 ON store_sales.ss_cdemo_sk= cd1.cd_demo_sk

        JOIN customer_demographics cd2 ON customer.c_current_cdemo_sk = cd2.cd_demo_sk

        JOIN promotion ON store_sales.ss_promo_sk = promotion.p_promo_sk

        JOIN household_demographics hd1 ON store_sales.ss_hdemo_sk = hd1.hd_demo_sk

        JOIN household_demographics hd2 ON customer.c_current_hdemo_sk = hd2.hd_demo_sk

        JOIN customer_address ad1 ON store_sales.ss_addr_sk = ad1.ca_address_sk

        JOIN customer_address ad2 ON customer.c_current_addr_sk = ad2.ca_address_sk

        JOIN income_band ib1 ON hd1.hd_income_band_sk = ib1.ib_income_band_sk

        JOIN income_band ib2 ON hd2.hd_income_band_sk = ib2.ib_income_band_sk

        JOIN item ON store_sales.ss_item_sk = item.i_item_sk

        JOIN

 (select cs_item_sk

        ,sum(cs_ext_list_price) as sale,sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit) as refund

  from catalog_sales JOIN catalog_returns

  ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk

    and catalog_sales.cs_order_number = catalog_returns.cr_order_number

  group by cs_item_sk

  having sum(cs_ext_list_price)&amp;gt;2*sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit)) cs_ui

ON store_sales.ss_item_sk = cs_ui.cs_item_sk

  WHERE  

         cd1.cd_marital_status &amp;lt;&amp;gt; cd2.cd_marital_status and

         i_color in (&amp;amp;apos;maroon&amp;amp;apos;,&amp;amp;apos;burnished&amp;amp;apos;,&amp;amp;apos;dim&amp;amp;apos;,&amp;amp;apos;steel&amp;amp;apos;,&amp;amp;apos;navajo&amp;amp;apos;,&amp;amp;apos;chocolate&amp;amp;apos;) and

         i_current_price between 35 and 35 + 10 and

         i_current_price between 35 + 1 and 35 + 15

group by i_product_name ,i_item_sk ,s_store_name ,s_zip ,ad1.ca_street_number

       ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,ad2.ca_street_number

       ,ad2.ca_street_name ,ad2.ca_city ,ad2.ca_zip ,d1.d_year ,d2.d_year ,d3.d_year

) cs2

ON cs1.item_sk=cs2.item_sk

where 

     cs1.syear = 2000 and

     cs2.syear = 2000 + 1 and

     cs2.cnt &amp;lt;= cs1.cnt and

     cs1.store_name = cs2.store_name and

     cs1.store_zip = cs2.store_zip

order by cs1.product_name ,cs1.store_name ,cs2.cnt


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is a clone of" type="Cloners">8263</link>
			
			
			<link description="is duplicated by" type="Duplicate">8316</link>
			
			
			<link description="is related to" type="Reference">8283</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-11 00:51:35" id="8369" opendate="2014-10-06 21:37:24" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>SimpleFetchOptimizer needs to re-enable FS caching before scanning dirs</summary>
			
			
			<description>SimpleFetchOptimizer spends a lot of CPU within itself because hive disables HDFS fs caching (fs.hdfs.impl.disable.cache).
SimpleFetchOptimizer needs a revisit for its optimization rules, along with a fix for this case.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionProxy.java</file>
			
			
			<file type="D">org.apache.hive.service.auth.TUGIContainingProcessor.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSession.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">4501</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-28 04:22:00" id="8604" opendate="2014-10-25 07:42:09" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Re-enable auto_sortmerge_join_5 on tez</summary>
			
			
			<description/>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">8614</link>
			
			
			<link description="is related to" type="Reference">8603</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-29 04:10:33" id="8614" opendate="2014-10-27 18:38:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Upgrade hive to use tez version 0.5.2-SNAPSHOT</summary>
			
			
			<description/>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">8604</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-30 19:48:19" id="8653" opendate="2014-10-29 20:11:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO: Push Semi Join through, Project/Filter/Join</summary>
			
			
			<description>CLEAR LIBRARY CACHE</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">8526</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-10-31 19:21:45" id="8526" opendate="2014-10-20 22:56:24" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hive : CBO incorrect join order in TPC-DS Q45 as self join selectivity has incorrect CE</summary>
			
			
			<description>The join order has Item joined last where it should be joined first
Query 



select  ca_zip, ca_county, sum(ws_sales_price)

 from

    web_sales

    JOIN customer ON web_sales.ws_bill_customer_sk = customer.c_customer_sk

    JOIN customer_address ON customer.c_current_addr_sk = customer_address.ca_address_sk 

    JOIN date_dim ON web_sales.ws_sold_date_sk = date_dim.d_date_sk

    JOIN item ON web_sales.ws_item_sk = item.i_item_sk 

 where

        ( item.i_item_id in (select i_item_id

                             from item i2

                             where i2.i_item_sk in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)

                             )

            )

        and d_qoy = 2 and d_year = 2000

 group by ca_zip, ca_county

 order by ca_zip, ca_county

 limit 100



Plan



2014-10-20 18:43:16,521 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12330)) - HiveSortRel(fetch=[100]): rowcount = 1.710158597922807E7, cumulative cost = {7.169080587598123E10 rows, 3.420317295845614E7 cpu, 0.0 io}, id = 579

  HiveSortRel(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC]): rowcount = 1.710158597922807E7, cumulative cost = {6.827294821015483E10 rows, 1.710158697922807E7 cpu, 0.0 io}, id = 577

    HiveProjectRel(ca_zip=[$0], ca_county=[$1], _o__c2=[$2]): rowcount = 1.710158597922807E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 575

      HiveAggregateRel(group=[{0, 1}], agg#0=[sum($2)]): rowcount = 1.710158597922807E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 573

        HiveProjectRel($f0=[$2], $f1=[$1], $f2=[$0]): rowcount = 6.0197670310147226E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 571

          HiveProjectRel(ws_sales_price=[$2], ca_county=[$7], ca_zip=[$8]): rowcount = 6.0197670310147226E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 569

            HiveFilterRel(condition=[AND(=($11, 2), =($10, 2000))]): rowcount = 6.0197670310147226E7, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 567

              SemiJoinRel(condition=[=($13, $14)], joinType=[inner]): rowcount = 3.371069537368245E10, cumulative cost = {6.485509054432843E10 rows, 1.0 cpu, 0.0 io}, id = 565

                HiveProjectRel(ws_item_sk=[$0], ws_bill_customer_sk=[$1], ws_sales_price=[$2], ws_sold_date_sk=[$3], c_customer_sk=[$9], c_current_addr_sk=[$10], ca_address_sk=[$11], ca_county=[$12], ca_zip=[$13], d_date_sk=[$6], d_year=[$7], d_qoy=[$8], i_item_sk=[$4], i_item_id=[$5]): rowcount = 3.371069537368245E10, cumulative cost = {6.485509054332843E10 rows, 0.0 cpu, 0.0 io}, id = 669

                  HiveJoinRel(condition=[=($1, $9)], joinType=[inner]): rowcount = 3.371069537368245E10, cumulative cost = {6.485509054332843E10 rows, 0.0 cpu, 0.0 io}, id = 667

                    HiveJoinRel(condition=[=($3, $6)], joinType=[inner]): rowcount = 2.1594638446E10, cumulative cost = {4.3189811941E10 rows, 0.0 cpu, 0.0 io}, id = 664

                      HiveJoinRel(condition=[=($0, $4)], joinType=[inner]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100446E10 rows, 0.0 cpu, 0.0 io}, id = 601

                        HiveProjectRel(ws_item_sk=[$2], ws_bill_customer_sk=[$3], ws_sales_price=[$20], ws_sold_date_sk=[$33]): rowcount = 2.1594638446E10, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 497

                          HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.web_sales]]): rowcount = 2.1594638446E10, cumulative cost = {0}, id = 341

                        HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 555

                          HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 340

                      HiveProjectRel(d_date_sk=[$0], d_year=[$6], d_qoy=[$10]): rowcount = 73049.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 551

                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 342

                    HiveJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 7.064015632843196E7, cumulative cost = {1.2E8 rows, 0.0 cpu, 0.0 io}, id = 598

                      HiveProjectRel(c_customer_sk=[$0], c_current_addr_sk=[$4]): rowcount = 8.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 500

                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer]]): rowcount = 8.0E7, cumulative cost = {0}, id = 343

                      HiveProjectRel(ca_address_sk=[$0], ca_county=[$7], ca_zip=[$9]): rowcount = 4.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 547

                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer_address]]): rowcount = 4.0E7, cumulative cost = {0}, id = 339

                HiveProjectRel(i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 563

                  HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 561

                    HiveFilterRel(condition=[in($0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29)]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 559

                      HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 340



Then I rewrote the query trying to force CBO to generate the correct join order



with items as (select i_item_sk from 

item  where

        ( item.i_item_id in (select i_item_id

                             from item i2

                             where i2.i_item_sk in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)

                             )

            )

)



select  ca_zip, ca_county, sum(ws_sales_price)

 from

    web_sales

    JOIN items ON web_sales.ws_item_sk = items.i_item_sk 

    JOIN customer ON web_sales.ws_bill_customer_sk = customer.c_customer_sk

    JOIN customer_address ON customer.c_current_addr_sk = customer_address.ca_address_sk 

    JOIN date_dim ON web_sales.ws_sold_date_sk = date_dim.d_date_sk

 where

 d_qoy = 2 and d_year = 2000

 group by ca_zip, ca_county

 order by ca_zip, ca_county

 limit 100



But the correct join order wasn&amp;amp;apos;t generated because CE for item x item + filter has a selectivity of 1.



2014-10-20 18:46:27,120 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12330)) - HiveSortRel(fetch=[100]): rowcount = 1.6595391288544238E7, cumulative cost = {2.8364280421639153E10 rows, 3.3190782577088475E7 cpu, 0.0 io}, id = 1291

  HiveSortRel(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC]): rowcount = 1.6595391288544238E7, cumulative cost = {2.505357243157397E10 rows, 1.6595391288544238E7 cpu, 0.0 io}, id = 1289

    HiveProjectRel(ca_zip=[$0], ca_county=[$1], _o__c2=[$2]): rowcount = 1.6595391288544238E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1287

      HiveAggregateRel(group=[{0, 1}], agg#0=[sum($2)]): rowcount = 1.6595391288544238E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1285

        HiveProjectRel($f0=[$9], $f1=[$8], $f2=[$2]): rowcount = 6.019767031014723E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1283

          HiveProjectRel(ws_item_sk=[$5], ws_bill_customer_sk=[$6], ws_sales_price=[$7], ws_sold_date_sk=[$8], i_item_sk=[$12], c_customer_sk=[$0], c_current_addr_sk=[$1], ca_address_sk=[$2], ca_county=[$3], ca_zip=[$4], d_date_sk=[$9], d_year=[$10], d_qoy=[$11]): rowcount = 6.019767031014723E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1380

            HiveJoinRel(condition=[=($6, $0)], joinType=[inner]): rowcount = 6.019767031014723E7, cumulative cost = {2.174286444150879E10 rows, 0.0 cpu, 0.0 io}, id = 1378

              HiveJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 7.064015632843196E7, cumulative cost = {1.2E8 rows, 0.0 cpu, 0.0 io}, id = 1309

                HiveProjectRel(c_customer_sk=[$0], c_current_addr_sk=[$4]): rowcount = 8.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1269

                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer]]): rowcount = 8.0E7, cumulative cost = {0}, id = 1035

                HiveProjectRel(ca_address_sk=[$0], ca_county=[$7], ca_zip=[$9]): rowcount = 4.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1273

                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer_address]]): rowcount = 4.0E7, cumulative cost = {0}, id = 1032

              HiveJoinRel(condition=[=($0, $7)], joinType=[inner]): rowcount = 3.856185436785714E7, cumulative cost = {2.16336624308125E10 rows, 0.0 cpu, 0.0 io}, id = 1376

                HiveJoinRel(condition=[=($3, $4)], joinType=[inner]): rowcount = 3.856185436785714E7, cumulative cost = {2.159463857644464E10 rows, 0.0 cpu, 0.0 io}, id = 1316

                  HiveProjectRel(ws_item_sk=[$2], ws_bill_customer_sk=[$3], ws_sales_price=[$20], ws_sold_date_sk=[$33]): rowcount = 2.1594638446E10, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1205

                    HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.web_sales]]): rowcount = 2.1594638446E10, cumulative cost = {0}, id = 1033

                  HiveProjectRel(d_date_sk=[$0], d_year=[$6], d_qoy=[$10]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1279

                    HiveFilterRel(condition=[AND(=($10, 2), =($6, 2000))]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1277

                      HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 1034

                HiveProjectRel(i_item_sk=[$0]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 1265

                  HiveFilterRel(condition=[=(1, 1)]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 1263

                    SemiJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 1261

                      HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1253

                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 1024

                      HiveProjectRel(i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1259

                        HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1257

                          HiveFilterRel(condition=[in($0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29)]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 1255

                            HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 1024



This query generates the correct join order 



 with items as (select i_item_sk from 

item  where

         item.i_item_id in (select i_item_id

                             from item i2

                             where i2.i_item_sk in (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)

                             )

            

),

  ws as (

 select ws_bill_customer_sk,ws_sales_price,ws_sold_date_sk

from  web_sales

    JOIN items ON web_sales.ws_item_sk = items.i_item_sk 

 )

 select  ca_zip, ca_county, sum(ws_sales_price)

 from ws 

    JOIN customer ON ws.ws_bill_customer_sk = customer.c_customer_sk

    JOIN customer_address ON customer.c_current_addr_sk = customer_address.ca_address_sk 

    JOIN date_dim ON ws.ws_sold_date_sk = date_dim.d_date_sk

 where d_qoy = 2 and d_year = 2000

 group by ca_zip, ca_county

 order by ca_zip, ca_county

 limit 100



Plan 



2014-10-20 19:13:15,989 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12330)) - HiveSortRel(fetch=[100]): rowcount = 1.6595391288544238E7, cumulative cost = {4.99203570142713E10 rows, 3.3190783577088475E7 cpu, 0.0 io}, id = 4367

  HiveSortRel(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC]): rowcount = 1.6595391288544238E7, cumulative cost = {4.6609649024206116E10 rows, 1.6595392288544238E7 cpu, 0.0 io}, id = 4365

    HiveProjectRel(ca_zip=[$0], ca_county=[$1], _o__c2=[$2]): rowcount = 1.6595391288544238E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4363

      HiveAggregateRel(group=[{0, 1}], agg#0=[sum($2)]): rowcount = 1.6595391288544238E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4361

        HiveProjectRel($f0=[$7], $f1=[$6], $f2=[$1]): rowcount = 6.019767031014723E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4359

          HiveProjectRel(ws_bill_customer_sk=[$5], ws_sales_price=[$6], ws_sold_date_sk=[$7], c_customer_sk=[$0], c_current_addr_sk=[$1], ca_address_sk=[$2], ca_county=[$3], ca_zip=[$4], d_date_sk=[$8], d_year=[$9], d_qoy=[$10]): rowcount = 6.019767031014723E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4426

            HiveJoinRel(condition=[=($5, $0)], joinType=[inner]): rowcount = 6.019767031014723E7, cumulative cost = {4.329894103414093E10 rows, 1.0 cpu, 0.0 io}, id = 4424

              HiveJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 7.064015632843196E7, cumulative cost = {1.2E8 rows, 0.0 cpu, 0.0 io}, id = 4392

                HiveProjectRel(c_customer_sk=[$0], c_current_addr_sk=[$4]): rowcount = 8.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4345

                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer]]): rowcount = 8.0E7, cumulative cost = {0}, id = 4101

                HiveProjectRel(ca_address_sk=[$0], ca_county=[$7], ca_zip=[$9]): rowcount = 4.0E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4349

                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.customer_address]]): rowcount = 4.0E7, cumulative cost = {0}, id = 4099

              HiveJoinRel(condition=[=($2, $3)], joinType=[inner]): rowcount = 3.856185436785714E7, cumulative cost = {4.318973902344464E10 rows, 1.0 cpu, 0.0 io}, id = 4395

                HiveProjectRel(ws_bill_customer_sk=[$1], ws_sales_price=[$2], ws_sold_date_sk=[$3]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100447E10 rows, 1.0 cpu, 0.0 io}, id = 4343

                  HiveProjectRel(ws_item_sk=[$0], ws_bill_customer_sk=[$1], ws_sales_price=[$2], ws_sold_date_sk=[$3], i_item_sk=[$4]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100447E10 rows, 1.0 cpu, 0.0 io}, id = 4388

                    HiveJoinRel(condition=[=($0, $4)], joinType=[inner]): rowcount = 2.1594638446E10, cumulative cost = {2.1595100447E10 rows, 1.0 cpu, 0.0 io}, id = 4383

                      HiveProjectRel(ws_item_sk=[$2], ws_bill_customer_sk=[$3], ws_sales_price=[$20], ws_sold_date_sk=[$33]): rowcount = 2.1594638446E10, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4277

                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.web_sales]]): rowcount = 2.1594638446E10, cumulative cost = {0}, id = 4096

                      HiveProjectRel(i_item_sk=[$0]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 4339

                        HiveFilterRel(condition=[=(1, 1)]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 4337

                          SemiJoinRel(condition=[=($1, $2)], joinType=[inner]): rowcount = 462000.0, cumulative cost = {1.0 rows, 1.0 cpu, 0.0 io}, id = 4335

                            HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4327

                              HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 4088

                            HiveProjectRel(i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4333

                              HiveProjectRel(i_item_sk=[$0], i_item_id=[$1]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4331

                                HiveFilterRel(condition=[in($0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29)]): rowcount = 1.05119214745814, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4329

                                  HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 4088

                HiveProjectRel(d_date_sk=[$0], d_year=[$6], d_qoy=[$10]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4355

                  HiveFilterRel(condition=[AND(=($10, 2), =($6, 2000))]): rowcount = 130.44464285714287, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4353

                    HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 4100




</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">8653</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-11-01 21:10:47" id="8461" opendate="2014-10-14 21:20:30" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Make Vectorized Decimal query results match Non-Vectorized query results with respect to trailing zeroes... .0000</summary>
			
			
			<description/>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDouble.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.IDecimalInExpr.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.TestConstantVectorExpression.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">8541</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-11-08 02:29:15" id="8752" opendate="2014-11-06 00:32:08" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Disjunction cardinality estimation has selectivity of 1</summary>
			
			
			<description>TPC-DS Q89 has the wrong join order.
Store_sales should be joining with item first then date_dim.
The issue is that the predicate on item shows a selectivity of 1 



((i_category in (&amp;amp;apos;Home&amp;amp;apos;,&amp;amp;apos;Books&amp;amp;apos;,&amp;amp;apos;Electronics&amp;amp;apos;) and

          i_class in (&amp;amp;apos;wallpaper&amp;amp;apos;,&amp;amp;apos;parenting&amp;amp;apos;,&amp;amp;apos;musical&amp;amp;apos;)

         )

      or (i_category in (&amp;amp;apos;Shoes&amp;amp;apos;,&amp;amp;apos;Jewelry&amp;amp;apos;,&amp;amp;apos;Men&amp;amp;apos;) and

          i_class in (&amp;amp;apos;womens&amp;amp;apos;,&amp;amp;apos;birdal&amp;amp;apos;,&amp;amp;apos;pants&amp;amp;apos;) 

        ))






                HiveProjectRel(i_item_sk=[$0], i_brand=[$8], i_class=[$10], i_category=[$12]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4052

                      HiveFilterRel(condition=[OR(AND(in($12, &amp;amp;apos;Home&amp;amp;apos;, &amp;amp;apos;Books&amp;amp;apos;, &amp;amp;apos;Electronics&amp;amp;apos;), in($10, &amp;amp;apos;wallpaper&amp;amp;apos;, &amp;amp;apos;parenting&amp;amp;apos;, &amp;amp;apos;musical&amp;amp;apos;)), AND(in($12, &amp;amp;apos;Shoes&amp;amp;apos;, &amp;amp;apos;Jewelry&amp;amp;apos;, &amp;amp;apos;Men&amp;amp;apos;), in($10, &amp;amp;apos;womens&amp;amp;apos;, &amp;amp;apos;birdal&amp;amp;apos;, &amp;amp;apos;pants&amp;amp;apos;)))]): rowcount = 462000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 4050

                        HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_30000.item]]): rowcount = 462000.0, cumulative cost = {0}, id = 3818



Query





select  *

from(

select i_category, i_class, i_brand,

       s_store_name, s_company_name,

       d_moy,

       sum(ss_sales_price) sum_sales,

       avg(sum(ss_sales_price)) over

         (partition by i_category, i_brand, s_store_name, s_company_name)

         avg_monthly_sales

from item, store_sales, date_dim, store

where store_sales.ss_item_sk = item.i_item_sk and

      store_sales.ss_sold_date_sk = date_dim.d_date_sk and

      store_sales.ss_store_sk = store.s_store_sk and

      d_year in (2000) and

        ((i_category in (&amp;amp;apos;Home&amp;amp;apos;,&amp;amp;apos;Books&amp;amp;apos;,&amp;amp;apos;Electronics&amp;amp;apos;) and

          i_class in (&amp;amp;apos;wallpaper&amp;amp;apos;,&amp;amp;apos;parenting&amp;amp;apos;,&amp;amp;apos;musical&amp;amp;apos;)

         )

      or (i_category in (&amp;amp;apos;Shoes&amp;amp;apos;,&amp;amp;apos;Jewelry&amp;amp;apos;,&amp;amp;apos;Men&amp;amp;apos;) and

          i_class in (&amp;amp;apos;womens&amp;amp;apos;,&amp;amp;apos;birdal&amp;amp;apos;,&amp;amp;apos;pants&amp;amp;apos;) 

        ))

group by i_category, i_class, i_brand,

         s_store_name, s_company_name, d_moy) tmp1

where case when (avg_monthly_sales &amp;lt;&amp;gt; 0) then (abs(sum_sales - avg_monthly_sales) / avg_monthly_sales) else null end &amp;gt; 0.1

order by sum_sales - avg_monthly_sales, s_store_name

limit 100



The result of the wrong join order is that the query runs in 335 seconds compared to 124 seconds with the correct join order.
Removing the disjunction in the item filter produces the correct plan



 i_category in (&amp;amp;apos;Home&amp;amp;apos;,&amp;amp;apos;Books&amp;amp;apos;,&amp;amp;apos;Electronics&amp;amp;apos;) and

          i_class in (&amp;amp;apos;wallpaper&amp;amp;apos;,&amp;amp;apos;parenting&amp;amp;apos;,&amp;amp;apos;musical&amp;amp;apos;)


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">8768</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2014-11-08 22:40:44" id="8768" opendate="2014-11-06 22:35:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO: Fix filter selectivity for &quot;in clause&quot; &amp; &quot;&lt;&gt;&quot; </summary>
			
			
			<description/>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">8752</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-01-08 17:50:26" id="9278" opendate="2015-01-07 01:44:45" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Cached expression feature broken in one case</summary>
			
			
			<description>Different query result depending on whether hive.cache.expr.evaluation is true or false.  When true, no query results are produced (this is wrong).
The q file:

set hive.cache.expr.evaluation=true;



CREATE TABLE cache_expr_repro (date_str STRING);

LOAD DATA LOCAL INPATH &amp;amp;apos;../../data/files/cache_expr_repro.txt&amp;amp;apos; INTO TABLE cache_expr_repro;



SELECT MONTH(date_str) AS `mon`, CAST((MONTH(date_str) - 1) / 3 + 1 AS int) AS `quarter`,   YEAR(date_str) AS `year` FROM cache_expr_repro WHERE ((CAST((MONTH(date_str) - 1) / 3 + 1 AS int) = 1) AND (YEAR(date_str) = 2015)) GROUP BY MONTH(date_str), CAST((MONTH(date_str) - 1) / 3 + 1 AS int),   YEAR(date_str) ;



cache_expr_repro.txt

2015-01-01 00:00:00

2015-02-01 00:00:00

2015-01-01 00:00:00

2015-02-01 00:00:00

2015-01-01 00:00:00

2015-01-01 00:00:00

2015-02-01 00:00:00

2015-02-01 00:00:00

2015-01-01 00:00:00

2015-01-01 00:00:00


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">9459</link>
			
			
			<link description="is duplicated by" type="Duplicate">9632</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-01-09 09:24:00" id="9249" opendate="2015-01-04 09:06:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveVarchar when joining tables</summary>
			
			
			<description>VectorColumnAssignFactory doesn&amp;amp;apos;t handle HiveCharWritable / HiveVarcharWritable objects.
Either:
HiveVarcharWritable cannot be cast to ... HiveVarchar
or
HiveCharWritable cannot be cast to ... HiveChar



Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveVarchar

	at org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory$17.assignObjectValue(VectorColumnAssignFactory.java:417)

	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.internalForward(VectorMapJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject(CommonJoinOperator.java:670)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:748)

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:299)

	... 24 more


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">9739</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-01-28 02:00:18" id="9459" opendate="2015-01-26 00:03:31" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Concat plus date functions appear to be broken in 0.14</summary>
			
			
			<description>In the below example I create year_month and month_year vars. These each should be yyyymm and mmyyyy integer strings but it appears as if hive is calling the first function twice such that it is returning mmmm and yyyyyyyy.
hive&amp;gt; select
    &amp;gt; month(a.joined) month,
    &amp;gt; year(a.joined) year,
    &amp;gt; concat(cast(year(a.joined) as string),cast(month(a.joined) as string)) year_month,
    &amp;gt; concat(cast(month(a.joined) as string),cast(year(a.joined) as string)) month_year
    &amp;gt; from a limit 20;
OK
month	year	year_month	month_year
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
7	2014	20142014	77
Time taken: 0.109 seconds, Fetched: 20 row(s)
Other users appear to experience similar issues in this stack overflow: http://stackoverflow.com/questions/27740866/convert-date-to-decimal-format-in-hive .
I tested this in 0.13 and 0.14 and it does not appear to be an issue in 0.13.
I looked around and could not find a similar issue so hopefully this is not a duplicate.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9278</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-02-09 09:59:25" id="9616" opendate="2015-02-09 05:21:49" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hive 0.14</summary>
			
			
			<description>Hi, 
I am using hive 0.14 version which will support all crud operation as said by support team
I am not able to select specific columns to insert, like 
insert into table table1 id,name,sal select id,name,sal from table2 where table1.id = table2.id</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9481</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-02-12 08:01:51" id="9632" opendate="2015-02-10 01:00:45" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>inconsistent results between year(), month(), day(), and the actual values in formulas</summary>
			
			
			<description>In wanting to create a date dimension value which would match our existing database environment, I figured I would be able to do as I have done in the past and use the following formula:
(year(date)*10000)+(month(date)*100)+day(date)
Given the date of 2015-01-09, the above formula should result in a value of 20150109.  Instead, the resulting value is 20353515.
SELECT
                          &amp;gt; adjusted_activity_date_utc,
                          &amp;gt; year(adjusted_activity_date_utc),
                          &amp;gt; month(adjusted_activity_date_utc),
                          &amp;gt; day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000),
                          &amp;gt; (month(adjusted_activity_date_utc)*100),
                          &amp;gt; day(adjusted_activity_date_utc)
                          &amp;gt; from event_histories limit 5;
OK
adjusted_activity_date_utc	_c1	_c2	_c3	_c4	_c5	_c6	_c7
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
2015-01-09	2015	1	9	20353515	20150000	100	9
Oddly enough, this works as expected when a specific date value is used for the column.
I have tried this with partition and non-partition columns and found the result to be the same.
SELECT
                          &amp;gt; adjusted_activity_date_utc,
                          &amp;gt; year(adjusted_activity_date_utc),
                          &amp;gt; month(adjusted_activity_date_utc),
                          &amp;gt; day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000)+(month(adjusted_activity_date_utc)*100)+day(adjusted_activity_date_utc),
                          &amp;gt; (year(adjusted_activity_date_utc)*10000),
                          &amp;gt; (month(adjusted_activity_date_utc)*100),
                          &amp;gt; day(adjusted_activity_date_utc)
                          &amp;gt; from event_histories
                          &amp;gt; where adjusted_activity_date_utc = &amp;amp;apos;2015-01-09&amp;amp;apos;
                          &amp;gt; limit 5;
OK
adjusted_activity_date_utc	_c1	_c2	_c3	_c4	_c5	_c6	_c7
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
2015-01-09	2015	1	9	20150109	20150000	100	9
</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9278</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-02-12 22:08:21" id="9669" opendate="2015-02-12 12:33:18" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>selected columns</summary>
			
			
			<description>Hi Team,
In Hive 1.0, selected columns patch is updated?
Because i am not able to insert selected columns.
I am using Hive 1.0 bin, how can i apply patch directly in Hive instead of trunk or source.
Thanks in advance.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9481</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-02-13 18:32:23" id="9481" opendate="2015-01-27 22:48:33" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>allow column list specification in INSERT statement</summary>
			
			
			<description>Given a table FOO(a int, b int, c int), ANSI SQL supports insert into FOO(c,b) select x,y from T.  The expectation is that &amp;amp;apos;x&amp;amp;apos; is written to column &amp;amp;apos;c&amp;amp;apos; and &amp;amp;apos;y&amp;amp;apos; is written column &amp;amp;apos;b&amp;amp;apos; and &amp;amp;apos;a&amp;amp;apos; is set to NULL, assuming column &amp;amp;apos;a&amp;amp;apos; is NULLABLE.
Hive does not support this.  In Hive one has to ensure that the data producing statement has a schema that matches target table schema.
Since Hive doesn&amp;amp;apos;t support DEFAULT value for columns in CREATE TABLE, when target schema is explicitly provided, missing columns will be set to NULL if they are NULLABLE, otherwise an error will be raised.
If/when DEFAULT clause is supported, this can be enhanced to set default value rather than NULL.
Thus, given 

create table source (a int, b int);

create table target (x int, y int, z int);

create table target2 (x int, y int, z int);




insert into target(y,z) select * from source;

will mean 

insert into target select null as x, a, b from source;

and 

insert into target(z,y) select * from source;

will meant 

insert into target select null as x, b, a from source;

Also,

from source 

  insert into target(y,z) select null as x, * 

  insert into target2(y,z) select null as x, source.*;



and for partitioned tables, given

Given:

CREATE TABLE pageviews (userid VARCHAR(64), link STRING, &quot;from&quot; STRING)

  PARTITIONED BY (datestamp STRING) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC;



INSERT INTO TABLE pageviews PARTITION (datestamp = &amp;amp;apos;2014-09-23&amp;amp;apos;)(userid,link)  

   VALUES (&amp;amp;apos;jsmith&amp;amp;apos;, &amp;amp;apos;mail.com&amp;amp;apos;);



And dynamic partitioning

INSERT INTO TABLE pageviews PARTITION (datestamp)(userid,datestamp,link) 

    VALUES (&amp;amp;apos;jsmith&amp;amp;apos;, &amp;amp;apos;2014-09-23&amp;amp;apos;, &amp;amp;apos;mail.com&amp;amp;apos;);



In all cases, the schema specification contains columns of the target table which are matched by position to the values produced by VALUES clause/SELECT statement.  If the producer side provides values for a dynamic partition column, the column should be in the specified schema.  Static partition values are part of the partition spec and thus are not produced by the producer and should not be part of the schema specification.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">9616</link>
			
			
			<link description="is duplicated by" type="Duplicate">9669</link>
			
			
			<link description="relates to" type="Reference">7646</link>
			
			
			<link description="is related to" type="Reference">10776</link>
			
			
			<link description="is related to" type="Reference">10828</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-03-03 02:17:00" id="9624" opendate="2015-02-09 19:21:04" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>NullPointerException in MapJoinOperator.processOp(MapJoinOperator.java:253) for TPC-DS Q75 against un-partitioned schema</summary>
			
			
			<description>Running TPC-DS Q75 against a non-partitioned schema fails with 



Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1422895755428_0924_1_29 [Reducer 27] killed/failed due to:null]



This line maps to hashMapRowGetters = new ReusableGetAdaptor[mapJoinTables.length] in the code snippet below



     alias = (byte) tag;

      if (hashMapRowGetters == null) {

        hashMapRowGetters = new ReusableGetAdaptor[mapJoinTables.length];

        MapJoinKey refKey = getRefKey(alias);

        for (byte pos = 0; pos &amp;lt; order.length; pos++) {

          if (pos != alias) {

            hashMapRowGetters[pos] = mapJoinTables[pos].createGetter(refKey);

          }

        }

      }



Query 





WITH all_sales AS (

 SELECT d_year

       ,i_brand_id

       ,i_class_id

       ,i_category_id

       ,i_manufact_id

       ,SUM(sales_cnt) AS sales_cnt

       ,SUM(sales_amt) AS sales_amt

 FROM (SELECT d_year

             ,i_brand_id

             ,i_class_id

             ,i_category_id

             ,i_manufact_id

             ,cs_quantity - COALESCE(cr_return_quantity,0) AS sales_cnt

             ,cs_ext_sales_price - COALESCE(cr_return_amount,0.0) AS sales_amt

       FROM catalog_sales JOIN item ON i_item_sk=cs_item_sk

                          JOIN date_dim ON d_date_sk=cs_sold_date_sk

                          LEFT JOIN catalog_returns ON (cs_order_number=cr_order_number 

                                                    AND cs_item_sk=cr_item_sk)

       WHERE i_category=&amp;amp;apos;Sports&amp;amp;apos;

       UNION ALL

       SELECT d_year

             ,i_brand_id

             ,i_class_id

             ,i_category_id

             ,i_manufact_id

             ,ss_quantity - COALESCE(sr_return_quantity,0) AS sales_cnt

             ,ss_ext_sales_price - COALESCE(sr_return_amt,0.0) AS sales_amt

       FROM store_sales JOIN item ON i_item_sk=ss_item_sk

                        JOIN date_dim ON d_date_sk=ss_sold_date_sk

                        LEFT JOIN store_returns ON (ss_ticket_number=sr_ticket_number 

                                                AND ss_item_sk=sr_item_sk)

       WHERE i_category=&amp;amp;apos;Sports&amp;amp;apos;

       UNION ALL

       SELECT d_year

             ,i_brand_id

             ,i_class_id

             ,i_category_id

             ,i_manufact_id

             ,ws_quantity - COALESCE(wr_return_quantity,0) AS sales_cnt

             ,ws_ext_sales_price - COALESCE(wr_return_amt,0.0) AS sales_amt

       FROM web_sales JOIN item ON i_item_sk=ws_item_sk

                      JOIN date_dim ON d_date_sk=ws_sold_date_sk

                      LEFT JOIN web_returns ON (ws_order_number=wr_order_number 

                                            AND ws_item_sk=wr_item_sk)

       WHERE i_category=&amp;amp;apos;Sports&amp;amp;apos;) sales_detail

 GROUP BY d_year, i_brand_id, i_class_id, i_category_id, i_manufact_id)

 SELECT  prev_yr.d_year AS prev_year

                          ,curr_yr.d_year AS year

                          ,curr_yr.i_brand_id

                          ,curr_yr.i_class_id

                          ,curr_yr.i_category_id

                          ,curr_yr.i_manufact_id

                          ,prev_yr.sales_cnt AS prev_yr_cnt

                          ,curr_yr.sales_cnt AS curr_yr_cnt

                          ,curr_yr.sales_cnt-prev_yr.sales_cnt AS sales_cnt_diff

                          ,curr_yr.sales_amt-prev_yr.sales_amt AS sales_amt_diff

 FROM all_sales curr_yr, all_sales prev_yr

 WHERE curr_yr.i_brand_id=prev_yr.i_brand_id

   AND curr_yr.i_class_id=prev_yr.i_class_id

   AND curr_yr.i_category_id=prev_yr.i_category_id

   AND curr_yr.i_manufact_id=prev_yr.i_manufact_id

   AND curr_yr.d_year=2002

   AND prev_yr.d_year=2002-1

   AND CAST(curr_yr.sales_cnt AS DECIMAL(17,2))/CAST(prev_yr.sales_cnt AS DECIMAL(17,2))&amp;lt;0.9

 ORDER BY sales_cnt_diff

 limit 100



explain 



STAGE DEPENDENCIES:

  Stage-1 is a root stage

  Stage-0 depends on stages: Stage-1



STAGE PLANS:

  Stage: Stage-1

    Tez

      Edges:

        Map 1 &amp;lt;- Map 6 (BROADCAST_EDGE)

        Map 14 &amp;lt;- Map 16 (BROADCAST_EDGE)

        Map 18 &amp;lt;- Reducer 15 (BROADCAST_EDGE), Union 3 (CONTAINS)

        Map 19 &amp;lt;- Map 23 (BROADCAST_EDGE)

        Map 26 &amp;lt;- Map 28 (BROADCAST_EDGE)

        Map 31 &amp;lt;- Map 33 (BROADCAST_EDGE)

        Map 35 &amp;lt;- Reducer 32 (BROADCAST_EDGE), Union 21 (CONTAINS)

        Map 9 &amp;lt;- Map 11 (BROADCAST_EDGE)

        Reducer 10 &amp;lt;- Map 12 (SIMPLE_EDGE), Map 13 (BROADCAST_EDGE), Map 9 (SIMPLE_EDGE), Union 3 (CONTAINS)

        Reducer 15 &amp;lt;- Map 14 (SIMPLE_EDGE), Map 17 (SIMPLE_EDGE)

        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE), Map 8 (BROADCAST_EDGE), Union 3 (CONTAINS)

        Reducer 20 &amp;lt;- Map 19 (SIMPLE_EDGE), Map 24 (SIMPLE_EDGE), Map 25 (BROADCAST_EDGE), Union 21 (CONTAINS)

        Reducer 22 &amp;lt;- Union 21 (SIMPLE_EDGE)

        Reducer 27 &amp;lt;- Map 26 (SIMPLE_EDGE), Map 29 (SIMPLE_EDGE), Map 30 (BROADCAST_EDGE), Union 21 (CONTAINS)

        Reducer 32 &amp;lt;- Map 31 (SIMPLE_EDGE), Map 34 (SIMPLE_EDGE)

        Reducer 4 &amp;lt;- Reducer 22 (BROADCAST_EDGE), Union 3 (SIMPLE_EDGE)

        Reducer 5 &amp;lt;- Reducer 4 (SIMPLE_EDGE)

      DagName: mmokhtar_20150207174141_8f167b31-c893-4c6e-86d6-855d20744d92:1

      Vertices:

        Map 1 

            Map Operator Tree:

                TableScan

                  alias: catalog_sales

                  filterExpr: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 817736652 Data size: 16354733056 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: cs_sold_date_sk (type: int), cs_item_sk (type: int), cs_order_number (type: int), cs_quantity (type: int), cs_ext_sales_price (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4

                      Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col0 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col1, _col2, _col3, _col4, _col6

                        input vertices:

                          1 Map 6

                        Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col2 (type: int), _col1 (type: int)

                          sort order: ++

                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)

                          Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE

                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)

            Execution mode: vectorized

        Map 11 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: 2002 (type: int)

            Execution mode: vectorized

        Map 12 

            Map Operator Tree:

                TableScan

                  alias: store_returns

                  filterExpr: sr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 167243952 Data size: 2675903232 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: sr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: sr_item_sk (type: int), sr_ticket_number (type: int), sr_return_quantity (type: int), sr_return_amt (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col1 (type: int), _col0 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)

                        Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: int), _col3 (type: float)

            Execution mode: vectorized

        Map 13 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col5

                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)

            Execution mode: vectorized

        Map 14 

            Map Operator Tree:

                TableScan

                  alias: web_sales

                  filterExpr: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 447759411 Data size: 8955188224 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: ws_sold_date_sk (type: int), ws_item_sk (type: int), ws_order_number (type: int), ws_quantity (type: int), ws_ext_sales_price (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4

                      Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col0 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col1, _col2, _col3, _col4, _col6

                        input vertices:

                          1 Map 16

                        Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col2 (type: int), _col1 (type: int)

                          sort order: ++

                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)

                          Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE

                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)

            Execution mode: vectorized

        Map 16 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: 2002 (type: int)

            Execution mode: vectorized

        Map 17 

            Map Operator Tree:

                TableScan

                  alias: web_returns

                  filterExpr: wr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 50457044 Data size: 807312704 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: wr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: wr_item_sk (type: int), wr_order_number (type: int), wr_return_quantity (type: int), wr_return_amt (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col1 (type: int), _col0 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)

                        Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: int), _col3 (type: float)

            Execution mode: vectorized

        Map 18 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                  Filter Operator

                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                    Select Operator

                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col5

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col1 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16

                        input vertices:

                          0 Reducer 15

                        Select Operator

                          expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)

                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                          Group By Operator

                            aggregations: sum(_col5), sum(_col6)

                            keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                            mode: hash

                            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                            Reduce Output Operator

                              key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                              sort order: +++++

                              Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                              value expressions: _col5 (type: bigint), _col6 (type: double)

            Execution mode: vectorized

        Map 19 

            Map Operator Tree:

                TableScan

                  alias: catalog_sales

                  filterExpr: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 817736652 Data size: 16354733056 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (cs_sold_date_sk is not null and cs_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: cs_sold_date_sk (type: int), cs_item_sk (type: int), cs_order_number (type: int), cs_quantity (type: int), cs_ext_sales_price (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4

                      Statistics: Num rows: 817736652 Data size: 16348976724 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col0 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col1, _col2, _col3, _col4, _col6

                        input vertices:

                          1 Map 23

                        Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col2 (type: int), _col1 (type: int)

                          sort order: ++

                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)

                          Statistics: Num rows: 148779579 Data size: 2380473264 Basic stats: COMPLETE Column stats: COMPLETE

                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)

            Execution mode: vectorized

        Map 23 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: ((d_year = 2001) and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((d_year = 2001) and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: 2001 (type: int)

            Execution mode: vectorized

        Map 24 

            Map Operator Tree:

                TableScan

                  alias: catalog_returns

                  filterExpr: cr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 108409176 Data size: 1734546816 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: cr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: cr_item_sk (type: int), cr_order_number (type: int), cr_return_quantity (type: int), cr_return_amount (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col1 (type: int), _col0 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)

                        Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: int), _col3 (type: float)

            Execution mode: vectorized

        Map 25 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col5

                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)

            Execution mode: vectorized

        Map 26 

            Map Operator Tree:

                TableScan

                  alias: store_sales

                  filterExpr: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 1174353612 Data size: 23487072256 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: ss_sold_date_sk (type: int), ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int), ss_ext_sales_price (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4

                      Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col0 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col1, _col2, _col3, _col4, _col6

                        input vertices:

                          1 Map 28

                        Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col2 (type: int), _col1 (type: int)

                          sort order: ++

                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)

                          Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE

                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)

            Execution mode: vectorized

        Map 28 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: ((d_year = 2001) and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((d_year = 2001) and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: 2001 (type: int)

            Execution mode: vectorized

        Map 29 

            Map Operator Tree:

                TableScan

                  alias: store_returns

                  filterExpr: sr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 167243952 Data size: 2675903232 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: sr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: sr_item_sk (type: int), sr_ticket_number (type: int), sr_return_quantity (type: int), sr_return_amt (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col1 (type: int), _col0 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)

                        Statistics: Num rows: 167243952 Data size: 2667828428 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: int), _col3 (type: float)

            Execution mode: vectorized

        Map 30 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col5

                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)

            Execution mode: vectorized

        Map 31 

            Map Operator Tree:

                TableScan

                  alias: web_sales

                  filterExpr: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 447759411 Data size: 8955188224 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (ws_sold_date_sk is not null and ws_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: ws_sold_date_sk (type: int), ws_item_sk (type: int), ws_order_number (type: int), ws_quantity (type: int), ws_ext_sales_price (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4

                      Statistics: Num rows: 447759411 Data size: 8955044072 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col0 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col1, _col2, _col3, _col4, _col6

                        input vertices:

                          1 Map 33

                        Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col2 (type: int), _col1 (type: int)

                          sort order: ++

                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)

                          Statistics: Num rows: 81465661 Data size: 1303450576 Basic stats: COMPLETE Column stats: COMPLETE

                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)

            Execution mode: vectorized

        Map 33 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: ((d_year = 2001) and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((d_year = 2001) and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: 2001 (type: int)

            Execution mode: vectorized

        Map 34 

            Map Operator Tree:

                TableScan

                  alias: web_returns

                  filterExpr: wr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 50457044 Data size: 807312704 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: wr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: wr_item_sk (type: int), wr_order_number (type: int), wr_return_quantity (type: int), wr_return_amt (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col1 (type: int), _col0 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)

                        Statistics: Num rows: 50457044 Data size: 804725540 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: int), _col3 (type: float)

            Execution mode: vectorized

        Map 35 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                  Filter Operator

                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                    Select Operator

                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col5

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col1 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16

                        input vertices:

                          0 Reducer 32

                        Select Operator

                          expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)

                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                          Group By Operator

                            aggregations: sum(_col5), sum(_col6)

                            keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                            mode: hash

                            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                            Reduce Output Operator

                              key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                              sort order: +++++

                              Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                              value expressions: _col5 (type: bigint), _col6 (type: double)

            Execution mode: vectorized

        Map 6 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 45363 Data size: 362905 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 405 Data size: 3240 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 405 Data size: 1620 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: 2002 (type: int)

            Execution mode: vectorized

        Map 7 

            Map Operator Tree:

                TableScan

                  alias: catalog_returns

                  filterExpr: cr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 108409176 Data size: 1734546816 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: cr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: cr_item_sk (type: int), cr_order_number (type: int), cr_return_quantity (type: int), cr_return_amount (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col1 (type: int), _col0 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col1 (type: int), _col0 (type: int)

                        Statistics: Num rows: 108409176 Data size: 1729935996 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: int), _col3 (type: float)

            Execution mode: vectorized

        Map 8 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                  Statistics: Num rows: 118835 Data size: 3089722 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((((((i_category = &amp;amp;apos;Sports&amp;amp;apos;) and i_item_sk is not null) and i_brand_id is not null) and i_class_id is not null) and i_category_id is not null) and i_manufact_id is not null) (type: boolean)

                    Statistics: Num rows: 11836 Data size: 1301772 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: i_item_sk (type: int), i_brand_id (type: int), i_class_id (type: int), i_category_id (type: int), i_manufact_id (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col5

                      Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 11836 Data size: 236532 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col5 (type: int)

            Execution mode: vectorized

        Map 9 

            Map Operator Tree:

                TableScan

                  alias: store_sales

                  filterExpr: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 1174353612 Data size: 23487072256 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (ss_sold_date_sk is not null and ss_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: ss_sold_date_sk (type: int), ss_item_sk (type: int), ss_ticket_number (type: int), ss_quantity (type: int), ss_ext_sales_price (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4

                      Statistics: Num rows: 1174353612 Data size: 23383406888 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col0 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col1, _col2, _col3, _col4, _col6

                        input vertices:

                          1 Map 11

                        Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col2 (type: int), _col1 (type: int)

                          sort order: ++

                          Map-reduce partition columns: _col2 (type: int), _col1 (type: int)

                          Statistics: Num rows: 213662719 Data size: 3418603504 Basic stats: COMPLETE Column stats: COMPLETE

                          value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int)

            Execution mode: vectorized

        Reducer 10 

            Reduce Operator Tree:

              Merge Join Operator

                condition map:

                     Left Outer Join0 to 1

                keys:

                  0 _col2 (type: int), _col1 (type: int)

                  1 _col1 (type: int), _col0 (type: int)

                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10

                Map Join Operator

                  condition map:

                       Inner Join 0 to 1

                  keys:

                    0 _col1 (type: int)

                    1 _col0 (type: int)

                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16

                  input vertices:

                    1 Map 13

                  Select Operator

                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)

                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                    Group By Operator

                      aggregations: sum(_col5), sum(_col6)

                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                      mode: hash

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        sort order: +++++

                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        value expressions: _col5 (type: bigint), _col6 (type: double)

        Reducer 15 

            Reduce Operator Tree:

              Merge Join Operator

                condition map:

                     Left Outer Join0 to 1

                keys:

                  0 _col2 (type: int), _col1 (type: int)

                  1 _col1 (type: int), _col0 (type: int)

                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10

                Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE

                Reduce Output Operator

                  key expressions: _col1 (type: int)

                  sort order: +

                  Map-reduce partition columns: _col1 (type: int)

                  Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE

                  value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int), _col9 (type: int), _col10 (type: float)

        Reducer 2 

            Reduce Operator Tree:

              Merge Join Operator

                condition map:

                     Left Outer Join0 to 1

                keys:

                  0 _col2 (type: int), _col1 (type: int)

                  1 _col1 (type: int), _col0 (type: int)

                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10

                Map Join Operator

                  condition map:

                       Inner Join 0 to 1

                  keys:

                    0 _col1 (type: int)

                    1 _col0 (type: int)

                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16

                  input vertices:

                    1 Map 8

                  Select Operator

                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)

                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                    Group By Operator

                      aggregations: sum(_col5), sum(_col6)

                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                      mode: hash

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        sort order: +++++

                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        value expressions: _col5 (type: bigint), _col6 (type: double)

        Reducer 20 

            Reduce Operator Tree:

              Merge Join Operator

                condition map:

                     Left Outer Join0 to 1

                keys:

                  0 _col2 (type: int), _col1 (type: int)

                  1 _col1 (type: int), _col0 (type: int)

                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10

                Map Join Operator

                  condition map:

                       Inner Join 0 to 1

                  keys:

                    0 _col1 (type: int)

                    1 _col0 (type: int)

                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16

                  input vertices:

                    1 Map 25

                  Select Operator

                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)

                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                    Group By Operator

                      aggregations: sum(_col5), sum(_col6)

                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                      mode: hash

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        sort order: +++++

                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        value expressions: _col5 (type: bigint), _col6 (type: double)

        Reducer 22 

            Reduce Operator Tree:

              Group By Operator

                aggregations: sum(VALUE._col0), sum(VALUE._col1)

                keys: KEY._col0 (type: int), KEY._col1 (type: int), KEY._col2 (type: int), KEY._col3 (type: int), KEY._col4 (type: int)

                mode: mergepartial

                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE

                Reduce Output Operator

                  key expressions: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                  sort order: ++++

                  Map-reduce partition columns: _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                  Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE

                  value expressions: _col0 (type: int), _col5 (type: bigint), _col6 (type: double)

        Reducer 27 

            Reduce Operator Tree:

              Merge Join Operator

                condition map:

                     Left Outer Join0 to 1

                keys:

                  0 _col2 (type: int), _col1 (type: int)

                  1 _col1 (type: int), _col0 (type: int)

                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10

                Map Join Operator

                  condition map:

                       Inner Join 0 to 1

                  keys:

                    0 _col1 (type: int)

                    1 _col0 (type: int)

                  outputColumnNames: _col3, _col4, _col6, _col9, _col10, _col12, _col13, _col14, _col16

                  input vertices:

                    1 Map 30

                  Select Operator

                    expressions: _col6 (type: int), _col12 (type: int), _col13 (type: int), _col14 (type: int), _col16 (type: int), (_col3 - COALESCE(_col9,0)) (type: int), (UDFToDouble(_col4) - COALESCE(_col10,0.0)) (type: double)

                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                    Group By Operator

                      aggregations: sum(_col5), sum(_col6)

                      keys: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                      mode: hash

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        sort order: +++++

                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                        value expressions: _col5 (type: bigint), _col6 (type: double)

        Reducer 32 

            Reduce Operator Tree:

              Merge Join Operator

                condition map:

                     Left Outer Join0 to 1

                keys:

                  0 _col2 (type: int), _col1 (type: int)

                  1 _col1 (type: int), _col0 (type: int)

                outputColumnNames: _col1, _col3, _col4, _col6, _col9, _col10

                Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE

                Reduce Output Operator

                  key expressions: _col1 (type: int)

                  sort order: +

                  Map-reduce partition columns: _col1 (type: int)

                  Statistics: Num rows: 8204 Data size: 164080 Basic stats: COMPLETE Column stats: COMPLETE

                  value expressions: _col3 (type: int), _col4 (type: float), _col6 (type: int), _col9 (type: int), _col10 (type: float)

        Reducer 4 

            Reduce Operator Tree:

              Group By Operator

                aggregations: sum(VALUE._col0), sum(VALUE._col1)

                keys: KEY._col0 (type: int), KEY._col1 (type: int), KEY._col2 (type: int), KEY._col3 (type: int), KEY._col4 (type: int)

                mode: mergepartial

                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE

                Map Join Operator

                  condition map:

                       Inner Join 0 to 1

                  keys:

                    0 _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                    1 _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int)

                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col12, _col13

                  input vertices:

                    1 Reducer 22

                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((CAST( _col5 AS decimal(17,2)) / CAST( _col12 AS decimal(17,2))) &amp;lt; CAST( 0.9 AS decimal(37,20))) (type: boolean)

                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE

                    Select Operator

                      expressions: _col7 (type: int), _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int), _col12 (type: bigint), _col5 (type: bigint), (_col5 - _col12) (type: bigint), (_col6 - _col13) (type: double)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9

                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col8 (type: bigint)

                        sort order: +

                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE

                        TopN Hash Memory Usage: 0.04

                        value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: int), _col5 (type: int), _col6 (type: bigint), _col7 (type: bigint), _col9 (type: double)

        Reducer 5 

            Reduce Operator Tree:

              Select Operator

                expressions: VALUE._col0 (type: int), VALUE._col1 (type: int), VALUE._col2 (type: int), VALUE._col3 (type: int), VALUE._col4 (type: int), VALUE._col5 (type: int), VALUE._col6 (type: bigint), VALUE._col7 (type: bigint), KEY.reducesinkkey0 (type: bigint), VALUE._col8 (type: double)

                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9

                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE

                Limit

                  Number of rows: 100

                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE

                  File Output Operator

                    compressed: false

                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: COMPLETE

                    table:

                        input format: org.apache.hadoop.mapred.TextInputFormat

                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

        Union 21 

            Vertex: Union 21

        Union 3 

            Vertex: Union 3



  Stage: Stage-0

    Fetch Operator

      limit: 100

      Processor Tree:

        ListSink



Time taken: 12.351 seconds, Fetched: 821 row(s)





The full exception



Status: Failed

15/02/07 17:43:20 [main]: ERROR SessionState: Status: Failed

Vertex failed, vertexName=Reducer 27, vertexId=vertex_1422895755428_0924_1_29, diagnostics=[Task failed, taskId=task_1422895755428_0924_1_29_000020, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1422895755428_0924_1_29 [Reducer 27] killed/failed due to:null]

15/02/07 17:43:20 [main]: ERROR SessionState: Vertex failed, vertexName=Reducer 27, vertexId=vertex_1422895755428_0924_1_29, diagnostics=[Task failed, taskId=task_1422895755428_0924_1_29_000020, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more

], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:328)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:269)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)

	... 13 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:{&quot;reducesinkkey0&quot;:76,&quot;reducesinkkey1&quot;:19267},&quot;value&quot;:{&quot;_col1&quot;:50,&quot;_col2&quot;:1629.5,&quot;_col4&quot;:2001}}

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:337)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:258)

	... 15 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: null

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:314)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:638)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(CommonJoinOperator.java:433)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:525)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(CommonJoinOperator.java:522)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject(CommonJoinOperator.java:451)

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:752)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:248)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:213)

	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.processOp(CommonMergeJoinOperator.java:196)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:328)

	... 16 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:253)

	... 27 more


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9832</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-03-12 20:05:55" id="9739" opendate="2015-02-20 14:28:49" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Various queries fails with Tez/ORC file org.apache.hadoop.hive.ql.exec.tez.TezTask due to Caused by: java.lang.ClassCastException</summary>
			
			
			<description>This fails when using Tez and ORC. 
It will run when text files are used or text/ORC and MapReduce and not Tez used.
Is this another example of a type issue per https://issues.apache.org/jira/browse/HIVE-9735
select rnum, c1, c2 from tset1 as t1 where exists ( select c1 from tset2 where c1 = t1.c1 )
This will run in both Tez and MapReduce using a text file
select rnum, c1, c2 from t_tset1 as t1 where exists ( select c1 from t_tset2 where c1 = t1.c1 )
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:294)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)
	... 13 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: org.apache.hadoop.hive.serde2.io.HiveCharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveChar
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:311)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.processOp(VectorMapJoinOperator.java:249)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.processOp(VectorFilterOperator.java:111)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
	... 17 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveCharWritable cannot be cast to org.apache.hadoop.hive.common.type.HiveChar
	at org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory$18.assignObjectValue(VectorColumnAssignFactory.java:432)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.internalForward(VectorMapJoinOperator.java:196)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject(CommonJoinOperator.java:670)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:748)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:299)
	... 24 more
create table  if not exists T_TSET1 (RNUM int , C1 int, C2 char(3))
 ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos; LINES TERMINATED BY &amp;amp;apos;\n&amp;amp;apos; 
 STORED AS textfile ;
create table  if not exists T_TSET1 (RNUM int , C1 int, C2 char(3))
 STORED AS ORC ;
create table  if not exists T_TSET2 (RNUM int , C1 int, C2 char(3))
 ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos; LINES TERMINATED BY &amp;amp;apos;\n&amp;amp;apos; 
 STORED AS textfile ;
TSET1 data
0|10|AAA
1|10|AAA
2|10|AAA
3|20|BBB
4|30|CCC
5|40|DDD
6|50|\N
7|60|\N
8|\N|AAA
9|\N|AAA
10|\N|\N
11|\N|\N
TSET2 DATA
0|10|AAA
1|10|AAA
2|40|DDD
3|50|EEE
4|60|FFF</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9249</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-04-10 00:08:13" id="9647" opendate="2015-02-10 23:54:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Discrepancy in cardinality estimates between partitioned and un-partitioned tables </summary>
			
			
			<description>High-level summary
HiveRelMdSelectivity.computeInnerJoinSelectivity relies on per column number of distinct value to estimate join selectivity.
The way statistics are aggregated for partitioned tables results in discrepancy in number of distinct values which results in different plans between partitioned and un-partitioned schemas.
The table below summarizes the NDVs in computeInnerJoinSelectivity which are used to estimate selectivity of joins.


Column	
Partitioned count distincts
 	Un-Partitioned count distincts


sr_customer_sk	
71,245	
1,415,625


sr_item_sk	
38,846
	62,562


sr_ticket_number	
71,245	
34,931,085


ss_customer_sk	
88,476
	1,415,625


ss_item_sk	
38,846
	62,562


ss_ticket_number
	100,756	
56,256,175


The discrepancy is because NDV calculation for a partitioned table assumes that the NDV range is contained within each partition and is calculates as &quot;select max(NUM_DISTINCTS) from PART_COL_STATS .
This is problematic for columns like ticket number which are naturally increasing with the partitioned date column ss_sold_date_sk.
Suggestions
Use Hyper Log Log as suggested by Gopal, there is an HLL implementation for HBASE co-porccessors which we can use as a reference here 
Using the global stats from TAB_COL_STATS and the per partition stats from PART_COL_STATS extrapolate the NDV for the qualified partitions as in :
Max ( (NUM_DISTINCTS from TAB_COL_STATS) x (Number of qualified partitions) / (Number of Partitions), max(NUM_DISTINCTS) from PART_COL_STATS))
More details
While doing TPC-DS Partitioned vs. Un-Partitioned runs I noticed that many of the plans are different, then I dumped the CBO logical plan and I found that join estimates are drastically different
Unpartitioned schema :



2015-02-10 11:33:27,624 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12624)) - Plan After Join Reordering:

HiveProjectRel(store_sales_quantitycount=[$0], store_sales_quantityave=[$1], store_sales_quantitystdev=[$2], store_sales_quantitycov=[/($2, $1)], as_store_returns_quantitycount=[$3], as_store_returns_quantityave=[$4], as_store_returns_quantitystdev=[$5], store_returns_quantitycov=[/($5, $4)]): rowcount = 1.0, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2956

  HiveAggregateRel(group=[{}], agg#0=[count($0)], agg#1=[avg($0)], agg#2=[stddev_samp($0)], agg#3=[count($1)], agg#4=[avg($1)], agg#5=[stddev_samp($1)]): rowcount = 1.0, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2954

    HiveProjectRel($f0=[$4], $f1=[$8]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2952

      HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$1], ss_customer_sk=[$2], ss_ticket_number=[$3], ss_quantity=[$4], sr_item_sk=[$5], sr_customer_sk=[$6], sr_ticket_number=[$7], sr_return_quantity=[$8], d_date_sk=[$9], d_quarter_name=[$10]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2982

        HiveJoinRel(condition=[=($9, $0)], joinType=[inner]): rowcount = 40.05611776795562, cumulative cost = {6.056835407771381E8 rows, 0.0 cpu, 0.0 io}, id = 2980

          HiveJoinRel(condition=[AND(AND(=($2, $6), =($1, $5)), =($3, $7))], joinType=[inner]): rowcount = 28880.460910696, cumulative cost = {6.05654559E8 rows, 0.0 cpu, 0.0 io}, id = 2964

            HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$2], ss_customer_sk=[$3], ss_ticket_number=[$9], ss_quantity=[$10]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2920

              HiveTableScanRel(table=[[tpcds_bin_orc_200.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 2822

            HiveProjectRel(sr_item_sk=[$2], sr_customer_sk=[$3], sr_ticket_number=[$9], sr_return_quantity=[$10]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2923

              HiveTableScanRel(table=[[tpcds_bin_orc_200.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 2823

          HiveProjectRel(d_date_sk=[$0], d_quarter_name=[$15]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2948

            HiveFilterRel(condition=[=($15, &amp;amp;apos;2000Q1&amp;amp;apos;)]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2946

              HiveTableScanRel(table=[[tpcds_bin_orc_200.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 2821



Partitioned schema :



2015-02-10 11:32:16,880 DEBUG [main]: parse.SemanticAnalyzer (SemanticAnalyzer.java:apply(12624)) - Plan After Join Reordering:

HiveProjectRel(store_sales_quantitycount=[$0], store_sales_quantityave=[$1], store_sales_quantitystdev=[$2], store_sales_quantitycov=[/($2, $1)], as_store_returns_quantitycount=[$3], as_store_returns_quantityave=[$4], as_store_returns_quantitystdev=[$5], store_returns_quantitycov=[/($5, $4)]): rowcount = 1.0, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2791

  HiveAggregateRel(group=[{}], agg#0=[count($0)], agg#1=[avg($0)], agg#2=[stddev_samp($0)], agg#3=[count($1)], agg#4=[avg($1)], agg#5=[stddev_samp($1)]): rowcount = 1.0, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2789

    HiveProjectRel($f0=[$3], $f1=[$8]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2787

      HiveProjectRel(ss_item_sk=[$4], ss_customer_sk=[$5], ss_ticket_number=[$6], ss_quantity=[$7], ss_sold_date_sk=[$8], sr_item_sk=[$0], sr_customer_sk=[$1], sr_ticket_number=[$2], sr_return_quantity=[$3], d_date_sk=[$9], d_quarter_name=[$10]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2817

        HiveJoinRel(condition=[AND(AND(=($5, $1), =($4, $0)), =($6, $2))], joinType=[inner]): rowcount = 100840.08570910375, cumulative cost = {6.064175958973647E8 rows, 0.0 cpu, 0.0 io}, id = 2815

          HiveProjectRel(sr_item_sk=[$1], sr_customer_sk=[$2], sr_ticket_number=[$8], sr_return_quantity=[$9]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2758

            HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 2658

          HiveJoinRel(condition=[=($5, $4)], joinType=[inner]): rowcount = 762935.5811373093, cumulative cost = {5.500766553162274E8 rows, 0.0 cpu, 0.0 io}, id = 2801

            HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_ticket_number=[$8], ss_quantity=[$9], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2755

              HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 2657

            HiveProjectRel(d_date_sk=[$0], d_quarter_name=[$15]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2783

              HiveFilterRel(condition=[=($15, &amp;amp;apos;2000Q1&amp;amp;apos;)]): rowcount = 101.31622746185853, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 2781

                HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 2656



This was puzzling knowing that the stats for both tables are identical in TAB_COL_STATS.
Column statistics from TAB_COL_STATS, notice how the column statistics are identical in both cases.


DB_NAME	
COLUMN_NAME
	COLUMN_TYPE
	NUM_NULLS
	LONG_HIGH_VALUE
	LONG_LOW_VALUE
	MAX_COL_LEN
	NUM_DISTINCTS


tpcds_bin_orc_200
	d_date_sk
	int
	0
	2,488,070
	2,415,022
	NULL
	65,332


tpcds_bin_partitioned_orc_200
	d_date_sk
	int
	0
	2,488,070
	2,415,022
	NULL
	65,332


tpcds_bin_orc_200	
d_quarter_name
	string
	0
	NULL
	NULL
	6
	721


tpcds_bin_partitioned_orc_200
	d_quarter_name
	string
	0
	NULL
	NULL
	6
	721


tpcds_bin_orc_200
	sr_customer_sk
	int
	1,009,571
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_partitioned_orc_200
	sr_customer_sk
	int
	1,009,571
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_orc_200	
sr_item_sk
	int
	0
	48,000
	1
	NULL
	62,562


tpcds_bin_partitioned_orc_200
	sr_item_sk
	int
	0
	48,000
	1
	NULL
	62,562


tpcds_bin_orc_200	
sr_ticket_number
	int
	0
	48,000,000
	1
	NULL
	34,931,085


tpcds_bin_partitioned_orc_200
	sr_ticket_number
	int
	0
	48,000,000
	1
	NULL
	34,931,085


tpcds_bin_orc_200	
ss_customer_sk
	int
	12,960,424
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_partitioned_orc_200
	ss_customer_sk
	int
	12,960,424
	1,600,000
	1
	NULL
	1,415,625


tpcds_bin_orc_200	
ss_item_sk
	int
	0
	48,000
	1
	NULL
	62,562


tpcds_bin_partitioned_orc_200
	ss_item_sk
	int
	0
	48,000
	1	
NULL
	62,562


tpcds_bin_orc_200
	ss_sold_date_sk
	int
	0
	2,452,642
	2,450,816
	NULL
	2,226


tpcds_bin_partitioned_orc_200
	ss_sold_date_sk	
int
	0
	2,452,642
	2,450,816
	NULL
	2,226


tpcds_bin_orc_200	
ss_ticket_number
	int
	0
	48,000,000
	1
	NULL
	56,256,175


tpcds_bin_partitioned_orc_200
	ss_ticket_number
	int
	0
	48,000,000
	1
	NULL
	56,256,175


For partitioned tables we get the statistics using get_aggr_stats_for which eventually issues the query below



select 

    COLUMN_NAME,

    COLUMN_TYPE,

    

    max(NUM_DISTINCTS),

    

from

    PART_COL_STATS

Where

where

    DB_NAME = 

        and TABLE_NAME = 

        and COLUMN_NAME in 

        and PARTITION_NAME in (1  N)

group by COLUMN_NAME , COLUMN_TYPE;



 </description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">9717</link>
			
			
			<link description="relates to" type="Reference">10612</link>
			
			
			<link description="is related to" type="Reference">9905</link>
			
			
			<link description="is related to" type="Reference">9717</link>
			
			
			<link description="is related to" type="Reference">9689</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-04-10 00:10:34" id="9717" opendate="2015-02-18 19:16:52" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>The max/min function used by AggrStats for decimal type is not what we expected</summary>
			
			
			<description>In current version hive-schema-1.2.0, in TABLE PART_COL_STATS, we store the &quot;BIG_DECIMAL_LOW_VALUE&quot; and &quot;BIG_DECIMAL_HIGH_VALUE&quot; as varchar. For example,
derby
&quot;BIG_DECIMAL_LOW_VALUE&quot; VARCHAR(4000), &quot;BIG_DECIMAL_HIGH_VALUE&quot; VARCHAR(4000)
mssql
BIG_DECIMAL_HIGH_VALUE varchar(255) NULL,
    BIG_DECIMAL_LOW_VALUE varchar(255) NULL,
mysql
`BIG_DECIMAL_LOW_VALUE` varchar(4000) CHARACTER SET latin1 COLLATE latin1_bin,
 `BIG_DECIMAL_HIGH_VALUE` varchar(4000) CHARACTER SET latin1 COLLATE latin1_bin,
oracle
BIG_DECIMAL_LOW_VALUE VARCHAR2(4000),
 BIG_DECIMAL_HIGH_VALUE VARCHAR2(4000),
postgres
&quot;BIG_DECIMAL_LOW_VALUE&quot; character varying(4000) DEFAULT NULL::character varying,
 &quot;BIG_DECIMAL_HIGH_VALUE&quot; character varying(4000) DEFAULT NULL::character varying,
And, when we do the aggrstats, we do a MAX/MIN of all the BIG_DECIMAL_HIGH_VALU/BIG_DECIMAL_LOW_VALUEE of partitions. We are expecting a max/min of a decimal (a number). However, it is actually a max/min of a varchar (a string). As a result, &amp;amp;apos;900&amp;amp;apos; is more than &amp;amp;apos;1000&amp;amp;apos;. This also affects the extrapolation of the status. The proposed solution is to use a CAST function to cast it to decimal. </description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.IExtrapolatePartStatus.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9647</link>
			
			
			<link description="relates to" type="Reference">9647</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-04-16 18:36:07" id="10356" opendate="2015-04-16 01:35:30" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>LLAP: query80 fails with vectorization cast issue </summary>
			
			
			<description>Reducer 6 fails:

Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134524737E8

\N\N01.2847032699693155E96.41569738480791E7-5.956161019898126E8

\N\N04.682909323885761E82.288924051203157E7-5.995957665973593E7

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:332)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:180)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:172)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:168)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134524737E8

\N\N01.2847032699693155E96.41569738480791E7-5.956161019898126E8

\N\N04.682909323885761E82.288924051203157E7-5.995957665973593E7

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:254)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134524737E8

\N\N01.2847032699693155E96.41569738480791E7-5.956161019898126E8

\N\N04.682909323885761E82.288924051203157E7-5.995957665973593E7

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1428572510173_0231_1_24 [Reducer 5] killed/failed due to:null]Vertex killed, vertexName=Reducer 6, vertexId=vertex_1428572510173_0231_1_25, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1428572510173_0231_1_25 [Reducer 6] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:1



How to repro: run query80 on scale factor 200. I might look tomorrow to see if this is specific to LLAP or not</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10244</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-05-28 23:19:50" id="10244" opendate="2015-04-07 21:25:05" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization : TPC-DS Q80 fails with java.lang.ClassCastException when hive.vectorized.execution.reduce.enabled is enabled</summary>
			
			
			<description>Query 



set hive.vectorized.execution.reduce.enabled=true;

with ssr as

 (select  s_store_id as store_id,

          sum(ss_ext_sales_price) as sales,

          sum(coalesce(sr_return_amt, 0)) as returns,

          sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit

  from store_sales left outer join store_returns on

         (ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number),

     date_dim,

     store,

     item,

     promotion

 where ss_sold_date_sk = d_date_sk

       and d_date between cast(&amp;amp;apos;1998-08-04&amp;amp;apos; as date) 

                  and (cast(&amp;amp;apos;1998-09-04&amp;amp;apos; as date))

       and ss_store_sk = s_store_sk

       and ss_item_sk = i_item_sk

       and i_current_price &amp;gt; 50

       and ss_promo_sk = p_promo_sk

       and p_channel_tv = &amp;amp;apos;N&amp;amp;apos;

 group by s_store_id)

 ,

 csr as

 (select  cp_catalog_page_id as catalog_page_id,

          sum(cs_ext_sales_price) as sales,

          sum(coalesce(cr_return_amount, 0)) as returns,

          sum(cs_net_profit - coalesce(cr_net_loss, 0)) as profit

  from catalog_sales left outer join catalog_returns on

         (cs_item_sk = cr_item_sk and cs_order_number = cr_order_number),

     date_dim,

     catalog_page,

     item,

     promotion

 where cs_sold_date_sk = d_date_sk

       and d_date between cast(&amp;amp;apos;1998-08-04&amp;amp;apos; as date)

                  and (cast(&amp;amp;apos;1998-09-04&amp;amp;apos; as date))

        and cs_catalog_page_sk = cp_catalog_page_sk

       and cs_item_sk = i_item_sk

       and i_current_price &amp;gt; 50

       and cs_promo_sk = p_promo_sk

       and p_channel_tv = &amp;amp;apos;N&amp;amp;apos;

group by cp_catalog_page_id)

 ,

 wsr as

 (select  web_site_id,

          sum(ws_ext_sales_price) as sales,

          sum(coalesce(wr_return_amt, 0)) as returns,

          sum(ws_net_profit - coalesce(wr_net_loss, 0)) as profit

  from web_sales left outer join web_returns on

         (ws_item_sk = wr_item_sk and ws_order_number = wr_order_number),

     date_dim,

     web_site,

     item,

     promotion

 where ws_sold_date_sk = d_date_sk

       and d_date between cast(&amp;amp;apos;1998-08-04&amp;amp;apos; as date)

                  and (cast(&amp;amp;apos;1998-09-04&amp;amp;apos; as date))

        and ws_web_site_sk = web_site_sk

       and ws_item_sk = i_item_sk

       and i_current_price &amp;gt; 50

       and ws_promo_sk = p_promo_sk

       and p_channel_tv = &amp;amp;apos;N&amp;amp;apos;

group by web_site_id)

  select  channel

        , id

        , sum(sales) as sales

        , sum(returns) as returns

        , sum(profit) as profit

 from 

 (select &amp;amp;apos;store channel&amp;amp;apos; as channel

        , concat(&amp;amp;apos;store&amp;amp;apos;, store_id) as id

        , sales

        , returns

        , profit

 from   ssr

 union all

 select &amp;amp;apos;catalog channel&amp;amp;apos; as channel

        , concat(&amp;amp;apos;catalog_page&amp;amp;apos;, catalog_page_id) as id

        , sales

        , returns

        , profit

 from  csr

 union all

 select &amp;amp;apos;web channel&amp;amp;apos; as channel

        , concat(&amp;amp;apos;web_site&amp;amp;apos;, web_site_id) as id

        , sales

        , returns

        , profit

 from   wsr

 ) x

 group by channel, id with rollup

 order by channel

         ,id

 limit 100



Exception 



Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]

15/04/07 05:14:52 [main]: ERROR SessionState: Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]

Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]

15/04/07 05:14:52 [main]: ERROR SessionState: Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]

DAG failed due to vertex failure. failedVertices:1 killedVertices:1

15/04/07 05:14:52 [main]: ERROR SessionState: DAG failed due to vertex failure. failedVertices:1 killedVertices:1

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:1

15/04/07 05:14:52 [main]: ERROR ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 5, vertexId=vertex_1426707664723_1377_1_22, diagnostics=[Task failed, taskId=task_1426707664723_1377_1_22_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:415)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:267)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:248)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)

	... 14 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) \N\N09.285817653506076E84.639990363237801E7-1.1814318134887291E8

\N\N04.682909323885761E82.2415242712669864E7-5.966176123188091E7

\N\N01.2847032699693155E96.300096113768728E7-5.94963316209578E8

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:394)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)

	... 16 more

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.copyGroupKey(VectorGroupKeyHelper.java:94)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeGroupBatches.processBatch(VectorGroupByOperator.java:729)

	at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:878)

	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectors(ReduceRecordSource.java:378)

	... 17 more

]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_1377_1_22 [Reducer 5] killed/failed due to:null]Vertex killed, vertexName=Reducer 6, vertexId=vertex_1426707664723_1377_1_23, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_1377_1_23 [Reducer 6] killed/failed due to:null]DAG failed due to vertex failure. failedVertices:1 killedVertices:1



Plan 





STAGE DEPENDENCIES:

  Stage-1 is a root stage

  Stage-0 depends on stages: Stage-1



STAGE PLANS:

  Stage: Stage-1

    Tez

      Edges:

        Map 12 &amp;lt;- Map 14 (BROADCAST_EDGE), Map 15 (BROADCAST_EDGE), Map 16 (BROADCAST_EDGE), Map 17 (BROADCAST_EDGE), Map 18 (BROADCAST_EDGE)

        Map 19 &amp;lt;- Map 21 (BROADCAST_EDGE), Map 22 (BROADCAST_EDGE), Map 23 (BROADCAST_EDGE), Map 24 (BROADCAST_EDGE), Map 25 (BROADCAST_EDGE)

        Reducer 13 &amp;lt;- Map 12 (SIMPLE_EDGE), Union 4 (CONTAINS)

        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE), Map 10 (BROADCAST_EDGE), Map 11 (BROADCAST_EDGE), Map 7 (SIMPLE_EDGE), Map 8 (BROADCAST_EDGE), Map 9 (BROADCAST_EDGE)

        Reducer 20 &amp;lt;- Map 19 (SIMPLE_EDGE), Union 4 (CONTAINS)

        Reducer 3 &amp;lt;- Reducer 2 (SIMPLE_EDGE), Union 4 (CONTAINS)

        Reducer 5 &amp;lt;- Union 4 (SIMPLE_EDGE)

        Reducer 6 &amp;lt;- Reducer 5 (SIMPLE_EDGE)

      DagName: mmokhtar_20150407051226_eb6d232e-cb00-4174-8b2f-d70aa2b3fb15:1

      Vertices:

        Map 1 

            Map Operator Tree:

                TableScan

                  alias: store_sales

                  filterExpr: ((ss_item_sk is not null and ss_promo_sk is not null) and ss_store_sk is not null) (type: boolean)

                  Statistics: Num rows: 550076554 Data size: 47370018896 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((ss_item_sk is not null and ss_promo_sk is not null) and ss_store_sk is not null) (type: boolean)

                    Statistics: Num rows: 524469260 Data size: 14487496336 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: ss_item_sk (type: int), ss_store_sk (type: int), ss_promo_sk (type: int), ss_ticket_number (type: int), ss_ext_sales_price (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                      Statistics: Num rows: 524469260 Data size: 14487496336 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col3 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col0 (type: int), _col3 (type: int)

                        Statistics: Num rows: 524469260 Data size: 14487496336 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: int), _col2 (type: int), _col4 (type: float), _col5 (type: float), _col6 (type: int)

            Execution mode: vectorized

        Map 10 

            Map Operator Tree:

                TableScan

                  alias: promotion

                  filterExpr: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)

                  Statistics: Num rows: 450 Data size: 530848 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)

                    Statistics: Num rows: 225 Data size: 20025 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: p_promo_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE

            Execution mode: vectorized

        Map 11 

            Map Operator Tree:

                TableScan

                  alias: store

                  filterExpr: s_store_sk is not null (type: boolean)

                  Statistics: Num rows: 212 Data size: 405680 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: s_store_sk is not null (type: boolean)

                    Statistics: Num rows: 212 Data size: 22048 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: s_store_sk (type: int), s_store_id (type: string)

                      outputColumnNames: _col0, _col1

                      Statistics: Num rows: 212 Data size: 22048 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 212 Data size: 22048 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: string)

            Execution mode: vectorized

        Map 12 

            Map Operator Tree:

                TableScan

                  alias: catalog_sales

                  filterExpr: ((cs_item_sk is not null and cs_promo_sk is not null) and cs_catalog_page_sk is not null) (type: boolean)

                  Statistics: Num rows: 286549727 Data size: 37743959324 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((cs_item_sk is not null and cs_promo_sk is not null) and cs_catalog_page_sk is not null) (type: boolean)

                    Statistics: Num rows: 285112475 Data size: 7974560516 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: cs_catalog_page_sk (type: int), cs_item_sk (type: int), cs_promo_sk (type: int), cs_order_number (type: int), cs_ext_sales_price (type: float), cs_net_profit (type: float), cs_sold_date_sk (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                      Statistics: Num rows: 285112475 Data size: 7974560516 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Left Outer Join0 to 1

                        keys:

                          0 _col1 (type: int), _col3 (type: int)

                          1 _col0 (type: int), _col1 (type: int)

                        outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col6, _col9, _col10

                        input vertices:

                          1 Map 14

                        Statistics: Num rows: 3412616 Data size: 109203712 Basic stats: COMPLETE Column stats: COMPLETE

                        Map Join Operator

                          condition map:

                               Inner Join 0 to 1

                          keys:

                            0 _col6 (type: int)

                            1 _col0 (type: int)

                          outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col9, _col10

                          input vertices:

                            1 Map 15

                          Statistics: Num rows: 3815661 Data size: 106838508 Basic stats: COMPLETE Column stats: COMPLETE

                          Map Join Operator

                            condition map:

                                 Inner Join 0 to 1

                            keys:

                              0 _col1 (type: int)

                              1 _col0 (type: int)

                            outputColumnNames: _col0, _col2, _col4, _col5, _col9, _col10

                            input vertices:

                              1 Map 16

                            Statistics: Num rows: 1271887 Data size: 30525288 Basic stats: COMPLETE Column stats: COMPLETE

                            Map Join Operator

                              condition map:

                                   Inner Join 0 to 1

                              keys:

                                0 _col2 (type: int)

                                1 _col0 (type: int)

                              outputColumnNames: _col0, _col4, _col5, _col9, _col10

                              input vertices:

                                1 Map 17

                              Statistics: Num rows: 635944 Data size: 12718880 Basic stats: COMPLETE Column stats: COMPLETE

                              Map Join Operator

                                condition map:

                                     Inner Join 0 to 1

                                keys:

                                  0 _col0 (type: int)

                                  1 _col0 (type: int)

                                outputColumnNames: _col4, _col5, _col9, _col10, _col18

                                input vertices:

                                  1 Map 18

                                Statistics: Num rows: 635944 Data size: 73769504 Basic stats: COMPLETE Column stats: COMPLETE

                                Select Operator

                                  expressions: _col18 (type: string), _col4 (type: float), COALESCE(_col9,0) (type: float), (_col5 - COALESCE(_col10,0)) (type: float)

                                  outputColumnNames: _col0, _col1, _col2, _col3

                                  Statistics: Num rows: 635944 Data size: 73769504 Basic stats: COMPLETE Column stats: COMPLETE

                                  Group By Operator

                                    aggregations: sum(_col1), sum(_col2), sum(_col3)

                                    keys: _col0 (type: string)

                                    mode: hash

                                    outputColumnNames: _col0, _col1, _col2, _col3

                                    Statistics: Num rows: 10590 Data size: 1313160 Basic stats: COMPLETE Column stats: COMPLETE

                                    Reduce Output Operator

                                      key expressions: _col0 (type: string)

                                      sort order: +

                                      Map-reduce partition columns: _col0 (type: string)

                                      Statistics: Num rows: 10590 Data size: 1313160 Basic stats: COMPLETE Column stats: COMPLETE

                                      value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double)

            Execution mode: vectorized

        Map 14 

            Map Operator Tree:

                TableScan

                  alias: catalog_returns

                  filterExpr: cr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 28798881 Data size: 2942039156 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: cr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 28798881 Data size: 456171072 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: cr_item_sk (type: int), cr_order_number (type: int), cr_return_amount (type: float), cr_net_loss (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 28798881 Data size: 456171072 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col1 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int)

                        Statistics: Num rows: 28798881 Data size: 456171072 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: float), _col3 (type: float)

            Execution mode: vectorized

        Map 15 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                      Select Operator

                        expressions: _col0 (type: int)

                        outputColumnNames: _col0

                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                        Group By Operator

                          keys: _col0 (type: int)

                          mode: hash

                          outputColumnNames: _col0

                          Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE

                          Dynamic Partitioning Event Operator

                            Target Input: catalog_sales

                            Partition key expr: cs_sold_date_sk

                            Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE

                            Target column: cs_sold_date_sk

                            Target Vertex: Map 12

            Execution mode: vectorized

        Map 16 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 48000 Data size: 68732712 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 16000 Data size: 127832 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: i_item_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE

            Execution mode: vectorized

        Map 17 

            Map Operator Tree:

                TableScan

                  alias: promotion

                  filterExpr: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)

                  Statistics: Num rows: 450 Data size: 530848 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)

                    Statistics: Num rows: 225 Data size: 20025 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: p_promo_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE

            Execution mode: vectorized

        Map 18 

            Map Operator Tree:

                TableScan

                  alias: catalog_page

                  filterExpr: cp_catalog_page_sk is not null (type: boolean)

                  Statistics: Num rows: 11718 Data size: 5400282 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: cp_catalog_page_sk is not null (type: boolean)

                    Statistics: Num rows: 11718 Data size: 1218672 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: cp_catalog_page_sk (type: int), cp_catalog_page_id (type: string)

                      outputColumnNames: _col0, _col1

                      Statistics: Num rows: 11718 Data size: 1218672 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 11718 Data size: 1218672 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: string)

            Execution mode: vectorized

        Map 19 

            Map Operator Tree:

                TableScan

                  alias: web_sales

                  filterExpr: ((ws_item_sk is not null and ws_promo_sk is not null) and ws_web_site_sk is not null) (type: boolean)

                  Statistics: Num rows: 143966864 Data size: 19001610332 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((ws_item_sk is not null and ws_promo_sk is not null) and ws_web_site_sk is not null) (type: boolean)

                    Statistics: Num rows: 143930635 Data size: 4029840544 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: ws_item_sk (type: int), ws_web_site_sk (type: int), ws_promo_sk (type: int), ws_order_number (type: int), ws_ext_sales_price (type: float), ws_net_profit (type: float), ws_sold_date_sk (type: int)

                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6

                      Statistics: Num rows: 143930635 Data size: 4029840544 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Left Outer Join0 to 1

                        keys:

                          0 _col0 (type: int), _col3 (type: int)

                          1 _col0 (type: int), _col1 (type: int)

                        outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col6, _col9, _col10

                        input vertices:

                          1 Map 21

                        Statistics: Num rows: 2406359 Data size: 77003488 Basic stats: COMPLETE Column stats: COMPLETE

                        Map Join Operator

                          condition map:

                               Inner Join 0 to 1

                          keys:

                            0 _col6 (type: int)

                            1 _col0 (type: int)

                          outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col9, _col10

                          input vertices:

                            1 Map 22

                          Statistics: Num rows: 2690560 Data size: 75335680 Basic stats: COMPLETE Column stats: COMPLETE

                          Map Join Operator

                            condition map:

                                 Inner Join 0 to 1

                            keys:

                              0 _col0 (type: int)

                              1 _col0 (type: int)

                            outputColumnNames: _col1, _col2, _col4, _col5, _col9, _col10

                            input vertices:

                              1 Map 23

                            Statistics: Num rows: 896854 Data size: 21524496 Basic stats: COMPLETE Column stats: COMPLETE

                            Map Join Operator

                              condition map:

                                   Inner Join 0 to 1

                              keys:

                                0 _col2 (type: int)

                                1 _col0 (type: int)

                              outputColumnNames: _col1, _col4, _col5, _col9, _col10

                              input vertices:

                                1 Map 24

                              Statistics: Num rows: 448427 Data size: 8968540 Basic stats: COMPLETE Column stats: COMPLETE

                              Map Join Operator

                                condition map:

                                     Inner Join 0 to 1

                                keys:

                                  0 _col1 (type: int)

                                  1 _col0 (type: int)

                                outputColumnNames: _col4, _col5, _col9, _col10, _col18

                                input vertices:

                                  1 Map 25

                                Statistics: Num rows: 448427 Data size: 52017532 Basic stats: COMPLETE Column stats: COMPLETE

                                Select Operator

                                  expressions: _col18 (type: string), _col4 (type: float), COALESCE(_col9,0) (type: float), (_col5 - COALESCE(_col10,0)) (type: float)

                                  outputColumnNames: _col0, _col1, _col2, _col3

                                  Statistics: Num rows: 448427 Data size: 52017532 Basic stats: COMPLETE Column stats: COMPLETE

                                  Group By Operator

                                    aggregations: sum(_col1), sum(_col2), sum(_col3)

                                    keys: _col0 (type: string)

                                    mode: hash

                                    outputColumnNames: _col0, _col1, _col2, _col3

                                    Statistics: Num rows: 17 Data size: 2108 Basic stats: COMPLETE Column stats: COMPLETE

                                    Reduce Output Operator

                                      key expressions: _col0 (type: string)

                                      sort order: +

                                      Map-reduce partition columns: _col0 (type: string)

                                      Statistics: Num rows: 17 Data size: 2108 Basic stats: COMPLETE Column stats: COMPLETE

                                      value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double)

            Execution mode: vectorized

        Map 21 

            Map Operator Tree:

                TableScan

                  alias: web_returns

                  filterExpr: wr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 13749816 Data size: 1237758344 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: wr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 13749816 Data size: 217404672 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: wr_item_sk (type: int), wr_order_number (type: int), wr_return_amt (type: float), wr_net_loss (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 13749816 Data size: 217404672 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col1 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int)

                        Statistics: Num rows: 13749816 Data size: 217404672 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: float), _col3 (type: float)

            Execution mode: vectorized

        Map 22 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                      Select Operator

                        expressions: _col0 (type: int)

                        outputColumnNames: _col0

                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                        Group By Operator

                          keys: _col0 (type: int)

                          mode: hash

                          outputColumnNames: _col0

                          Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE

                          Dynamic Partitioning Event Operator

                            Target Input: web_sales

                            Partition key expr: ws_sold_date_sk

                            Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE

                            Target column: ws_sold_date_sk

                            Target Vertex: Map 19

            Execution mode: vectorized

        Map 23 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 48000 Data size: 68732712 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 16000 Data size: 127832 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: i_item_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE

            Execution mode: vectorized

        Map 24 

            Map Operator Tree:

                TableScan

                  alias: promotion

                  filterExpr: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)

                  Statistics: Num rows: 450 Data size: 530848 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((p_channel_tv = &amp;amp;apos;N&amp;amp;apos;) and p_promo_sk is not null) (type: boolean)

                    Statistics: Num rows: 225 Data size: 20025 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: p_promo_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 225 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE

            Execution mode: vectorized

        Map 25 

            Map Operator Tree:

                TableScan

                  alias: web_site

                  filterExpr: web_site_sk is not null (type: boolean)

                  Statistics: Num rows: 38 Data size: 70614 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: web_site_sk is not null (type: boolean)

                    Statistics: Num rows: 38 Data size: 3952 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: web_site_sk (type: int), web_site_id (type: string)

                      outputColumnNames: _col0, _col1

                      Statistics: Num rows: 38 Data size: 3952 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 38 Data size: 3952 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col1 (type: string)

            Execution mode: vectorized

        Map 7 

            Map Operator Tree:

                TableScan

                  alias: store_returns

                  filterExpr: sr_item_sk is not null (type: boolean)

                  Statistics: Num rows: 55578005 Data size: 4155315616 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: sr_item_sk is not null (type: boolean)

                    Statistics: Num rows: 55578005 Data size: 881176504 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: sr_item_sk (type: int), sr_ticket_number (type: int), sr_return_amt (type: float), sr_net_loss (type: float)

                      outputColumnNames: _col0, _col1, _col2, _col3

                      Statistics: Num rows: 55578005 Data size: 881176504 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int), _col1 (type: int)

                        sort order: ++

                        Map-reduce partition columns: _col0 (type: int), _col1 (type: int)

                        Statistics: Num rows: 55578005 Data size: 881176504 Basic stats: COMPLETE Column stats: COMPLETE

                        value expressions: _col2 (type: float), _col3 (type: float)

            Execution mode: vectorized

        Map 8 

            Map Operator Tree:

                TableScan

                  alias: date_dim

                  filterExpr: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)

                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: (d_date BETWEEN 1998-08-04 AND 1998-09-04 and d_date_sk is not null) (type: boolean)

                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: d_date_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                      Select Operator

                        expressions: _col0 (type: int)

                        outputColumnNames: _col0

                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE

                        Group By Operator

                          keys: _col0 (type: int)

                          mode: hash

                          outputColumnNames: _col0

                          Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE

                          Dynamic Partitioning Event Operator

                            Target Input: store_sales

                            Partition key expr: ss_sold_date_sk

                            Statistics: Num rows: 18262 Data size: 73048 Basic stats: COMPLETE Column stats: COMPLETE

                            Target column: ss_sold_date_sk

                            Target Vertex: Map 1

            Execution mode: vectorized

        Map 9 

            Map Operator Tree:

                TableScan

                  alias: item

                  filterExpr: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)

                  Statistics: Num rows: 48000 Data size: 68732712 Basic stats: COMPLETE Column stats: COMPLETE

                  Filter Operator

                    predicate: ((i_current_price &amp;gt; 50.0) and i_item_sk is not null) (type: boolean)

                    Statistics: Num rows: 16000 Data size: 127832 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: i_item_sk (type: int)

                      outputColumnNames: _col0

                      Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE

                      Reduce Output Operator

                        key expressions: _col0 (type: int)

                        sort order: +

                        Map-reduce partition columns: _col0 (type: int)

                        Statistics: Num rows: 16000 Data size: 64000 Basic stats: COMPLETE Column stats: COMPLETE

            Execution mode: vectorized

        Reducer 13 

            Reduce Operator Tree:

              Group By Operator

                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)

                keys: KEY._col0 (type: string)

                mode: mergepartial

                outputColumnNames: _col0, _col1, _col2, _col3

                Select Operator

                  expressions: &amp;amp;apos;catalog channel&amp;amp;apos; (type: string), concat(&amp;amp;apos;catalog_page&amp;amp;apos;, _col0) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)

                  outputColumnNames: _col0, _col1, _col2, _col3, _col4

                  Group By Operator

                    aggregations: sum(_col2), sum(_col3), sum(_col4)

                    keys: _col0 (type: string), _col1 (type: string), &amp;amp;apos;0&amp;amp;apos; (type: string)

                    mode: hash

                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5

                    Reduce Output Operator

                      key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)

                      sort order: +++

                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string)

                      value expressions: _col3 (type: double), _col4 (type: double), _col5 (type: double)

        Reducer 2 

            Reduce Operator Tree:

              Merge Join Operator

                condition map:

                     Left Outer Join0 to 1

                keys:

                  0 _col0 (type: int), _col3 (type: int)

                  1 _col0 (type: int), _col1 (type: int)

                outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col6, _col9, _col10

                Statistics: Num rows: 7811006 Data size: 249952192 Basic stats: COMPLETE Column stats: COMPLETE

                Map Join Operator

                  condition map:

                       Inner Join 0 to 1

                  keys:

                    0 _col6 (type: int)

                    1 _col0 (type: int)

                  outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col9, _col10

                  input vertices:

                    1 Map 8

                  Statistics: Num rows: 8733520 Data size: 244538560 Basic stats: COMPLETE Column stats: COMPLETE

                  Map Join Operator

                    condition map:

                         Inner Join 0 to 1

                    keys:

                      0 _col0 (type: int)

                      1 _col0 (type: int)

                    outputColumnNames: _col1, _col2, _col4, _col5, _col9, _col10

                    input vertices:

                      1 Map 9

                    Statistics: Num rows: 2911174 Data size: 69868176 Basic stats: COMPLETE Column stats: COMPLETE

                    Map Join Operator

                      condition map:

                           Inner Join 0 to 1

                      keys:

                        0 _col2 (type: int)

                        1 _col0 (type: int)

                      outputColumnNames: _col1, _col4, _col5, _col9, _col10

                      input vertices:

                        1 Map 10

                      Statistics: Num rows: 1455587 Data size: 29111740 Basic stats: COMPLETE Column stats: COMPLETE

                      Map Join Operator

                        condition map:

                             Inner Join 0 to 1

                        keys:

                          0 _col1 (type: int)

                          1 _col0 (type: int)

                        outputColumnNames: _col4, _col5, _col9, _col10, _col18

                        input vertices:

                          1 Map 11

                        Statistics: Num rows: 1455587 Data size: 168848092 Basic stats: COMPLETE Column stats: COMPLETE

                        Select Operator

                          expressions: _col18 (type: string), _col4 (type: float), COALESCE(_col9,0) (type: float), (_col5 - COALESCE(_col10,0)) (type: float)

                          outputColumnNames: _col0, _col1, _col2, _col3

                          Statistics: Num rows: 1455587 Data size: 168848092 Basic stats: COMPLETE Column stats: COMPLETE

                          Group By Operator

                            aggregations: sum(_col1), sum(_col2), sum(_col3)

                            keys: _col0 (type: string)

                            mode: hash

                            outputColumnNames: _col0, _col1, _col2, _col3

                            Statistics: Num rows: 234 Data size: 29016 Basic stats: COMPLETE Column stats: COMPLETE

                            Reduce Output Operator

                              key expressions: _col0 (type: string)

                              sort order: +

                              Map-reduce partition columns: _col0 (type: string)

                              Statistics: Num rows: 234 Data size: 29016 Basic stats: COMPLETE Column stats: COMPLETE

                              value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: double)

        Reducer 20 

            Reduce Operator Tree:

              Group By Operator

                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)

                keys: KEY._col0 (type: string)

                mode: mergepartial

                outputColumnNames: _col0, _col1, _col2, _col3

                Select Operator

                  expressions: &amp;amp;apos;web channel&amp;amp;apos; (type: string), concat(&amp;amp;apos;web_site&amp;amp;apos;, _col0) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)

                  outputColumnNames: _col0, _col1, _col2, _col3, _col4

                  Group By Operator

                    aggregations: sum(_col2), sum(_col3), sum(_col4)

                    keys: _col0 (type: string), _col1 (type: string), &amp;amp;apos;0&amp;amp;apos; (type: string)

                    mode: hash

                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5

                    Reduce Output Operator

                      key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)

                      sort order: +++

                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string)

                      value expressions: _col3 (type: double), _col4 (type: double), _col5 (type: double)

        Reducer 3 

            Reduce Operator Tree:

              Group By Operator

                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)

                keys: KEY._col0 (type: string)

                mode: mergepartial

                outputColumnNames: _col0, _col1, _col2, _col3

                Select Operator

                  expressions: &amp;amp;apos;store channel&amp;amp;apos; (type: string), concat(&amp;amp;apos;store&amp;amp;apos;, _col0) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)

                  outputColumnNames: _col0, _col1, _col2, _col3, _col4

                  Group By Operator

                    aggregations: sum(_col2), sum(_col3), sum(_col4)

                    keys: _col0 (type: string), _col1 (type: string), &amp;amp;apos;0&amp;amp;apos; (type: string)

                    mode: hash

                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5

                    Reduce Output Operator

                      key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)

                      sort order: +++

                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string)

                      value expressions: _col3 (type: double), _col4 (type: double), _col5 (type: double)

        Reducer 5 

            Reduce Operator Tree:

              Group By Operator

                aggregations: sum(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)

                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string)

                mode: mergepartial

                outputColumnNames: _col0, _col1, _col3, _col4, _col5

                Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE

                pruneGroupingSetId: true

                Select Operator

                  expressions: _col0 (type: string), _col1 (type: string), _col3 (type: double), _col4 (type: double), _col5 (type: double)

                  outputColumnNames: _col0, _col1, _col2, _col3, _col4

                  Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE

                  Reduce Output Operator

                    key expressions: _col0 (type: string), _col1 (type: string)

                    sort order: ++

                    Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE

                    TopN Hash Memory Usage: 0.04

                    value expressions: _col2 (type: double), _col3 (type: double), _col4 (type: double)

            Execution mode: vectorized

        Reducer 6 

            Reduce Operator Tree:

              Select Operator

                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), VALUE._col0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: double)

                outputColumnNames: _col0, _col1, _col2, _col3, _col4

                Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE

                Limit

                  Number of rows: 100

                  Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE

                  File Output Operator

                    compressed: false

                    Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE

                    table:

                        input format: org.apache.hadoop.mapred.TextInputFormat

                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

            Execution mode: vectorized

        Union 4 

            Vertex: Union 4



  Stage: Stage-0

    Fetch Operator

      limit: 100

      Processor Tree:

        ListSink


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.2.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10356</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-05-29 00:26:44" id="9718" opendate="2015-02-18 20:04:10" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Insert into dynamic partitions with same column structure in the &quot;distibute by&quot; clause barfs</summary>
			
			
			<description>Sample reproducible query: 



SET hive.exec.dynamic.partition.mode=nonstrict;

SET hive.exec.dynamic.partition=true;



 insert overwrite table nation_new_p partition (some)

select n_name as name1, n_name as name2, n_name as name3 from nation distribute by name3;



Note: Make sure there is data in the source table to reproduce the issue. 
During the optimizations done for Jira: https://issues.apache.org/jira/browse/HIVE-4867, an optimization of deduplication of columns is done. But, when one of the columns is used as part of partitioned/distribute by, its not taken care of.  
The above query produces exception as follows:



Diagnostic Messages for this Task:

java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {&quot;n_nationkey&quot;:0,&quot;n_name&quot;:&quot;ALGERIA&quot;,&quot;n_regionkey&quot;:0,&quot;n_comment&quot;:&quot; haggle. carefully final deposits detect slyly agai&quot;}

	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)

	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)

	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)

	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)

	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)

	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:295)

	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:181)

	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:224)

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)

	at java.util.concurrent.FutureTask.run(FutureTask.java:262)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:744)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {&quot;n_nationkey&quot;:0,&quot;n_name&quot;:&quot;ALGERIA&quot;,&quot;n_regionkey&quot;:0,&quot;n_comment&quot;:&quot; haggle. carefully final deposits detect slyly agai&quot;}

	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)

	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)

	... 12 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0]

	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)

	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)

	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)

	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)

	... 13 more

Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0]

	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)

	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)

	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)

	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)

	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)

	... 19 more



Tables used are: 



CREATE EXTERNAL TABLE `nation`(

  `n_nationkey` int,

  `n_name` string,

  `n_regionkey` int,

  `n_comment` string)

ROW FORMAT DELIMITED

  FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos;

STORED AS INPUTFORMAT

  &amp;amp;apos;org.apache.hadoop.mapred.TextInputFormat&amp;amp;apos;

OUTPUTFORMAT

  &amp;amp;apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;amp;apos;;



and 



CREATE TABLE `nation_new_p`(

  `n_name1` string,

  `n_name2` string)

PARTITIONED BY (

  `some` string)

ROW FORMAT DELIMITED

  FIELDS TERMINATED BY &amp;amp;apos;|&amp;amp;apos;

STORED AS INPUTFORMAT

  &amp;amp;apos;org.apache.hadoop.mapred.TextInputFormat&amp;amp;apos;

OUTPUTFORMAT

  &amp;amp;apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&amp;amp;apos;



Sample data for the table is provided by the file attached with. </description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">9655</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-30 12:24:52" id="11095" opendate="2015-06-24 13:30:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>SerDeUtils  another bug ,when Text is reused</summary>
			
			
			<description>
The method transformTextFromUTF8 have a  error bug, It invoke a bad method of Text,getBytes()!

The method getBytes of Text returns the raw bytes; however, only data up to Text.length is valid.A better way is  use copyBytes()  if you need the returned array to be precisely the length of the data.

But the copyBytes is added behind hadoop1. 



How I found this bug
When i query data from a lzo table  I found in results  the length of the current row is always largr than the previous row and sometimesthe current row contains the contents of the previous row For example i execute a sql ,



select * from web_searchhub where logdate=2015061003



the result of sql see blow.Notice that ,the second row content contains the first row content.

INFO [03:00:05.589] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42098,session=3151,thread=254 2015061003

INFO [03:00:05.594] &amp;lt;18941e66-9962-44ad-81bc-3519f47ba274&amp;gt; session=901,thread=223ession=3151,thread=254 2015061003



The content of origin lzo file content see below ,just 2 rows.

INFO [03:00:05.635] &amp;lt;b88e0473-7530-494c-82d8-e2d2ebd2666c_forweb&amp;gt; session=3148,thread=285

INFO [03:00:05.635] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42095,session=3148,thread=285



I think this error is caused by the Text reuse,and I found the solutions .
Addicational, table create sql is : 



CREATE EXTERNAL TABLE `web_searchhub`(

`line` string)

PARTITIONED BY (

`logdate` string)

ROW FORMAT DELIMITED

FIELDS TERMINATED BY &amp;amp;apos;

U0000&amp;amp;apos;

WITH SERDEPROPERTIES (

&amp;amp;apos;serialization.encoding&amp;amp;apos;=&amp;amp;apos;GBK&amp;amp;apos;)

STORED AS INPUTFORMAT &quot;com.hadoop.mapred.DeprecatedLzoTextInputFormat&quot;

OUTPUTFORMAT &quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&quot;;

LOCATION

&amp;amp;apos;viewfs://nsX/user/hive/warehouse/raw.db/web/web_searchhub&amp;amp;apos; 


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10983</link>
			
			
			<link description="relates to" type="Reference">10983</link>
			
			
			<link description="is related to" type="Reference">11112</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-30 17:51:14" id="10983" opendate="2015-06-11 09:07:49" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>SerDeUtils bug  ,when Text is reused </summary>
			
			
			<description>
The mothod transformTextToUTF8 and transformTextFromUTF8  have a error bug,It invoke a bad method of Text,getBytes()!

The method getBytes of Text returns the raw bytes; however, only data up to Text.length is valid.A better way is  use copyBytes()  if you need the returned array to be precisely the length of the data.

But the copyBytes is added behind hadoop1. 



When i query data from a lzo table  I found  in results  the length of the current row is always largr  than the previous row and sometimesthe current  row contains the contents of the previous row For example i execute a sql ,



select *   from web_searchhub where logdate=2015061003



the result of sql see blow.Notice that ,the second row content contains the first row content.

INFO [03:00:05.589] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42098,session=3151,thread=254 2015061003

INFO [03:00:05.594] &amp;lt;18941e66-9962-44ad-81bc-3519f47ba274&amp;gt; session=901,thread=223ession=3151,thread=254 2015061003



The content  of origin lzo file content see below ,just 2 rows.

INFO [03:00:05.635] &amp;lt;b88e0473-7530-494c-82d8-e2d2ebd2666c_forweb&amp;gt; session=3148,thread=285

INFO [03:00:05.635] HttpFrontServer::FrontSH msgRecv:Remote=/10.13.193.68:42095,session=3148,thread=285



I think this error is caused by the Text reuse,and I found the solutions .
Addicational, table create sql is : 



CREATE EXTERNAL TABLE `web_searchhub`(

  `line` string)

PARTITIONED BY (

  `logdate` string)

ROW FORMAT DELIMITED

  FIELDS TERMINATED BY &amp;amp;apos;\\U0000&amp;amp;apos;

WITH SERDEPROPERTIES (

  &amp;amp;apos;serialization.encoding&amp;amp;apos;=&amp;amp;apos;GBK&amp;amp;apos;)

STORED AS INPUTFORMAT  &quot;com.hadoop.mapred.DeprecatedLzoTextInputFormat&quot;

          OUTPUTFORMAT &quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&quot;;



LOCATION

  &amp;amp;apos;viewfs://nsX/user/hive/warehouse/raw.db/web/web_searchhub&amp;amp;apos; 


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11095</link>
			
			
			<link description="duplicates" type="Duplicate">11112</link>
			
			
			<link description="incorporates" type="Incorporates">11112</link>
			
			
			<link description="is related to" type="Reference">11095</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-04 02:58:13" id="11657" opendate="2015-08-26 21:58:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HIVE-2573 introduces some issues during metastore init (and CLI init)</summary>
			
			
			<description>HIVE-2573 introduced static reload functions call.
It has a few problems:
1) When metastore client is initialized using an externally supplied config (i.e. Hive.get(HiveConf)), it still gets called during static init using the main service config. In my case, even though I have uris in the supplied config to connect to remote MS (which eventually happens), the static call creates objectstore, which is undesirable.
2) It breaks compat - old metastores do not support this call so new clients will fail, and there&amp;amp;apos;s no workaround like not using a new feature because the static call is always made</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11801</link>
			
			
			<link description="is related to" type="Reference">2573</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-09-11 19:21:11" id="11801" opendate="2015-09-11 18:21:16" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>In HMS HA env, &quot;show databases&quot; fails when&quot;current&quot; HMS is stopped.</summary>
			
			
			<description>Reproduce steps:

Enable HMS HA on a cluster
Use beeline to connect to HS2 and execute command show databases. Don&amp;amp;apos;t quit beeline after command has finished
Stop the first HMS in configuration hive.metastore.uri
Execute show databases in beeline again. Will get below error:

MetaException(message:Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe)




The error message in HS2 is as below:

2015-09-08 12:06:53,236 ERROR hive.log: Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe

org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe

	at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:161)

	at org.apache.thrift.transport.TSaslTransport.flush(TSaslTransport.java:501)

	at org.apache.thrift.transport.TSaslClientTransport.flush(TSaslClientTransport.java:37)

	at org.apache.hadoop.hive.thrift.TFilterTransport.flush(TFilterTransport.java:77)

	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:65)

	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_databases(ThriftHiveMetastore.java:692)

	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_databases(ThriftHiveMetastore.java:684)

	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:964)

	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:91)

	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)

	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1909)

	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)

	at org.apache.hive.service.cli.operation.GetSchemasOperation.runInternal(GetSchemasOperation.java:59)

	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)

	at org.apache.hive.service.cli.session.HiveSessionImpl.getSchemas(HiveSessionImpl.java:462)

	at org.apache.hive.service.cli.CLIService.getSchemas(CLIService.java:296)

	at org.apache.hive.service.cli.thrift.ThriftCLIService.GetSchemas(ThriftCLIService.java:534)

	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1373)

	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1358)

	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)

	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)

	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.net.SocketException: Broken pipe

	at java.net.SocketOutputStream.socketWrite0(Native Method)

	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)

	at java.net.SocketOutputStream.write(SocketOutputStream.java:153)

	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)

	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)

	at org.apache.thrift.transport.TIOStreamTransport.flush(TIOStreamTransport.java:159)

	... 31 more

2015-09-08 12:06:53,238 ERROR hive.log: Converting exception to MetaException

2015-09-08 12:06:53,238 WARN org.apache.hive.service.cli.thrift.ThriftCLIService: Error getting schemas:

org.apache.hive.service.cli.HiveSQLException: MetaException(message:Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe)

	at org.apache.hive.service.cli.operation.GetSchemasOperation.runInternal(GetSchemasOperation.java:65)

	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)

	at org.apache.hive.service.cli.session.HiveSessionImpl.getSchemas(HiveSessionImpl.java:462)

	at org.apache.hive.service.cli.CLIService.getSchemas(CLIService.java:296)

	at org.apache.hive.service.cli.thrift.ThriftCLIService.GetSchemas(ThriftCLIService.java:534)

	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1373)

	at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetSchemas.getResult(TCLIService.java:1358)

	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)

	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)

	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)

	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: MetaException(message:Got exception: org.apache.thrift.transport.TTransportException java.net.SocketException: Broken pipe)

	at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:1178)

	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:966)

	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:91)

	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)

	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1909)

	at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)

	at org.apache.hive.service.cli.operation.GetSchemasOperation.runInternal(GetSchemasOperation.java:59)

	... 13 more


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11657</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-06 18:33:07" id="7316" opendate="2014-06-30 20:30:46" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hive fails on zero length files</summary>
			
			
			<description>Flume will, at times, generate zero length files. This causes queries to fail on Avro data and likely sequence file as well.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11977</link>
			
			
			<link description="relates to" type="Reference">12607</link>
			
			
			<link description="relates to" type="Reference">1530</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-07 16:26:00" id="11977" opendate="2015-09-28 18:25:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive should handle an external avro table with zero length files present</summary>
			
			
			<description>If a zero length file is in the top level directory housing an external avro table,  all hive queries on the table fail.
This issue is that org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader creates a new org.apache.avro.file.DataFileReader and DataFileReader throws an exception when trying to read an empty file (because the empty file lacks the magic number marking it as avro).  
AvroGenericRecordReader should detect an empty file and then behave reasonably.
Caused by: java.io.IOException: Not a data file.
at org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:102)
at org.apache.avro.file.DataFileReader.&amp;lt;init&amp;gt;(DataFileReader.java:97)
at org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.&amp;lt;init&amp;gt;(AvroGenericRecordReader.java:81)
at org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.getRecordReader(AvroContainerInputFormat.java:51)
at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:246)
... 25 more</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">7316</link>
			
			
			<link description="relates to" type="Reference">12607</link>
			
			
			<link description="relates to" type="Reference">1530</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-30 08:25:47" id="9495" opendate="2015-01-28 08:53:16" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Map Side aggregation affecting map performance</summary>
			
			
			<description>When trying to run a simple aggregation query with hive.map.aggr=true, map tasks take a lot of time in Hive 0.14 as against  with hive.map.aggr=false.
e.g.
Consider the query:



INSERT OVERWRITE TABLE lineitem_tgt_agg

select alias.a0 as a0,

 alias.a2 as a1,

 alias.a1 as a2,

 alias.a3 as a3,

 alias.a4 as a4

from (

 select alias.a0 as a0,

  SUM(alias.a1) as a1,

  SUM(alias.a2) as a2,

  SUM(alias.a3) as a3,

  SUM(alias.a4) as a4

 from (

  select lineitem_sf500.l_orderkey as a0,

   CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * (1 - lineitem_sf500.l_discount) * (1 + lineitem_sf500.l_tax) as double) as a1,

   lineitem_sf500.l_quantity as a2,

   CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * lineitem_sf500.l_discount as double) as a3,

   CAST(lineitem_sf500.l_quantity * lineitem_sf500.l_extendedprice * lineitem_sf500.l_tax as double) as a4

  from lineitem_sf500

  ) alias

 group by alias.a0

 ) alias;



The above query was run with ~376GB of data / ~3billion records in the source.
It takes ~10 minutes with hive.map.aggr=false.
With map side aggregation set to true, the map tasks don&amp;amp;apos;t complete even after an hour.
</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">11502</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-05 18:18:06" id="12252" opendate="2015-10-23 23:03:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Streaming API HiveEndPoint can be created w/o partitionVals for partitioned table</summary>
			
			
			<description>When this happens, the write from Streaming API to this end point will succeed but it will place the data in the table directory which is not correct
Need to make the API throw in this case.
</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.3.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.ConnectionError.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-06 01:18:15" id="12315" opendate="2015-11-02 18:05:25" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>vectorization_short_regress.q has a wrong result issue for a double calculation</summary>
			
			
			<description>I suspect it is related to the fancy optimizations in vectorized double divide that try to quickly process the batch without checking each row for null.



 public static void setNullAndDivBy0DataEntriesDouble(

      DoubleColumnVector v, boolean selectedInUse, int[] sel, int n, DoubleColumnVector denoms) {

    assert v.isRepeating || !denoms.isRepeating;

    v.noNulls = false;

    double[] vector = denoms.vector;

    if (v.isRepeating &amp;amp;&amp;amp; (v.isNull[0] = (v.isNull[0] || vector[0] == 0))) {

      v.vector[0] = DoubleColumnVector.NULL_VALUE;


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-06 16:46:58" id="12344" opendate="2015-11-05 08:14:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Wrong types inferred for SemiJoin generation in CBO</summary>
			
			
			<description>The method projectNonColumnEquiConditions in HiveCalciteUtil will assign the type wrongly for newly created conditions. The problem is in this block:

      RexNode cond = rexBuilder.makeCall(SqlStdOperatorTable.EQUALS,

          rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newLeftOffset + i),

          rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newRightOffset + i));



It looks like a code copy-paste mistake. In addition, index i is incorrect, as newLeftFields contains all the fields, not only the ones of the new condition.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>0.14.1, 1.3.0, 2.0.0, 1.0.2, 1.2.2, 1.1.2</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-16 19:56:02" id="11036" opendate="2015-06-17 20:34:47" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Race condition in DataNucleus makes Metastore to hang</summary>
			
			
			<description>Under moderate to high concurrent query workload Metastore gets deadlocked in DataNucleus</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestMetastoreVersion.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hive.beeline.cli.TestHiveCli.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">6113</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-17 23:08:53" id="12384" opendate="2015-11-11 21:12:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Union Operator may produce incorrect result on TEZ</summary>
			
			
			<description>Union queries may produce incorrect result on TEZ.
TEZ removes union op, thus might loose the implicit cast in union op.
Reproduction test case:
set hive.cbo.enable=false;
set hive.execution.engine=tez;
select (x/sum over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select &amp;amp;apos;100000000&amp;amp;apos; x from (select * from src limit 2) s3)u order by y;
select (x/sum over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u order by y;</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12423</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-26 20:39:43" id="12307" opendate="2015-10-30 19:51:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Streaming API TransactionBatch.close() must abort any remaining transactions in the batch</summary>
			
			
			<description>When the client of TransactionBatch API encounters an error it must close() the batch and start a new one.  This prevents attempts to continue writing to a file that may damaged in some way.
The close() should ensure to abort the any txns that still remain in the batch and close (best effort) all the files it&amp;amp;apos;s writing to.  The batch should also put itself into a mode where any future ops on this batch fail.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.ConnectionError.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.TransactionError.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">12440</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-03 06:59:58" id="12537" opendate="2015-11-28 00:33:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>RLEv2 doesn&amp;apos;t seem to work</summary>
			
			
			<description>Perhaps I&amp;amp;apos;m doing something wrong or is actually working as expected.
Putting 1 million constant int32 values produces an ORC file of 1MB. Surprisingly, 1 million consecutive ints produces a much smaller file.
Code and FileDump attached.



ObjectInspector inspector = ObjectInspectorFactory.getReflectionObjectInspector(

		Integer.class, ObjectInspectorFactory.ObjectInspectorOptions.JAVA);

Writer w = OrcFile.createWriter(new Path(&quot;/tmp/my.orc&quot;), 

			OrcFile.writerOptions(new Configuration())

				.compress(CompressionKind.NONE)

				.inspector(inspector)

				.encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)

				.version(OrcFile.Version.V_0_12)

		);



for (int i = 0; i &amp;lt; 1000000; ++i) {

	w.addRow(123);

}

w.close();



</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-04 20:13:50" id="12717" opendate="2015-12-21 03:53:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Enabled to accept quoting of all character backslash qooting mechanism to json_tuple UDTF</summary>
			
			
			<description>Similar to HIVE-11825, we need to enable ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER property in json_tuple UDTF
For example in HIVE-11825, there are null return in below statement
(https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-json_tuple)



SELECT a.timestamp, b.*

  FROM log a LATERAL VIEW json_tuple(a.appevent, &amp;amp;apos;eventid&amp;amp;apos;, &amp;amp;apos;eventname&amp;amp;apos;) b as f1, f2;


</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-11 17:43:02" id="12660" opendate="2015-12-11 23:28:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HS2 memory leak with .hiverc file use</summary>
			
			
			<description>The Operation objects created to process .hiverc file in HS2 are not closed.
In HiveSessionImpl, GlobalHivercFileProcessor calls executeStatementInternal but ignores the OperationHandle it returns.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestSessionGlobalInitFile.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">5160</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-12 19:19:56" id="12821" opendate="2016-01-09 00:19:59" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Thrift and RDBMS schema changes for HIVE-11965</summary>
			
			
			<description/>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			
			
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12832</link>
			
			
			<link description="depends upon" type="dependent">12819</link>
			
			
			<link description="is depended upon by" type="dependent">12822</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-12 19:22:12" id="12823" opendate="2016-01-09 01:17:18" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Thrift and RDBMS schema changes for HIVE-11956</summary>
			
			
			<description/>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
			
			
			<file type="M">org.apache.hive.beeline.HiveSchemaTool.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.CheckLockRequest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ShowLocksRequest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.TxnInfo.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12832</link>
			
			
			<link description="depends upon" type="dependent">12822</link>
			
			
			<link description="is depended upon by" type="dependent">12829</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-16 21:09:16" id="13285" opendate="2016-03-15 00:29:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Orc concatenation may drop old files from moving to final path</summary>
			
			
			<description>ORC concatenation uses combine hive input format for merging files. Under specific case where all files within a combine split are incompatible for merge (old files without stripe statistics) then these files are added to incompatible file set. But this file set is not processed as closeOp() will not be called (no output file writer will exist which will skip super.closeOp()). As a result, these incompatible files are not moved to final path.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">7509</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-01 20:43:17" id="13412" opendate="2016-04-02 07:51:12" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>External table -  fields terminated by &amp;apos;\u0044&amp;apos; - 0044 is being interpreted as decimal and not hex</summary>
			
			
			<description>, (comma) as the decimal value of &amp;amp;apos;44&amp;amp;apos; and hex value of &amp;amp;apos;2c&amp;amp;apos;
In the following example I&amp;amp;apos;m using  &amp;amp;apos;\u0044&amp;amp;apos; as delimiter which is being interpreted as comma.
hive&amp;gt; create external table test_delimiter_dec_unicode (c1 int,c2 int,c3 int) row format delimited fields terminated by &amp;amp;apos;\u0044&amp;amp;apos;;
OK
Time taken: 0.035 seconds
hive&amp;gt; show create table test_delimiter_dec_unicode;
OK
CREATE EXTERNAL TABLE `test_delimiter_dec_unicode`(
  `c1` int,
  `c2` int,
  `c3` int)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY &amp;amp;apos;,&amp;amp;apos;
...</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13434</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-15 01:32:07" id="14238" opendate="2016-07-14 09:19:36" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Ownership shouldn&amp;apos;t be checked if external table location doesn&amp;apos;t exist</summary>
			
			
			<description>When creating external table with SQL authorization, we require RWX permission + ownership of the table location. If the location doesn&amp;amp;apos;t exist, we check on parent dir (recursively), which means we require the user owns everything under parent dir. I think this is not necessary - we don&amp;amp;apos;t have to check ownership of parent dir, or we just check non-recursively.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">10022</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-26 21:05:31" id="10022" opendate="2015-03-19 20:47:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Authorization checks for non existent file/directory should not be recursive</summary>
			
			
			<description>I am testing a query like : 
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;
create table auth_noupd(i int) clustered by  into 2 buckets stored as orc location &amp;amp;apos;$
{OUTPUT}
&amp;amp;apos; TBLPROPERTIES (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);
Now, in the above query,  since authorization is true, 
we would end up calling doAuthorizationV2() which ultimately ends up calling SQLAuthorizationUtils.getPrivilegesFromFS() which calls a recursive method : FileUtils.isActionPermittedForFileHierarchy() with the object or the ancestor of the object we are trying to authorize if the object does not exist. 
The logic in FileUtils.isActionPermittedForFileHierarchy() is DFS.
Now assume, we have a path as a/b/c/d that we are trying to authorize.
In case, a/b/c/d does not exist, we would call FileUtils.isActionPermittedForFileHierarchy() with say a/b/ assuming a/b/c also does not exist.
If under the subtree at a/b, we have millions of files, then FileUtils.isActionPermittedForFileHierarchy()  is going to check file permission on each of those objects. 
I do not completely understand why do we have to check for file permissions in all the objects in  branch of the tree that we are not  trying to read from /write to.  
We could have checked file permission on the ancestor that exists and if it matches what we expect, the return true.
Please confirm if this is a bug so that I can submit a patch else let me know what I am missing ?</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.common.FileUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14238</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-08-11 17:29:50" id="12954" opendate="2016-01-28 10:57:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NPE with str_to_map on null strings</summary>
			
			
			<description>Running str_to_map on a null string will return a NullPointerException.
Workaround is to use coalesce.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-29 17:44:23" id="14778" opendate="2016-09-16 18:14:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>document threading model of Streaming API</summary>
			
			
			<description>The model is not obvious and needs to be documented properly.
A StreamingConnection internally maintains 2 MetaStoreClient objects (each has 1 Thrift client for actual RPC). Let&amp;amp;apos;s call them &quot;primary&quot; and &quot;heartbeat&quot;. Each TransactionBatch created from a given StreamingConnection, gets a reference to both of these MetaStoreClients. 
So the model is that there is at most 1 outstanding (not closed) TransactionBatch for any given StreamingConnection and for any given TransactionBatch there can be at most 2 threads accessing it concurrently. 1 thread calling TransactionBatch.heartbeat() (and nothing else) and the other calling all other methods.</description>
			
			
			<version>0.14.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
</bugrepository>
