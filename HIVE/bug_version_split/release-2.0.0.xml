<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="HIVE">
	<bug fixdate="2010-09-27 19:17:28" id="1378" opendate="2010-06-01 19:56:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Return value for map, array, and struct needs to return a string </summary>
			
			
			<description>In order to be able to select/display any data from JDBC Hive driver, return value for map, array, and struct needs to return a string</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>0.7.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">1859</link>
			
			
			<link description="is duplicated by" type="Duplicate">1606</link>
			
			
			<link description="is duplicated by" type="Duplicate">1860</link>
			
			
			<link description="is duplicated by" type="Duplicate">1861</link>
			
			
			<link description="is duplicated by" type="Duplicate">1863</link>
			
			
			<link description="relates to" type="Reference">1126</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2011-01-04 20:09:44" id="1863" opendate="2010-12-23 04:02:31" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Boolean columns in Hive tables containing NULL are treated as FALSE by the Hive JDBC driver.</summary>
			
			
			<description>(1) Using the Hive CLI, create a table using...
create table dt4_boolean
(
        dt4_id          int,
        dt4_testbool    boolean,
        dt4_string      string
)
row format delimited
        fields  terminated by &amp;amp;apos;,&amp;amp;apos;
        lines  terminated by &amp;amp;apos;\n&amp;amp;apos;;
(2) Create a file containing the following text...
1,true,Value is True
2,null,Data says null and must be null
3,,No value that means null
4,NoIdea,Data says NoIdea that&amp;amp;apos;s gonna be null
5,false,Value is FALSE
(3) Load the data in the file into the Hive table...
load data local inpath &amp;amp;apos;&amp;lt;DATA FILE PATH&amp;gt;&amp;amp;apos; overwrite into table dt4_boolean;
(4) Check the table works as expected using the Hive CLI...
hive&amp;gt; select * from dt4_boolean;
OK
1	true	Value is True
2	NULL	Data says null and must be null
3	NULL	No value that means null
4	NULL	Data says NoIdea that&amp;amp;apos;s gonna be null
5	false	Value is FALSE
Time taken: 0.049 seconds
(5) Using the Hive JDBC driver, execute the same Hive query (select * from dt4_boolean)
(5.1) The &quot;row_str&quot; values obtained by the Hive JDBC driver for deserialization are correct...
1	true	Value is True
2	NULL	Data says null and must be null
3	NULL	No value that means null
4	NULL	Data says NoIdea that&amp;amp;apos;s gonna be null
5	false	Value is FALSE
(5.2) However, when these &quot;row_str&quot; are deserialized by the DynamicSerDe to a java.lang.Object, the NULL boolean values are converted to FALSE - instead of being null.
As a consequence, the application making use of the Hive JDBC driver produces this (incorrect) output...
SQL&amp;gt; select dt4_id, dt4_testbool from dt4_boolean;
    DT4_ID DT4_TESTBOOL
---------- ------------
         1            true
         2            false
         3            false
         4            false
         5            false
...instead of producing this (correct) output...
SQL&amp;gt; select dt4_id, dt4_testbool from dt4_boolean;
    DT4_ID DT4_TESTBOOL
---------- ------------
         1            true
         2            NULL
         3            NULL
         4            NULL
         5            false</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>0.7.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">1378</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-06-16 14:32:55" id="11021" opendate="2015-06-16 14:20:35" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>ObjectStore should call closeAll() on JDO query object to release the resources</summary>
			
			
			<description>In ObjectStore class, in getMDatabase() and getMTable(), after retrieving the database and table info from the database, we should call closeAll() on JDO query to release the resource. It would cause the cursor leaking on the database otherwise.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10895</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-07-13 17:10:16" id="11237" opendate="2015-07-13 16:30:52" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Unable to drop a default partition with &quot;int&quot; type</summary>
			
			
			<description>
CREATE TABLE test (col1 string) PARTITIONED BY (p1 int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;amp;apos;\001&amp;amp;apos; STORED AS TEXTFILE;

INSERT OVERWRITE TABLE test PARTITION (p1) SELECT code, IF(salary &amp;gt; 60000, 100, null) as p1 FROM default.sample_07;

hive&amp;gt; SHOW PARTITIONS test;

OK

p1=100

p1=__HIVE_DEFAULT_PARTITION__

Time taken: 0.124 seconds, Fetched: 2 row(s)



hive&amp;gt; ALTER TABLE test DROP partition (p1 = &amp;amp;apos;__HIVE_DEFAULT_PARTITION__&amp;amp;apos;);

FAILED: SemanticException Unexpected unknown partitions for (p1 = null)



The default partition name &amp;amp;apos;_HIVE_DEFAULT_PARTITION_&amp;amp;apos; cannot be deleted.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11208</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-10-26 05:17:21" id="12234" opendate="2015-10-22 21:09:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Beeline quit tries to connect again if no existing connections</summary>
			
			
			<description>Beeline !quit calls close(), which then does the following check:

beeLine.getDatabaseConnection().getConnection() != null



This inadvertently tries to connect again.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.beeline.DatabaseConnection.java</file>
			
			
			<file type="M">org.apache.hive.beeline.Commands.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-02 17:22:56" id="12215" opendate="2015-10-19 19:24:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Exchange partition does not show outputs field for post/pre execute hooks</summary>
			
			
			<description>The pre/post execute hook interface has fields that indicate which Hive objects were read / written to as a result of running the query. For the exchange partition operation, these fields (ReadEntity and WriteEntity) are empty. 
This is an important issue as the hook interface may be configured to perform critical warehouse operations.
See

ql/src/test/results/clientpositive/exchange_partition3.q.out




PREHOOK: query: -- This will exchange both partitions hr=1 and hr=2

ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&amp;amp;apos;2013-04-05&amp;amp;apos;) WITH TABLE exchange_part_test2

PREHOOK: type: ALTERTABLE_EXCHANGEPARTITION

POSTHOOK: query: -- This will exchange both partitions hr=1 and hr=2

ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds=&amp;amp;apos;2013-04-05&amp;amp;apos;) WITH TABLE exchange_part_test2

POSTHOOK: type: ALTERTABLE_EXCHANGEPARTITION



Seems it should also print output fields.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11554</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-02 23:54:51" id="11293" opendate="2015-07-17 18:45:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveConnection.setAutoCommit(true) throws exception</summary>
			
			
			<description>Effectively autoCommit is always true for HiveConnection, however setAutoCommit(true) throws exception, causing problems in existing JDBC code.
Should be 



  @Override

  public void setAutoCommit(boolean autoCommit) throws SQLException {

    if (!autoCommit) {

      throw new SQLException(&quot;disabling autocommit is not supported&quot;);

    }

  }


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.HiveConnection.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcDriver2.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">6712</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-05 16:53:26" id="12304" opendate="2015-10-30 17:17:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>&quot;drop database cascade&quot; needs to unregister functions</summary>
			
			
			<description>Currently &quot;drop database cascade&quot; command doesn&amp;amp;apos;t unregister the functions under the database. If the functions are not unregistered, in some cases like &quot;describe db1.func1&quot; will still show the info for the function; or if the same database is recreated, &quot;drop if exists db1.func1&quot; will throw an exception since the function is considered existing from the registry while it doesn&amp;amp;apos;t exist in metastore.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Registry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">14631</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-05 22:17:13" id="12207" opendate="2015-10-18 06:34:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Query fails when non-ascii characters are used in string literals</summary>
			
			
			<description>While debugging HIVE-11721 I found that using non-ascii characters in string literals causes calcite planner to throw the following exception:



2015-10-17T23:07:20,586 ERROR [main]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(292)) - CBO failed, skipping CBO.

org.apache.calcite.runtime.CalciteException: Failed to encode &amp;amp;apos;&amp;amp;apos; in character set &amp;amp;apos;ISO-8859-1&amp;amp;apos;



The query is:



select concat(&quot;&quot;, &quot;&quot;) from src limit 1;



Other queries with non-ascii literals fail as well.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-11 21:48:53" id="12208" opendate="2015-10-18 07:10:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorized JOIN NPE on dynamically partitioned hash-join + map-join</summary>
			
			
			<description>TPC-DS Q82 with reducer vectorized join optimizations



  Reducer 5 &amp;lt;- Map 1 (CUSTOM_SIMPLE_EDGE), Map 2 (CUSTOM_SIMPLE_EDGE), Map 3 (BROADCAST_EDGE), Map 4 (CUSTOM_SIMPLE_EDGE)






set hive.optimize.dynamic.partition.hashjoin=true;

set hive.vectorized.execution.reduce.enabled=true;

set hive.mapjoin.hybridgrace.hashtable=false;



select  i_item_id

       ,i_item_desc

       ,i_current_price

 from item, inventory, date_dim, store_sales

 where i_current_price between 30 and 30+30

 and inv_item_sk = i_item_sk

 and d_date_sk=inv_date_sk

 and d_date between &amp;amp;apos;2002-05-30&amp;amp;apos; and &amp;amp;apos;2002-07-30&amp;amp;apos;

 and i_manufact_id in (437,129,727,663)

 and inv_quantity_on_hand between 100 and 500

 and ss_item_sk = i_item_sk

 group by i_item_id,i_item_desc,i_current_price

 order by i_item_id

 limit 100



possibly a trivial plan setup issue, since the NPE is pretty much immediate.



Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:368)

	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852)

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.forwardBigTableBatch(VectorMapJoinGenerateResultOperator.java:603)

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:362)

	... 19 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.commonSetup(VectorMapJoinInnerGenerateResultOperator.java:112)

	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:96)

	... 22 more


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-13 18:15:02" id="12396" opendate="2015-11-12 19:28:14" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>BucketingSortingReduceSinkOptimizer may still throw IOB exception for duplicate columns</summary>
			
			
			<description>HIVE-12332 didn&amp;amp;apos;t fix the issue completely.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-14 21:13:58" id="12407" opendate="2015-11-13 15:10:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Check fetch property to determine if a SortLimit contains a limit operation</summary>
			
			
			<description>Now that Calcite 1.5 went in, sometimes we end up with Sort and Limit operations in the same operator. limitRelNode in HiveCalciteUtil should check the fetch property of the SortLimit operator to determine if an operator is a Limit.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-19 22:16:22" id="12405" opendate="2015-11-13 07:11:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Comparison bug in HiveSplitGenerator.InputSplitComparator#compare()</summary>
			
			
			<description>&quot;compare()&quot; method in HiveSplitGenerator.InputSplitComparator has the following condition on line 281 which is always false and is most likely a typo:



if (startPos1 &amp;gt; startPos1) {



As a result, in certain conditions splits might be sorted in incorrect order.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-11-20 22:34:08" id="12017" opendate="2015-10-02 15:41:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Do not disable CBO by default when number of joins in a query is equal or less than 1</summary>
			
			
			<description>Instead, we could disable some parts of CBO that are not relevant if the query contains 1 or 0 joins. Implementation should be able to define easily other query patterns for which we might disable some parts of CBO (in case we want to do it in the future).</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">12391</link>
			
			
			<link description="duplicates" type="Duplicate">12040</link>
			
			
			<link description="incorporates" type="Incorporates">12509</link>
			
			
			<link description="relates to" type="Reference">12465</link>
			
			
			<link description="relates to" type="Reference">12477</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-23 18:05:12" id="12456" opendate="2015-11-18 20:18:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>QueryId can&amp;apos;t be stored in the configuration of the SessionState since multiple queries can run in a single session</summary>
			
			
			<description>Follow up on HIVE-11488 which stores the queryId in the sessionState conf. If multiple queries run at  the same time, then the logging will get wrong queryId from the sessionState.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.operation.ExecuteStatementOperation.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.operation.Operation.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.operation.SQLOperation.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is required by" type="Required">11488</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-25 08:53:54" id="12399" opendate="2015-11-12 22:08:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Native Vector MapJoin can encounter  &quot;Null key not expected in MapJoin&quot; and &quot;Unexpected NULL in map join small table&quot; exceptions</summary>
			
			
			<description>Instead of throw exception, just filter out NULLs in the Native Vector MapJoin operators.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12533</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-25 18:11:34" id="12487" opendate="2015-11-20 20:09:37" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix broken MiniLlap tests</summary>
			
			
			<description>Currently MiniLlap tests fail with the following error:



TestMiniLlapCliDriver - did not produce a TEST-*.xml file



Supposedly, it started happening after HIVE-12319.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12319</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-26 19:41:47" id="12503" opendate="2015-11-24 03:05:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>GBY-Join transpose rule may go in infinite loop</summary>
			
			
			<description>This happens when pushing aggregate is not found to be any cheaper. Can be reproduced by running cbo_rp_auto_join1.q with flag turned on.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12508</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-11-29 22:36:00" id="12533" opendate="2015-11-27 02:53:09" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Unexpected NULL in map join small table</summary>
			
			
			<description>
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected NULL in map join small table

        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.load(VectorMapJoinFastHashTableLoader.java:110)

        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:293)

        at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:174)

        at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:170)

        at org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.retrieve(LlapObjectCache.java:104)

        ... 5 more

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected NULL in map join small table

        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.putRow(VectorMapJoinFastLongHashTable.java:88)

        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.putRow(VectorMapJoinFastTableContainer.java:182)

        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.load(VectorMapJoinFastHashTableLoader.java:97)

        ... 9 more



\cc Gopal V</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringCommon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12399</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-01 02:12:15" id="12490" opendate="2015-11-21 12:24:40" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Metastore: Mysql ANSI_QUOTES is not there for some cases</summary>
			
			
			<description>


Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &amp;amp;apos;&quot;PART_COL_STATS&quot; where &quot;DB_NAME&quot; = &amp;amp;apos;tpcds_100&amp;amp;apos; and &quot;TABLE_NAME&quot; =

 &amp;amp;apos;store_sales&amp;amp;apos; at line 1

...

        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) ~[datanucleus-api-jdo-3.2.6.jar:?]

        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321) ~[datanucleus-api-jdo-3.2.6.jar:?]

        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1644) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.partsFoundForPartitions(MetaStoreDirectSql.java:1227) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.aggrColStatsForPartitions(MetaStoreDirectSql.java:1157) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:6659) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:6655) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2493) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:6655) [hive-exec-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_40]


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-02 23:54:24" id="12508" opendate="2015-11-24 12:48:56" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>HiveAggregateJoinTransposeRule places a heavy load on the metadata system</summary>
			
			
			<description>Finding out whether the input is already unique requires a call to areColumnsUnique that currently (until CALCITE-794 is fixed) places a heavy load on the metadata system. This can lead to long CBO planning.
This is a temporary fix that avoid the call to the method till then.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12503</link>
			
			
			<link description="relates to" type="Reference">794</link>
			
			
			<link description="relates to" type="Reference">10785</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-03 16:04:36" id="12575" opendate="2015-12-02 22:00:59" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Similar to HIVE-12574, collect_set and count() give incorrect result when partition size is smaller than window size</summary>
			
			
			<description>Will  fix these two functions separately since seems the issue and fix would be different from the other functions like max() and last_value(), etc.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12574</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-03 19:46:05" id="12532" opendate="2015-11-26 13:39:58" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP Cache: Uncompressed data cache has NPE</summary>
			
			
			<description>


2015-11-26 08:28:45,232 [TezTaskRunner_attempt_1448429572030_0255_2_02_000019_2(attempt_1448429572030_0255_2_02_000019_2)] WARN org.apache.tez.runtime.LogicalIOProcessorRuntimeTask: Ignoring exception when closing input a(cleanup). Exception class=java.io.IOException, message=java.lang.NullPointerException

java.io.IOException: java.lang.NullPointerException

	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:283)

	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.close(LlapInputFormat.java:275)

	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doClose(HiveRecordReader.java:50)

	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.close(HiveContextAwareRecordReader.java:104)

	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.close(TezGroupedSplitsInputFormat.java:177)

	at org.apache.tez.mapreduce.lib.MRReaderMapred.close(MRReaderMapred.java:96)

	at org.apache.tez.mapreduce.input.MRInput.close(MRInput.java:559)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.cleanup(LogicalIOProcessorRuntimeTask.java:872)

	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:104)

	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.preReadUncompressedStream(EncodedReaderImpl.java:795)

	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:320)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)

	... 5 more



Not clear if current.next can set it to null before the continue; 



      assert partOffset &amp;lt;= current.getOffset();

      if (partOffset == current.getOffset() &amp;amp;&amp;amp; current instanceof CacheChunk) {

        // We assume cache chunks would always match the way we read, so check and skip it.

        assert current.getOffset() == partOffset &amp;amp;&amp;amp; current.getEnd() == partEnd;

        lastUncompressed = (CacheChunk)current;

        current = current.next;

        continue;

      }


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-04 18:50:58" id="12578" opendate="2015-12-03 00:01:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive query failing with error ClassCastException org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc cannot be cast to org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc</summary>
			
			
			<description>Two tables:

CREATE TABLE table_1 (boolean_col_1 BOOLEAN, float_col_2 FLOAT, bigint_col_3 BIGINT, varchar0111_col_4 VARCHAR(111), bigint_col_5 BIGINT, float_col_6 FLOAT, boolean_col_7 BOOLEAN, decimal0101_col_8 DECIMAL(1, 1), decimal0904_col_9 DECIMAL(9, 4), char0112_col_10 CHAR(112), double_col_11 DOUBLE, boolean_col_12 BOOLEAN, double_col_13 DOUBLE, varchar0142_col_14 VARCHAR(142), timestamp_col_15 TIMESTAMP, decimal0502_col_16 DECIMAL(5, 2), smallint_col_25 SMALLINT, decimal3222_col_18 DECIMAL(32, 22), boolean_col_19 BOOLEAN, decimal2012_col_20 DECIMAL(20, 12), char0204_col_21 CHAR(204), double_col_61 DOUBLE, timestamp_col_23 TIMESTAMP, int_col_24 INT, float_col_25 FLOAT, smallint_col_26 SMALLINT, double_col_27 DOUBLE, char0180_col_28 CHAR(180), decimal1503_col_29 DECIMAL(15, 3), timestamp_col_30 TIMESTAMP, smallint_col_31 SMALLINT, decimal2020_col_32 DECIMAL(20, 20), timestamp_col_33 TIMESTAMP, boolean_col_34 BOOLEAN, decimal3025_col_35 DECIMAL(30, 25), decimal3117_col_36 DECIMAL(31, 17), timestamp_col_37 TIMESTAMP, varchar0146_col_38 VARCHAR(146), boolean_col_39 BOOLEAN, double_col_40 DOUBLE, float_col_41 FLOAT, timestamp_col_42 TIMESTAMP, double_col_43 DOUBLE, boolean_col_44 BOOLEAN, timestamp_col_45 TIMESTAMP, tinyint_col_8 TINYINT, int_col_47 INT, decimal0401_col_48 DECIMAL(4, 1), varchar0064_col_49 VARCHAR(64), string_col_50 STRING, double_col_51 DOUBLE, string_col_52 STRING, boolean_col_53 BOOLEAN, int_col_54 INT, boolean_col_55 BOOLEAN, string_col_56 STRING, double_col_57 DOUBLE, varchar0131_col_58 VARCHAR(131), boolean_col_59 BOOLEAN, bigint_col_22 BIGINT, char0184_col_61 CHAR(184), varchar0173_col_62 VARCHAR(173), timestamp_col_63 TIMESTAMP, decimal1709_col_26 DECIMAL(20, 5), timestamp_col_65 TIMESTAMP, timestamp_col_66 TIMESTAMP, timestamp_col_67 TIMESTAMP, boolean_col_68 BOOLEAN, decimal1208_col_20 DECIMAL(33, 11), decimal1605_col_70 DECIMAL(16, 5), varchar0010_col_71 VARCHAR(10), tinyint_col_72 TINYINT, timestamp_col_10 TIMESTAMP, decimal2714_col_74 DECIMAL(27, 14), double_col_75 DOUBLE, boolean_col_76 BOOLEAN, double_col_77 DOUBLE, string_col_78 STRING, boolean_col_79 BOOLEAN, boolean_col_80 BOOLEAN, decimal0803_col_81 DECIMAL(8, 3), decimal1303_col_82 DECIMAL(13, 3), tinyint_col_83 TINYINT, decimal3424_col_84 DECIMAL(34, 24), float_col_85 FLOAT, boolean_col_86 BOOLEAN, char0233_col_87 CHAR(233));



CREATE TABLE table_18 (timestamp_col_1 TIMESTAMP, double_col_2 DOUBLE, boolean_col_3 BOOLEAN, timestamp_col_4 TIMESTAMP, decimal2103_col_5 DECIMAL(21, 3), char0221_col_6 CHAR(221), tinyint_col_7 TINYINT, float_col_8 FLOAT, int_col_2 INT, timestamp_col_10 TIMESTAMP, char0228_col_11 CHAR(228), timestamp_col_12 TIMESTAMP, double_col_13 DOUBLE, tinyint_col_6 TINYINT, tinyint_col_33 TINYINT, smallint_col_38 SMALLINT, boolean_col_17 BOOLEAN, double_col_18 DOUBLE, boolean_col_19 BOOLEAN, bigint_col_20 BIGINT, decimal0504_col_37 DECIMAL(37, 34), boolean_col_22 BOOLEAN, double_col_23 DOUBLE, timestamp_col_24 TIMESTAMP, varchar0076_col_25 VARCHAR(76), timestamp_col_18 TIMESTAMP, boolean_col_27 BOOLEAN, decimal1611_col_22 DECIMAL(37, 5), boolean_col_29 BOOLEAN);



Query:

SELECT

COALESCE(498, LEAD(COALESCE(-973, -684, 515)) OVER (PARTITION BY (t2.int_col_2 + t1.smallint_col_25) ORDER BY (t2.int_col_2 + t1.smallint_col_25), FLOOR(t1.double_col_61) DESC), 524) AS int_col,

(t2.int_col_2) + (t1.smallint_col_25) AS int_col_1,

FLOOR(t1.double_col_61) AS float_col,

COALESCE(SUM(COALESCE(62, -380, -435)) OVER (PARTITION BY (t2.int_col_2 + t1.smallint_col_25) ORDER BY (t2.int_col_2 + t1.smallint_col_25) DESC, FLOOR(t1.double_col_61) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 48 FOLLOWING), 704) AS int_col_2

FROM table_1 t1

INNER JOIN table_18 t2 ON (((t2.tinyint_col_6) = (t1.bigint_col_22)) AND ((t2.decimal0504_col_37) = (t1.decimal1709_col_26))) AND ((t2.tinyint_col_33) = (t1.tinyint_col_8))

WHERE

(t2.smallint_col_38) IN (SELECT

COALESCE(-92, -994) AS int_col

FROM table_1 tt1

INNER JOIN table_18 tt2 ON (tt2.decimal1611_col_22) = (tt1.decimal1208_col_20)

WHERE

(t1.timestamp_col_10) = (tt2.timestamp_col_18));



We get the following error:

ClassCastException org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc cannot be cast to org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc



We need to add support for constants in Select clause of semijoin subquery.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="supercedes" type="Supercedes">12296</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-07 09:57:04" id="12477" opendate="2015-11-20 03:35:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Left Semijoins are incompatible with a cross-product</summary>
			
			
			<description>with HIVE-12017 in place, a few queries generate left sem-joins without a key.
This is an invalid plan and can be produced by doing.



explain logical select count(1) from store_sales where ss_sold_date_sk in (select d_date_sk from date_dim where d_date_sk = 1);



LOGICAL PLAN:  

$hdt$_0:$hdt$_0:$hdt$_0:store_sales

  TableScan (TS_0)

    alias: store_sales

    filterExpr: (ss_sold_date_sk = 1) (type: boolean)

    Filter Operator (FIL_20)

      predicate: (ss_sold_date_sk = 1) (type: boolean)

      Select Operator (SEL_2)

        Reduce Output Operator (RS_9)

          sort order: 

          Join Operator (JOIN_11)

            condition map:

                 Left Semi Join 0 to 1

            keys:

              0 

              1 

            Group By Operator (GBY_14)

              aggregations: count(1)

              mode: hash



without CBO



sq_1:date_dim

  TableScan (TS_1)

    alias: date_dim

    filterExpr: ((1) IN (RS[6]) and (d_date_sk = 1)) (type: boolean)

    Filter Operator (FIL_21)

      predicate: ((1) IN (RS[6]) and (d_date_sk = 1)) (type: boolean)

      Select Operator (SEL_3)

        expressions: 1 (type: int)

        outputColumnNames: _col0

        Group By Operator (GBY_5)

          keys: _col0 (type: int)

          mode: hash

          outputColumnNames: _col0

          Reduce Output Operator (RS_8)

            key expressions: _col0 (type: int)

            sort order: +

            Map-reduce partition columns: _col0 (type: int)

            Join Operator (JOIN_9)

              condition map:

                   Left Semi Join 0 to 1

              keys:

                0 ss_sold_date_sk (type: int)

                1 _col0 (type: int)

              Group By Operator (GBY_12)

                aggregations: count(1)

                mode: hash


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13082</link>
			
			
			<link description="is related to" type="Reference">12017</link>
			
			
			<link description="is depended upon by" type="dependent">13164</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-07 19:24:03" id="12601" opendate="2015-12-04 23:56:19" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HIVE-11985 change does not use partition deserializer</summary>
			
			
			<description>As commented in https://reviews.apache.org/r/38862/diff/5?file=1102759#file1102759line786 , the function Hive.getFieldsFromDeserializerForMsStorage is ignoring the deserializer passed to it and it is taking from the table instead.
However, for the call to the function from Partition.java , that is not the right behavior. The partition can potentially have a different deserializer.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">11985</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-08 01:34:36" id="12574" opendate="2015-12-02 21:42:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>windowing function returns incorrect result when the window size is larger than the partition size</summary>
			
			
			<description>In PTF windowing, when the partition is small and the window size is larger than the partition size, we are seeing incorrect result. It happens for max, min, first_value, last_value and sum functions. 

CREATE TABLE sdy1(

ord int,

type string);



The data is:

2 a

3 a

1 a 



The result is as follows for the query select ord, min(ord) over (partition by type order by ord rows between 1 preceding and 7 following)

1 1

2 1

3 1 



The expected result is:

1 1

2 1

3 2


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12575</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-08 08:20:22" id="12603" opendate="2015-12-06 00:07:31" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Add config to block queries that scan &gt; N number of partitions </summary>
			
			
			<description>Strict mode is useful for blocking queries that load all partitions, but it&amp;amp;apos;s still possible to put significant load on the HMS for queries that scan a large number of partitions. It would be useful to add a config provide a hard limit to the number of partitions scanned by a query.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">6492</link>
			
			
			<link description="is related to" type="Reference">9499</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-08 19:12:51" id="12302" opendate="2015-10-30 06:45:30" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Use KryoPool instead of thread-local caching</summary>
			
			
			<description>Kryo 3.x introduces a Pooling mechanism for Kryo
https://github.com/EsotericSoftware/kryo#pooling-kryo-instances



// Build pool with SoftReferences enabled (optional)

KryoPool pool = new KryoPool.Builder(factory).softReferences().build();


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.TestConvertAstToSearchArg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.spark.SplitSparkWorkResolver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestMetastoreExpr.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.SerializeFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.TestParquetRowGroupFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.metadata.Hive.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="requires" type="Required">12175</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-09 00:02:37" id="12330" opendate="2015-11-04 01:28:25" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix precommit Spark test part2</summary>
			
			
			<description>Regression because of HIVE-11489</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-10 20:18:13" id="12572" opendate="2015-12-02 21:17:25" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>select partitioned acid table order by throws java.io.FileNotFoundException</summary>
			
			
			<description>Run the below queries:

create table test_acid (a int) partitioned by (b int) clustered by (a) into 2 buckets stored as orc tblproperties (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);

insert into table test_acid partition (b=1) values (1), (2), (3), (4);

select * from acid_partitioned order by a;



The above fails with the following error:

15/12/02 21:12:30 INFO SessionState: Map 1: 0(+0,-4)/1	Reducer 2: 0/1

Status: Failed

15/12/02 21:12:30 ERROR SessionState: Status: Failed

Vertex failed, vertexName=Map 1, vertexId=vertex_1449077191499_0023_1_00, diagnostics=[Task failed, taskId=task_1449077191499_0023_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task: attempt_1449077191499_0023_1_00_000000_0:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)

	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)

	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)

	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)

	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)



	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:195)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)

	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:348)

	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)

	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)

	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)

	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)

	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)

	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)

	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)

	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)



	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:74)

	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:340)

	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)

	... 14 more

Caused by: java.io.IOException: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)

	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)

	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)

	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)

	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)



	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)

	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)

	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)

	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)

	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)

	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)

	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:141)

	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:113)

	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62)

	... 16 more

Caused by: java.io.FileNotFoundException: Path is not a file: /apps/hive/warehouse/test_acid/b=1

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)

	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)

	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)

	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)

	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)



	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)

	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)

	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)

	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)

	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)

	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1242)

	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1227)

	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1215)

	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:303)

	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:269)

	at org.apache.hadoop.hdfs.DFSInputStream.&amp;lt;init&amp;gt;(DFSInputStream.java:261)

	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1540)

	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)

	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)

	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)

	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)

	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)

	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:462)

	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.&amp;lt;init&amp;gt;(ReaderImpl.java:338)

	at org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.&amp;lt;init&amp;gt;(ReaderImpl.java:33)

	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedOrcFile.createReader(EncodedOrcFile.java:28)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.ensureOrcReader(OrcEncodedDataReader.java:580)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.getOrReadFileMetadata(OrcEncodedDataReader.java:594)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:217)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)

	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)

	... 5 more

Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path is not a file: /apps/hive/warehouse/test_acid/b=1

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)

	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)

	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)

	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:652)

	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)

	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)

	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)

	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)



	at org.apache.hadoop.ipc.Client.call(Client.java:1427)

	at org.apache.hadoop.ipc.Client.call(Client.java:1358)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)

	at com.sun.proxy.$Proxy35.getBlockLocations(Unknown Source)

	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)

	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:497)

	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)

	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)

	at com.sun.proxy.$Proxy36.getBlockLocations(Unknown Source)

	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1240)

	... 30 more


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12632</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-10 20:28:49" id="12599" opendate="2015-12-04 22:12:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add logging to debug rare unexpected refCount error from the LLAP IO layer</summary>
			
			
			<description>


java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex fa

  &amp;lt;responseData class=&quot;java.lang.String&quot;&amp;gt;Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 5, vertexId=vertex_1449122740455_0665_7_00, d

  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:195)

  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)

  at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:348)

  at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)

  at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)

  at java.security.AccessController.doPrivileged(Native Method)

  at javax.security.auth.Subject.doAs(Subject.java:422)

  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

  at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)

  at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)

  at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

  at java.util.concurrent.FutureTask.run(FutureTask.java:266)

  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

  at java.lang.Thread.run(Thread.java:745)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)

  at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:74)

  at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:352)

  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)

  ... 14 more

Caused by: java.io.IOException: java.io.IOException: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)

  at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)

  at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)

  at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)

  at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)

  at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)

  at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)

  at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)

  at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)

  at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62)

  ... 16 more

Caused by: java.io.IOException: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)

  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:292)

  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.nextCvb(LlapInputFormat.java:248)

  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:176)

  at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:106)

  at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)

  ... 22 more

Caused by: java.lang.AssertionError: Unexpected refCount -1: 0x57c9bd50(-1)

  at org.apache.hadoop.hive.llap.cache.LlapDataBuffer.decRef(LlapDataBuffer.java:116)

  at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.unlockBuffer(LowLevelCacheImpl.java:349)

  at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.releaseBuffer(LowLevelCacheImpl.java:338)

  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$DataWrapperForOrc.releaseBuffer(OrcEncodedDataReader.java:922)

  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.releaseInitialRefcount(EncodedReaderImpl.java:1037)

  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.ponderReleaseInitialRefcount(EncodedReaderImpl.java:1026)

  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.prepareRangesForCompressedRead(EncodedReaderImpl.java:691)

  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedStream(EncodedReaderImpl.java:608)

  at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:395)

  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)

  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)

  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)

  at java.security.AccessController.doPrivileged(Native Method)

  at javax.security.auth.Subject.doAs(Subject.java:422)

  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)

  at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)

  ... 5 more



Configured to use the LRFU cache if that is relevant.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-11 03:47:19" id="12596" opendate="2015-12-04 20:01:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Delete timestamp row throws java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]</summary>
			
			
			<description>Run the below:

create table test_acid( i int, ts timestamp)

                      clustered by (i) into 2 buckets

                      stored as orc

                      tblproperties (&amp;amp;apos;transactional&amp;amp;apos;=&amp;amp;apos;true&amp;amp;apos;);

insert into table test_acid values (1, &amp;amp;apos;2014-09-14 12:34:30&amp;amp;apos;);

delete from test_acid where ts = &amp;amp;apos;2014-15-16 17:18:19.20&amp;amp;apos;;



The below error is thrown:

15/12/04 19:55:49 INFO SessionState: Map 1: -/-	Reducer 2: 0/2

Status: Failed

15/12/04 19:55:49 ERROR SessionState: Status: Failed

Vertex failed, vertexName=Map 1, vertexId=vertex_1447960616881_0022_2_00, diagnostics=[Vertex vertex_1447960616881_0022_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: test_acid initializer failed, vertex=vertex_1447960616881_0022_2_00 [Map 1], java.lang.IllegalArgumentException: Timestamp format must be yyyy-mm-dd hh:mm:ss[.fffffffff]

	at java.sql.Timestamp.valueOf(Timestamp.java:237)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.boxLiteral(ConvertAstToSearchArg.java:160)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.findLiteral(ConvertAstToSearchArg.java:191)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.createLeaf(ConvertAstToSearchArg.java:268)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.createLeaf(ConvertAstToSearchArg.java:326)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.parse(ConvertAstToSearchArg.java:377)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.&amp;lt;init&amp;gt;(ConvertAstToSearchArg.java:68)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.create(ConvertAstToSearchArg.java:417)

	at org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.createFromConf(ConvertAstToSearchArg.java:436)

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context.&amp;lt;init&amp;gt;(OrcInputFormat.java:484)

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1121)

	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1207)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:369)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:481)

	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:160)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:246)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:240)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:240)

	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:227)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)



Not sure if this change is intended as the issue is not seen with ver. 1.2</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="supercedes" type="Supercedes">12404</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-11 21:50:54" id="12609" opendate="2015-12-07 21:54:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Remove javaXML serialization</summary>
			
			
			<description>We use kryo as default serializer and javaXML based serialization is not used in many places and is also not well tested. We should remove javaXML serialization and make kryo as the only serialization option.  </description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SplitOpTreeForDPP.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PTFDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.AggregationDesc.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-12 00:07:28" id="12445" opendate="2015-11-17 23:50:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Tracking of completed dags is a slow memory leak</summary>
			
			
			<description>LLAP daemons track completed DAGs, but never clean up these structures. This is primarily to disallow out of order executions. Evaluate whether that can be avoided - otherwise this structure needs to be cleaned up with a delay.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.daemon.impl.QueryFileCleaner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-13 11:44:44" id="12662" opendate="2015-12-12 10:46:12" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>StackOverflowError in HiveSortJoinReduceRule when limit=0</summary>
			
			
			<description>L96 of HiveSortJoinReduceRule, you will see 

    // Finally, if we do not reduce the input size, we bail out

    if (RexLiteral.intValue(sortLimit.fetch)

            &amp;gt;= RelMetadataQuery.getRowCount(reducedInput)) {

      return false;

    }



It is using  RelMetadataQuery.getRowCount which is always at least 1. This is the problem that we resolved in CALCITE-987.
To confirm this, I just run the q file :

set hive.mapred.mode=nonstrict;

set hive.optimize.limitjointranspose=true;

set hive.optimize.limitjointranspose.reductionpercentage=1f;

set hive.optimize.limitjointranspose.reductiontuples=0;



explain

select *

from src src1 right outer join (

  select *

  from src src2 left outer join src src3

  on src2.value = src3.value) src2

on src1.key = src2.key

limit 0;



  And I got

2015-12-11T10:21:04,435 ERROR [c1efb099-f900-46dc-9f74-97af0944a99d main[]]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(301)) - CBO failed, skipping CBO.

java.lang.RuntimeException: java.lang.StackOverflowError

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.rethrowCalciteException(CalcitePlanner.java:749) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:645) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:264) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10076) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:223) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:456) [hive-exec-2.1.0-SNAPSHOT.jar:?]

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310) [hive-exec-2.1.0-SNAPSHOT.jar:?]

        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1138) [hive-exec-2.1.0-SNAPSHOT.jar:?]

        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1187) [hive-exec-2.1.0-SNAPSHOT.jar:?]

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1063) [hive-exec-2.1.0-SNAPSHOT.jar:?]



via Pengcheng Xiong</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortJoinReduceRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">11684</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-18 00:59:49" id="12435" opendate="2015-11-17 16:51:33" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>SELECT COUNT(CASE WHEN...) GROUPBY returns 1 for &amp;apos;NULL&amp;apos; in a case of ORC and vectorization is enabled.</summary>
			
			
			<description>Run the following query:

create table count_case_groupby (key string, bool boolean) STORED AS orc;

insert into table count_case_groupby values (&amp;amp;apos;key1&amp;amp;apos;, true),(&amp;amp;apos;key2&amp;amp;apos;, false),(&amp;amp;apos;key3&amp;amp;apos;, NULL),(&amp;amp;apos;key4&amp;amp;apos;, false),(&amp;amp;apos;key5&amp;amp;apos;,NULL);



The table contains the following:

key1	true

key2	false

key3	NULL

key4	false

key5	NULL



The below query returns:

SELECT key, COUNT(CASE WHEN bool THEN 1 WHEN NOT bool THEN 0 ELSE NULL END) AS cnt_bool0_ok FROM count_case_groupby GROUP BY key;

key1	1

key2	1

key3	1

key4	1

key5	1



while it expects the following results:

key1	1

key2	1

key3	0

key4	1

key5	0



The query works with hive ver 1.2. Also it works when a table is not orc format.
Also even if it&amp;amp;apos;s an orc table, when vectorization is disabled, the query works.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12209</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-18 06:59:28" id="12542" opendate="2015-11-30 16:51:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Create HiveRelFactories</summary>
			
			
			<description>Calcite 1.5.0 introduced the use of RelFactories to create the operators. In particular, RelFactories contains the factories for all the operators in the system. Although we can still implement old rules by providing each individual factory (the constructor is deprecated, but it won&amp;amp;apos;t be removed till Calcite 2.0.0 is out), new rules will only provide constructors based on RelFactories. Thus, we propose to migrate immediately to the new interface.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortLimit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveWindowingFixRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-18 23:37:36" id="12633" opendate="2015-12-09 19:48:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: package included serde jars</summary>
			
			
			<description>Some SerDes like JSONSerde are not packaged with LLAP. One cannot localize jars on the daemon (due to security consideration if nothing else), so we should package them.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-19 01:55:34" id="12685" opendate="2015-12-16 00:49:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Remove redundant hive-site.xml under common/src/test/resources/</summary>
			
			
			<description>Currently there&amp;amp;apos;s such a property as below, which is obviously wrong



&amp;lt;property&amp;gt;

  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;

  &amp;lt;value&amp;gt;hive-site.xml&amp;lt;/value&amp;gt;

  &amp;lt;description&amp;gt;Override ConfVar defined in HiveConf&amp;lt;/description&amp;gt;

&amp;lt;/property&amp;gt;


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.conf.TestHiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12670</link>
			
			
			<link description="relates to" type="Reference">12628</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-21 21:07:13" id="12632" opendate="2015-12-09 19:45:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: don&amp;apos;t use IO elevator for ACID tables </summary>
			
			
			<description>Until HIVE-12631 is fixed, we need to avoid ACID tables in IO elevator. Right now, a FileNotFound error is thrown.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">12648</link>
			
			
			<link description="is duplicated by" type="Duplicate">12572</link>
			
			
			<link description="relates to" type="Reference">12631</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-22 00:14:42" id="12712" opendate="2015-12-19 00:32:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveInputFormat may fail to column names to read in some cases</summary>
			
			
			<description>The primary issue is when plan is generated pathToAliases map is populated with directory paths to table aliases. pathToAliases.put() uses path.toString() as map key. During probing, path.toUri().toString() is used. This can cause probe misses when path contains spaces in them. path.toUri() will escape the spaces in the path whereas path.toString() does not escape the spaces. As a result, HiveInputFormat can trigger a different code path which can fail to set list of columns to read from the source table. This was causing unexpected NPE in OrcInputFormat (after refactoring HIVE-11705) which removed null check for column names. The resulting exception is 



Caused by: java.lang.RuntimeException: ORC split generation failed with exception: java.lang.NullPointerException

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1288)

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1354)

        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:367)

        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:457)

        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:152)

        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:246)

        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:240)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:422)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:240)

        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:227)

        at java.util.concurrent.FutureTask.run(FutureTask.java:266)

        ... 3 more

Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException

        at java.util.concurrent.FutureTask.report(FutureTask.java:122)

        at java.util.concurrent.FutureTask.get(FutureTask.java:192)

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1282)

        ... 15 more

Caused by: java.lang.NullPointerException

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:422)

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:417)

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.access$2000(OrcInputFormat.java:134)

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1072)

        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:919)

        ... 4 more




</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-23 19:25:44" id="12577" opendate="2015-12-02 22:42:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NPE in LlapTaskCommunicator when unregistering containers</summary>
			
			
			<description>


2015-12-02 13:29:00,160 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread

java.lang.NullPointerException

        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator$EntityTracker.unregisterContainer(LlapTaskCommunicator.java:586)

        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.registerContainerEnd(LlapTaskCommunicator.java:188)

        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:389)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)

        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)

        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)

        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)

        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)

        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)

        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)

        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)

        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)

        at java.lang.Thread.run(Thread.java:745)

2015-12-02 13:29:00,167 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread

java.lang.NullPointerException

        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:386)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)

        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)

        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)

        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)

        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)

        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)

        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)

        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)

        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)

        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)

        at java.lang.Thread.run(Thread.java:745)


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="D">org.apache.hadoop.hive.llap.tezplugins.TestTaskCommunicator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-24 17:43:59" id="12728" opendate="2015-12-22 06:51:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Apply DDL restrictions for ORC schema evolution</summary>
			
			
			<description>HIVE-11981 added schema evolution for ORC. However, it does not enforce any restrictions in DDL that can break schema evolution. Following changes have to be enforced in DDL to support the assumptions in schema evolution (that columns will only be added).
1) Restrict changing the file format of the table
2) Restrict changing the serde of the table
3) Restrict replacing columns to not drop columns or do unsupported type widening
4) Restrict reordering columns
5) Restrict unsupported type promotions</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">14406</link>
			
			
			<link description="relates to" type="Reference">11981</link>
			
			
			<link description="is related to" type="Reference">12625</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-24 19:51:43" id="12743" opendate="2015-12-24 03:57:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>RCFileInputFormat needs to be registered with kryo</summary>
			
			
			<description>Ran into an issue with union distinct query that uses RCFile table with the following exception



Caused by: java.lang.IllegalArgumentException: Unable to create serializer &quot;org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer&quot; for class: org.apache.hadoop.hive.ql.io.RCFileInputFormat

        at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]




</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2015-12-24 21:20:08" id="12741" opendate="2015-12-24 00:55:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HS2 ShutdownHookManager holds extra of Driver instance in master/branch-2.0</summary>
			
			
			<description>HIVE-12187 was meant to fix the described memory leak, however because of interaction with HIVE-12187 in branch-2.0/master, the fix fails to take effect.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hive.common.util.ShutdownHookManager.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13476</link>
			
			
			<link description="is related to" type="Reference">12583</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2015-12-27 06:24:01" id="12740" opendate="2015-12-23 20:05:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NPE with HS2 when using null input format</summary>
			
			
			<description>When we have a query that returns empty rows and when using tez with hs2, we hit NPE:



java.util.concurrent.ExecutionException: java.lang.NullPointerException

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)

	at java.util.concurrent.FutureTask.get(FutureTask.java:192)

	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)

	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)

	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)

	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)

	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)

	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)

	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)

	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)

	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)

	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)

	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)

	... 4 more

15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;/PERFLOG method=getSplits start=1450378746335 end=1450378746433 duration=98 from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat&amp;gt;

15/12/17 18:59:06 ERROR exec.Task: Failed to execute tez graph.

org.apache.tez.dag.api.TezUncheckedException: Failed to generate InputSplits

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:124)

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)

	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)

	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)

	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)

	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)

	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)

	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)

	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)

	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)

	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)

	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)

Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:502)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)

	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)

	... 23 more

Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)

	at java.util.concurrent.FutureTask.get(FutureTask.java:192)

	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)

	... 27 more

Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)

	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)

	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)

	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)

	... 4 more

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask

15/12/17 18:59:06 ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask

15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;/PERFLOG method=Driver.execute start=1450378746093 end=1450378746434 duration=341 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver&amp;gt;

15/12/17 18:59:06 INFO log.PerfLogger: &amp;lt;/PERFLOG method=releaseLocks start=1450378746434 end=1450378746434 duration=0 from=org.apache.hadoop.hive.ql.Driver&amp;gt;

15/12/17 18:59:06 ERROR operation.Operation: Error running hive query:

org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask

	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:367)

	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:183)

	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)

	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)

	at java.security.AccessController.doPrivileged(Native Method)

	at javax.security.auth.Subject.doAs(Subject.java:422)

	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)

	at java.util.concurrent.FutureTask.run(FutureTask.java:266)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

	at java.lang.Thread.run(Thread.java:745)


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-06 18:10:42" id="12766" opendate="2015-12-30 23:44:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TezTask does not close DagClient after execution</summary>
			
			
			<description>TezTask does not close DagClient after execution, this can result in objects/threads created by Tez/Yarn not getting freed.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">14210</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-07 22:39:55" id="12786" opendate="2016-01-06 01:59:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO may fail for recoverable errors</summary>
			
			
			<description>In some cases, CBO may generate an error from which it may be possible to recover. </description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-08 14:56:10" id="12762" opendate="2015-12-30 16:53:59" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Common join on parquet tables returns incorrect result when hive.optimize.index.filter set to true</summary>
			
			
			<description>The following query will give incorrect result.

CREATE TABLE tbl1(id INT) STORED AS PARQUET;

INSERT INTO tbl1 VALUES(1), (2);



CREATE TABLE tbl2(id INT, value STRING) STORED AS PARQUET;

INSERT INTO tbl2 VALUES(1, &amp;amp;apos;value1&amp;amp;apos;);

INSERT INTO tbl2 VALUES(1, &amp;amp;apos;value2&amp;amp;apos;);



set hive.optimize.index.filter = true;

set hive.auto.convert.join=false;

select tbl1.id, t1.value, t2.value

FROM tbl1

JOIN (SELECT * FROM tbl2 WHERE value=&amp;amp;apos;value1&amp;amp;apos;) t1 ON tbl1.id=t1.id

JOIN (SELECT * FROM tbl2 WHERE value=&amp;amp;apos;value2&amp;amp;apos;) t2 ON tbl1.id=t2.id;



We are enforcing to use common join and tbl2 will have 2 files after 2 insertions underneath.
the map job contains 3 TableScan operators (2 for tbl2 and 1 for tbl1). When    hive.optimize.index.filter is set to true, we are incorrectly applying the later filtering condition to each block, which causes no data is returned for the subquery SELECT * FROM tbl2 WHERE value=&amp;amp;apos;value1&amp;amp;apos;.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.ExpressionTree.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-09 11:52:07" id="12800" opendate="2016-01-07 18:00:03" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveFilterSetOpTransposeRule might be executed over non deterministic filter predicates</summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSetOpTransposeRule.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-09 15:00:09" id="12784" opendate="2016-01-05 19:52:54" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Group by SemanticException: Invalid column reference</summary>
			
			
			<description>Some queries work fine in older versions throws SemanticException, the stack trace:

FAILED: SemanticException [Error 10002]: Line 96:1 Invalid column reference &amp;amp;apos;key2&amp;amp;apos;

15/12/21 18:56:44 [main]: ERROR ql.Driver: FAILED: SemanticException [Error 10002]: Line 96:1 Invalid column reference &amp;amp;apos;key2&amp;amp;apos;

org.apache.hadoop.hive.ql.parse.SemanticException: Line 96:1 Invalid column reference &amp;amp;apos;key2&amp;amp;apos;

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanGroupByOperator1(SemanticAnalyzer.java:4228)

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5670)

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:9007)

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9884)

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9777)

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10250)

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10261)

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10141)

at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)

at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)

at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)

at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)

at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1158)

at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)

at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1037)

at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)

at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)

at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)

at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)

at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:403)

at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:419)

at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:708)

at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)

at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:606)

at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

at org.apache.hadoop.util.RunJar.main(RunJar.java:136)



Reproduce:

create table tlb (key int, key1 int, key2 int);

create table src (key int, value string);

select key, key1, key2 from (select a.key, 0 as key1 , 0 as key2 from tlb a inner join src b on a.key = b.key) a group by key, key1, key2;


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12886</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-11 22:09:42" id="12824" opendate="2016-01-09 01:45:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO doesnt get triggered when aggregate function is used within windowing function </summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12750</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-11 23:50:11" id="12768" opendate="2015-12-31 02:27:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Thread safety: binary sortable serde decimal deserialization</summary>
			
			
			<description>We see thread safety issues due to static decimal buffer in binary sortable serde.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-12 21:53:18" id="12687" opendate="2015-12-16 02:42:45" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP Workdirs need to default to YARN local</summary>
			
			
			<description>


   LLAP_DAEMON_WORK_DIRS(&quot;hive.llap.daemon.work.dirs&quot;, &quot;&quot;



is a bad default &amp;amp; fails at startup if not overridden.
A better default would be to fall back onto YARN local dirs if this is not configured.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11358</link>
			
			
			<link description="is related to" type="Reference">14392</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-13 21:09:32" id="12815" opendate="2016-01-08 21:49:58" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>column stats NPE for a query w/o a table</summary>
			
			
			<description>I was running something like create table as select 1;
First it logs why it cannot get stats:

2016-01-08T21:46:31,876 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: stats.StatsUtils (StatsUtils.java:getTableColumnStats(756)) - Failed to retrieve table statistics:

org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Specified database/table does not exist : _dummy_database._dummy_table)

        at org.apache.hadoop.hive.ql.metadata.Hive.getTableColumnStatistics(Hive.java:3195) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:752) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:198) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]



and returns null, then it fails with NPE:

2016-01-08T21:46:31,885 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: ql.Driver (SessionState.java:printError(1010)) - FAILED: NullPointerException null

java.lang.NullPointerException

        at org.apache.hadoop.hive.ql.stats.StatsUtils.getDataSizeFromColumnStats(StatsUtils.java:1450)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:199)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144)

        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132)

        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)

        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)

        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)

        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)

        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)

        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)

        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)



Only &quot;NullPointerException null&quot; is logged to CLI... </description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-19 02:48:09" id="12758" opendate="2015-12-29 23:04:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Parallel compilation: Operator::resetId() is not thread-safe</summary>
			
			
			<description>


  private static AtomicInteger seqId;

...



  public Operator() {

    this(String.valueOf(seqId.getAndIncrement()));

  }



  public static void resetId() {

    seqId.set(0);

  }



Potential race-condition.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TezDummyStoreOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestUpdateDeleteSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ColumnStatsWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.RCFileMergeOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiGenerateResultOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSparkPartitionPruningSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Task.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.MapWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SimpleFetchAggregation.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.QueryPlanTreeTransformation.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TerminalOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.TestGenTezWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyGenerateResultOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Context.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ConditionalTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TemporaryHashSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.spark.SparkPartitionPruningSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.UnionWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSparkHashTableSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-19 09:20:55" id="12879" opendate="2016-01-15 17:52:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>RowResolver of Semijoin not updated in CalcitePlanner</summary>
			
			
			<description>When we generate a Calcite plan, we might need to cast the column referenced by equality conditions in a Semijoin because Hive works with a more relaxed data type system.
To cast these columns, we introduce a project operators over the Semijoin inputs. However, these columns were not included in the RowResolver of the Semijoin operator (I guess because they couldn&amp;amp;apos;t be referenced beyond the Semijoin). However, if above the Semijoin a Project operator with a windowing function is generated, the RR for the project is taken from the operator below, resulting in a mismatch.
The following query can be used to reproduce the problem (with CBO on):

CREATE TABLE table_1 (int_col_1 INT, decimal3003_col_2 DECIMAL(30, 3), timestamp_col_3 TIMESTAMP, decimal0101_col_4 DECIMAL(1, 1), double_col_5 DOUBLE, boolean_col_6 BOOLEAN, timestamp_col_7 TIMESTAMP, varchar0098_col_8 VARCHAR(98), int_col_9 INT, timestamp_col_10 TIMESTAMP, decimal0903_col_11 DECIMAL(9, 3), int_col_12 INT, bigint_col_13 BIGINT, boolean_col_14 BOOLEAN, char0254_col_15 CHAR(254), boolean_col_16 BOOLEAN, smallint_col_17 SMALLINT, float_col_18 FLOAT, decimal2608_col_19 DECIMAL(26, 8), varchar0216_col_20 VARCHAR(216), string_col_21 STRING, timestamp_col_22 TIMESTAMP, double_col_23 DOUBLE, smallint_col_24 SMALLINT, float_col_25 FLOAT, decimal2016_col_26 DECIMAL(20, 16), string_col_27 STRING, decimal0202_col_28 DECIMAL(2, 2), boolean_col_29 BOOLEAN, decimal2020_col_30 DECIMAL(20, 20), float_col_31 FLOAT, boolean_col_32 BOOLEAN, varchar0148_col_33 VARCHAR(148), decimal2121_col_34 DECIMAL(21, 21), timestamp_col_35 TIMESTAMP, float_col_36 FLOAT, float_col_37 FLOAT, string_col_38 STRING, decimal3420_col_39 DECIMAL(34, 20), smallint_col_40 SMALLINT, decimal1408_col_41 DECIMAL(14, 8), string_col_42 STRING, decimal0902_col_43 DECIMAL(9, 2), varchar0204_col_44 VARCHAR(204), float_col_45 FLOAT, tinyint_col_46 TINYINT, double_col_47 DOUBLE, timestamp_col_48 TIMESTAMP, double_col_49 DOUBLE, timestamp_col_50 TIMESTAMP, decimal0704_col_51 DECIMAL(7, 4), int_col_52 INT, double_col_53 DOUBLE, int_col_54 INT, timestamp_col_55 TIMESTAMP, decimal0505_col_56 DECIMAL(5, 5), char0155_col_57 CHAR(155), double_col_58 DOUBLE, timestamp_col_59 TIMESTAMP, double_col_60 DOUBLE, float_col_61 FLOAT, char0249_col_62 CHAR(249), float_col_63 FLOAT, smallint_col_64 SMALLINT, decimal1309_col_65 DECIMAL(13, 9), timestamp_col_66 TIMESTAMP, boolean_col_67 BOOLEAN, tinyint_col_68 TINYINT, tinyint_col_69 TINYINT, double_col_70 DOUBLE, bigint_col_71 BIGINT, boolean_col_72 BOOLEAN, float_col_73 FLOAT, char0222_col_74 CHAR(222), boolean_col_75 BOOLEAN, string_col_76 STRING, decimal2612_col_77 DECIMAL(26, 12), bigint_col_78 BIGINT, char0128_col_79 CHAR(128), tinyint_col_80 TINYINT, boolean_col_81 BOOLEAN, int_col_82 INT, boolean_col_83 BOOLEAN, decimal2622_col_84 DECIMAL(26, 22), boolean_col_85 BOOLEAN, boolean_col_86 BOOLEAN, decimal0907_col_87 DECIMAL(9, 7))

STORED AS orc;

CREATE TABLE table_18 (float_col_1 FLOAT, double_col_2 DOUBLE, decimal2518_col_3 DECIMAL(25, 18), boolean_col_4 BOOLEAN, bigint_col_5 BIGINT, boolean_col_6 BOOLEAN, boolean_col_7 BOOLEAN, char0035_col_8 CHAR(35), decimal2709_col_9 DECIMAL(27, 9), timestamp_col_10 TIMESTAMP, bigint_col_11 BIGINT, decimal3604_col_12 DECIMAL(36, 4), string_col_13 STRING, timestamp_col_14 TIMESTAMP, timestamp_col_15 TIMESTAMP, decimal1911_col_16 DECIMAL(19, 11), boolean_col_17 BOOLEAN, tinyint_col_18 TINYINT, timestamp_col_19 TIMESTAMP, timestamp_col_20 TIMESTAMP, tinyint_col_21 TINYINT, float_col_22 FLOAT, timestamp_col_23 TIMESTAMP)

STORED AS orc;



explain

SELECT

    COALESCE(498,

      LEAD(COALESCE(-973, -684, 515)) OVER (

        PARTITION BY (t2.tinyint_col_21 + t1.smallint_col_24)

        ORDER BY (t2.tinyint_col_21 + t1.smallint_col_24),

        FLOOR(t1.double_col_60) DESC),

      524) AS int_col

FROM table_1 t1 INNER JOIN table_18 t2

ON (((t2.tinyint_col_18) = (t1.bigint_col_13))

    AND ((t2.decimal2709_col_9) = (t1.decimal1309_col_65)))

    AND ((t2.tinyint_col_21) = (t1.tinyint_col_46))

WHERE (t2.tinyint_col_21) IN (

        SELECT COALESCE(-92, -994) AS int_col_3

        FROM table_1 tt1 INNER JOIN table_18 tt2

        ON (tt2.decimal1911_col_16) = (tt1.decimal1309_col_65)

        WHERE (tt1.timestamp_col_66) = (tt2.timestamp_col_19));


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-19 19:12:56" id="12820" opendate="2016-01-08 23:48:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Remove the check if carriage return and new line are used for separator or escape character</summary>
			
			
			<description>The change in HIVE-11785 doesn&amp;amp;apos;t allow \r or \n to be used as separator or escape character which may break some existing tables which uses \r as separator or escape character e.g..
This case actually can be supported regardless of SERIALIZATION_ESCAPE_CRLF set or not.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is depended upon by" type="dependent">11785</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-19 20:04:46" id="12657" opendate="2015-12-11 21:42:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>selectDistinctStar.q results differ with jdk 1.7 vs jdk 1.8</summary>
			
			
			<description>Encountered this issue when analysing test failures of HIVE-12609. selectDistinctStar.q produces the following diff when I ran with java version &quot;1.7.0_55&quot; and java version &quot;1.8.0_60&quot;



&amp;lt; 128   val_128 128     

---

&amp;gt; 128           128     val_128

1770c1770

&amp;lt; 224   val_224 224     

---

&amp;gt; 224           224     val_224

1776c1776

&amp;lt; 369   val_369 369     

---

&amp;gt; 369           369     val_369

1799,1810c1799,1810

&amp;lt; 146   val_146 146     val_146 146     val_146 2008-04-08      11

&amp;lt; 150   val_150 150     val_150 150     val_150 2008-04-08      11

&amp;lt; 213   val_213 213     val_213 213     val_213 2008-04-08      11

&amp;lt; 238   val_238 238     val_238 238     val_238 2008-04-08      11

&amp;lt; 255   val_255 255     val_255 255     val_255 2008-04-08      11

&amp;lt; 273   val_273 273     val_273 273     val_273 2008-04-08      11

&amp;lt; 278   val_278 278     val_278 278     val_278 2008-04-08      11

&amp;lt; 311   val_311 311     val_311 311     val_311 2008-04-08      11

&amp;lt; 401   val_401 401     val_401 401     val_401 2008-04-08      11

&amp;lt; 406   val_406 406     val_406 406     val_406 2008-04-08      11

&amp;lt; 66    val_66  66      val_66  66      val_66  2008-04-08      11

&amp;lt; 98    val_98  98      val_98  98      val_98  2008-04-08      11

---

&amp;gt; 146   val_146 2008-04-08      11      146     val_146 146     val_146

&amp;gt; 150   val_150 2008-04-08      11      150     val_150 150     val_150

&amp;gt; 213   val_213 2008-04-08      11      213     val_213 213     val_213

&amp;gt; 238   val_238 2008-04-08      11      238     val_238 238     val_238

&amp;gt; 255   val_255 2008-04-08      11      255     val_255 255     val_255

&amp;gt; 273   val_273 2008-04-08      11      273     val_273 273     val_273

&amp;gt; 278   val_278 2008-04-08      11      278     val_278 278     val_278

&amp;gt; 311   val_311 2008-04-08      11      311     val_311 311     val_311

&amp;gt; 401   val_401 2008-04-08      11      401     val_401 401     val_401

&amp;gt; 406   val_406 2008-04-08      11      406     val_406 406     val_406

&amp;gt; 66    val_66  2008-04-08      11      66      val_66  66      val_66

&amp;gt; 98    val_98  2008-04-08      11      98      val_98  98      val_98

4212c4212


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinCondTypeCheckProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-21 18:38:50" id="12864" opendate="2016-01-13 19:17:54" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>StackOverflowError parsing queries with very large predicates</summary>
			
			
			<description>We have seen that queries with very large predicates might fail with the following stacktrace:

016-01-12 05:47:36,516|beaver.machine|INFO|552|5072|Thread-22|Exception in thread &quot;main&quot; java.lang.StackOverflowError



2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:145)



2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)



2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,634|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:37,582|main|INFO|552|4568|MainThread|TEST &quot;test_WideQuery&quot; FAILED in 10.95 seconds



The problem could be solved by reimplementing some of the parsing methods so they are iterative instead of recursive.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-22 23:47:56" id="12809" opendate="2016-01-08 02:52:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Vectorization: fast-path for coalesce if input.noNulls = true</summary>
			
			
			<description>Coalesce can skip processing other columns, if all the input columns are non-null.
Possibly retaining, isRepeating=true.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12750</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-24 11:12:51" id="12911" opendate="2016-01-22 11:24:40" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>PPD might get exercised even when flag is false if CBO is on</summary>
			
			
			<description>Introduced in HIVE-11865.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is broken by" type="Regression">11865</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-26 01:00:04" id="12797" opendate="2016-01-06 23:01:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Synchronization issues with tez/llap session pool in hs2</summary>
			
			
			<description>The changes introduced as part of HIVE-12674 causes issues while shutting down hs2 when session pools are used.



java.util.ConcurrentModificationException

        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966) ~[?:1.8.0_45]

        at java.util.LinkedList$ListItr.remove(LinkedList.java:921) ~[?:1.8.0_45]

        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:288) ~[hive-exec-2.0.0.2.3.5.0-79.jar:2.0.0.2.3.5.0-79]

        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:479) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]

        at org.apache.hive.service.server.HiveServer2$2.run(HiveServer2.java:183) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is cloned by" type="Cloners">12926</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-26 03:18:50" id="12904" opendate="2016-01-21 19:45:17" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: deadlock in task scheduling</summary>
			
			
			<description>
Thread 34107: (state = BLOCKED)

 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.isInWaitQueue() @bci=0, line=690 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.finishableStateUpdated(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=8, line=485 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1500(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService, org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=3, line=78 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.finishableStateUpdated(boolean) @bci=27, line=733 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.sourceStateUpdated(java.lang.String) @bci=76, line=210 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.sourceStateUpdated(java.lang.String) @bci=5, line=164 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.registerSourceStateChange(java.lang.String, java.lang.String, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateProto) @bci=34, line=228 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=47, line=255 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=328 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.sourceStateUpdated(com.google.protobuf.RpcController, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=105 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.RpcController, com.google.protobuf.Message) @bci=80, line=13067 (Compiled frame)

 - org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(org.apache.hadoop.ipc.RPC$Server, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=246, line=616 (Compiled frame)

 - org.apache.hadoop.ipc.RPC$Server.call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=9, line=969 (Compiled frame)

 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=38, line=2151 (Compiled frame)

 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=1, line=2147 (Compiled frame)

 - java.security.AccessController.doPrivileged(java.security.PrivilegedExceptionAction, java.security.AccessControlContext) @bci=0 (Compiled frame)

 - javax.security.auth.Subject.doAs(javax.security.auth.Subject, java.security.PrivilegedExceptionAction) @bci=42, line=422 (Compiled frame)

 - org.apache.hadoop.security.UserGroupInformation.doAs(java.security.PrivilegedExceptionAction) @bci=14, line=1657 (Compiled frame)

 - org.apache.hadoop.ipc.Server$Handler.run() @bci=315, line=2145 (Interpreted frame)





and 





Thread 34500: (state = BLOCKED)

 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.unregisterForUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=0, line=195 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.unregisterFinishableStateUpdate(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=160 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.unregisterForFinishableStateUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=143 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.maybeUnregisterForFinishedStateNotifications() @bci=20, line=681 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(org.apache.tez.runtime.task.TaskRunner2Result) @bci=32, line=548 (Compiled frame)

 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(java.lang.Object) @bci=5, line=535 (Compiled frame)

 - com.google.common.util.concurrent.Futures$4.run() @bci=55, line=1149 (Compiled frame)

 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)

 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)

 - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)



&quot;IPC Server handler 0 on 15001&quot;:

  waiting to lock Monitor@0x00007f5d322ecb08 (Object@0x00007f67032cd2c0, a org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService$TaskWrapper),

  which is held by &quot;ExecutionCompletionThread #0&quot;

&quot;ExecutionCompletionThread #0&quot;:

  waiting to lock Monitor@0x00007f6066b9e8c8 (Object@0x00007f66b6570200, a org/apache/hadoop/hive/llap/daemon/impl/QueryInfo$FinishableStateTracker),

  which is held by &quot;IPC Server handler 0 on 15001&quot;



Found a total of 1 deadlock.





Looks like it&amp;amp;apos;s caused by synchronized blocks:

TaskWrapper:

public synchronized void maybeUnregisterForFinishedStateNotifications



Eventually calls 

FinishableStateTracker

synchronized void unregisterForUpdates(FinishableStateUpdateHandler handler) {



and 

FST

 synchronized void sourceStateUpdated(String sourceName) {

   

eventually calls

 public synchronized boolean isInWaitQueue() {



The latter returns the boolean, so it definitely doesn&amp;amp;apos;t need synchronized, however I don&amp;amp;apos;t know if there are other similar issues and what is necessary inside sync blocks, perhaps there&amp;amp;apos;s a better fix.
Overall I&amp;amp;apos;d say synch methods on objects that call any other non-trivial objects should not be used. Perhaps for now it would be good to replace all sync methods by sync blocks that cover entire method, as well as remove the unnecessary ones like the isWait... one. Then the scope of the blocks can be adjusted based on logic in future.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-01-27 08:50:21" id="12478" opendate="2015-11-20 08:38:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Improve Hive/Calcite Transitive Predicate inference</summary>
			
			
			<description>HiveJoinPushTransitivePredicatesRule does not pull up predicates for transitive inference if they contain more than one column.



EXPLAIN select * from srcpart join (select ds as ds, ds as `date` from srcpart where  (ds = &amp;amp;apos;2008-04-08&amp;amp;apos; and value=1)) s on (srcpart.ds = s.ds);


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="D">org.apache.hadoop.hive.ql.optimizer.calcite.HiveVolcanoPlannerContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveCostModel.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRulesRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.ql.optimizer.calcite.HiveHepPlannerContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveVolcanoPlanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">11110</link>
			
			
			<link description="is related to" type="Reference">12543</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-27 22:49:16" id="12926" opendate="2016-01-26 00:59:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Another synchronization issue with tez/llap session pool in hs2</summary>
			
			
			<description>The changes introduced as part of HIVE-12674 causes issues while shutting down hs2 when session pools are used.



java.util.ConcurrentModificationException

        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966) ~[?:1.8.0_45]

        at java.util.LinkedList$ListItr.remove(LinkedList.java:921) ~[?:1.8.0_45]

        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:288) ~[hive-exec-2.0.0.2.3.5.0-79.jar:2.0.0.2.3.5.0-79]

        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:479) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]

        at org.apache.hive.service.server.HiveServer2$2.run(HiveServer2.java:183) [hive-jdbc-2.0.0.2.3.5.0-79-standalone.jar:2.0.0.2.3.5.0-79]


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is a clone of" type="Cloners">12797</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-01-28 19:28:09" id="12933" opendate="2016-01-26 18:51:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Beeline will hang when authenticating with PAM when libjpam.so is missing</summary>
			
			
			<description>When we setup PAM authentication, we need to have libjpam.so under java.library.path. If it happens to misplace the .so file, rather than giving an exception, the client will hang forever.
Seems we should catch the exception when the lib is missing.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.auth.PamAuthenticationProviderImpl.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-01 18:39:40" id="12945" opendate="2016-01-27 19:08:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Bucket pruning: bucketing for -ve hashcodes have historical issues</summary>
			
			
			<description>The different ETL pathways differed in reducer choice slightly for -ve hashcodes.



(hashCode &amp;amp; Integer.MAX_VALUE) % numberOfBuckets;

!=

Math.abs(hashCode) % numberOfBuckets



Add a backwards compat option, which can be used to protect against old data left over from 0.13.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-01 19:17:47" id="12931" opendate="2016-01-26 16:00:17" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Shuffle tokens stay around forever in LLAP</summary>
			
			
			<description>Shuffle tokens are never cleaned up, resulting in a slow leak.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-01 19:27:00" id="12964" opendate="2016-01-29 20:05:00" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestOperationLoggingAPIWithMr,TestOperationLoggingAPIWithTez fail on branch-2.0 (with Java 7, at least)</summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-01 21:04:01" id="12947" opendate="2016-01-27 22:23:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>SMB join in tez has ClassCastException when container reuse is on</summary>
			
			
			<description>SMB join in tez has multiple work items that are connected based on input tag followed by input initialization etc. In case of container re-use, what ends up happening is that we try to reconnect the work items and fail. If we try to work around that issue by recognizing somehow that the cache was in play, we will run into other initialization issues with respect to record readers. So the plan is to disable caching of the SMB work items by clearing out during the close phase.



java.lang.RuntimeException: Map operator initialization failed

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:247)

        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)

        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)

        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)

        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)

        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:422)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)

        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)

        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)

        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)

        at java.util.concurrent.FutureTask.run(FutureTask.java:266)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

        at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.FileSinkOperator cannot be cast to org.apache.hadoop.hive.ql.exec.DummyStoreOperator

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:300)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getJoinParentOp(MapRecordProcessor.java:302)

        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)

        ... 15 more


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.0.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-09 20:55:47" id="12993" opendate="2016-02-03 19:46:17" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>user and password supplied from URL is overwritten by the empty user and password of the JDBC connection string when it&amp;apos;s calling from beeline</summary>
			
			
			<description>When we make the call beeline -u &quot;jdbc:hive2://localhost:10000/;user=aaa;password=bbb&quot;, the user and password are overwritten by the blank ones since internally it constructs a &quot;connect &amp;lt;url&amp;gt; &amp;amp;apos;&amp;amp;apos; &amp;amp;apos;&amp;amp;apos; &amp;lt;driver&amp;gt;&quot; call with empty user and password. </description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.beeline.DatabaseConnection.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.Utils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-11 21:55:54" id="10026" opendate="2015-03-20 00:43:57" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>LLAP: AM should get notifications on daemons going down or restarting</summary>
			
			
			<description>There&amp;amp;apos;s lost state otherwise, which can cause queries to hang.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>llap</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.registry.impl.LlapYarnRegistryImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12935</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-12 18:32:02" id="13036" opendate="2016-02-10 20:38:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Split hive.root.logger separately to make it compatible with log4j1.x (for remaining services)</summary>
			
			
			<description>Similar to HIVE-12402 but for HS2 and metastore this time.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.common.cli.CommonCliOptions.java</file>
			
			
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12402</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-18 17:59:52" id="12927" opendate="2016-01-26 02:31:44" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HBase metastore: sequences should be one per row, not all in one row</summary>
			
			
			<description>
  long getNextSequence(byte[] sequence) throws IOException {



Is not safe in presence of any concurrency. It should use HBase increment API.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-22 21:52:47" id="13105" opendate="2016-02-20 01:47:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP token hashCode and equals methods are incorrect</summary>
			
			
			<description>I had wrong assumptions about object vs functional equality. This would need to go to 2.0.1 (target version field is AWOL)
&quot;Luckily&quot; the implications are spurious access denied errors, and not the other way around.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-23 00:08:53" id="13082" opendate="2016-02-18 05:22:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Enable constant propagation optimization in query with left semi join</summary>
			
			
			<description>Currently constant folding is only allowed for inner or unique join, I think it is also applicable and allowed for left semi join. Otherwise the query like following having multiple joins with left semi joins will fail:

 

select table1.id, table1.val, table2.val2 from table1 inner join table2 on table1.val = &amp;amp;apos;t1val01&amp;amp;apos; and table1.id = table2.id left semi join table3 on table1.dimid = table3.id;



with errors:



java.lang.Exception: java.lang.RuntimeException: Error in configuring object

	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) ~[hadoop-mapreduce-client-common-2.6.0.jar:?]

	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522) [hadoop-mapreduce-client-common-2.6.0.jar:?]

Caused by: java.lang.RuntimeException: Error in configuring object

	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ~[hadoop-common-2.6.0.jar:?]

	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) ~[hadoop-common-2.6.0.jar:?]

	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.6.0.jar:?]

	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446) ~[hadoop-mapreduce-client-core-2.6.0.jar:?]

	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) ~[hadoop-mapreduce-client-core-2.6.0.jar:?]

	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) ~[hadoop-mapreduce-client-common-2.6.0.jar:?]

	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[?:1.7.0_45]

	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_45]

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_45]

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_45]

	at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_45]

...

Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3

	at java.util.ArrayList.rangeCheck(ArrayList.java:635) ~[?:1.7.0_45]

	at java.util.ArrayList.get(ArrayList.java:411) ~[?:1.7.0_45]

	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.&amp;lt;init&amp;gt;(StandardStructObjectInspector.java:109) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:326) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:311) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:181) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:319) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:78) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.initializeOp(MapJoinOperator.java:138) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:355) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]

	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:504) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12477</link>
			
			
			<link description="is depended upon by" type="dependent">13164</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-02-24 05:03:40" id="13094" opendate="2016-02-19 08:33:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CBO: Assertion error  in Case expression</summary>
			
			
			<description>Triggered by a trap case in the case evaluation



CASE WHEN (-2) &amp;gt;= 0  THEN SUBSTRING(str0, 1,CAST((-2) AS INT)) ELSE NULL






Exception in thread &quot;b367ad08-d900-4672-8e75-a4e90a52141b b367ad08-d900-4672-8e75-a4e90a52141b main&quot; java.lang.AssertionError: Internal error: Cannot add expression of different type to set:

set type is RecordType(VARCHAR(2147483647) CHARACTER SET &quot;ISO-8859-1&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f0, VARCHAR(2147483647) CHARACTER SET &quot;ISO-8859-1&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f1, VARCL

expression type is RecordType(VARCHAR(2147483647) CHARACTER SET &quot;ISO-8859-1&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f0, VARCHAR(2147483647) CHARACTER SET &quot;ISO-8859-1&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $fL

set is rel#12408:HiveProject.HIVE.[](input=HepRelVertex#12407,$f0=$0,$f1=$6,$f2=CASE(&amp;gt;=(-(2), 0), substring($6, 1, -(2)), null))

expression is HiveProject#12414

        at org.apache.calcite.util.Util.newInternal(Util.java:774)

        at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:317)

        at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:57)

        at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:224)

        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(HiveReduceExpressionsRule.java:208)

        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:318)

        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:514)

        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:392)

        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:285)

        at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:72)

        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207)

        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194)

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.hepPlan(CalcitePlanner.java:1265)

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPreJoinOrderingTransforms(CalcitePlanner.java:1125)

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:938)

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:878)

        at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:113)

        at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:969)


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-24 18:06:15" id="13126" opendate="2016-02-23 19:03:03" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Clean up MapJoinOperator properly to avoid object cache reuse with unintentional states</summary>
			
			
			<description>For a given job, one task may reuse other task&amp;amp;apos;s object cache (plan cache) such as MapJoinOperator. This is fine. But if we have some dirty states left over, it may cause issue like wrong results.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-02-26 21:20:03" id="12935" opendate="2016-01-26 22:14:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: Replace Yarn registry with Zookeeper registry</summary>
			
			
			<description>Existing YARN registry service for cluster membership has to depend on refresh intervals to get the list of instances/daemons that are running in the cluster. Better approach would be replace it with zookeeper based registry service so that custom listeners can be added to update healthiness of daemons in the cluster.  </description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
			
			
			<file type="D">org.apache.hadoop.hive.llap.registry.impl.LlapYarnRegistryImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">10026</link>
			
			
			<link description="supercedes" type="Supercedes">10004</link>
			
			
			<link description="supercedes" type="Supercedes">10648</link>
			
			
			<link description="is depended upon by" type="dependent">13168</link>
			
			
			<link description="is depended upon by" type="dependent">13167</link>
			
			
			<link description="is depended upon by" type="dependent">12959</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-01 22:45:32" id="13174" opendate="2016-02-26 23:34:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Remove Vectorizer noise in logs</summary>
			
			
			<description>If you have a table with a bin column you&amp;amp;apos;re hs2/client logs are full of the stack traces below. These should either be made debug or we just log the message not the trace.



2015-10-12 12:34:23,922 INFO  [main]: physical.Vectorizer (Vectorizer.java:validateExprNodeDesc(1249)) - Failed to vectorize

org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary

	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)

	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)

	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)

	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)

	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)

	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)

	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)

	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)

	at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)

	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)

	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)

	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)

	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)

	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)

	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)

	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)

	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)

	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)

	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)

	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)

	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)

	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)

	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)

	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)

	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)

	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)

	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.lang.reflect.Method.invoke(Method.java:606)

	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)

	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-03 06:54:36" id="13163" opendate="2016-02-26 01:26:33" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ORC MemoryManager thread checks are fatal, should WARN </summary>
			
			
			<description>The MemoryManager is tied to a WriterOptions on create, which can occur in a different thread from the writer calls.
This is unexpected, but safe and needs a warning not a fatal.



  /**

   * Light weight thread-safety check for multi-threaded access patterns

   */

  private void checkOwner() {

    Preconditions.checkArgument(ownerLock.isHeldByCurrentThread(),

        &quot;Owner thread expected %s, got %s&quot;,

        ownerLock.getOwner(),

        Thread.currentThread());

  }



</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.orc.impl.MemoryManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-03 20:51:59" id="13186" opendate="2016-02-29 22:53:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ALTER TABLE RENAME should lowercase table name and hdfs location</summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-07 08:49:16" id="13096" opendate="2016-02-19 14:41:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Cost to choose side table in MapJoin conversion based on cumulative cardinality</summary>
			
			
			<description>HIVE-11954 changed the logic to choose the side table in the MapJoin conversion algorithm. Initial heuristic for the cost was based on number of heavyweight operators.
This extends that work so the heuristic is based on accumulate cardinality. In the future, we should choose the side based on total latency for the input.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">14307</link>
			
			
			<link description="supercedes" type="Supercedes">11954</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-10 22:16:08" id="13175" opendate="2016-02-27 01:26:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Disallow making external tables transactional</summary>
			
			
			<description>The fact that compactor rewrites contents of ACID tables is in conflict with what is expected of external tables.
Conversely, end user can write to External table which certainly not what is expected of ACID table.
So we should explicitly disallow making an external table ACID.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-11 03:43:13" id="13236" opendate="2016-03-08 21:51:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: token renewal interval needs to be set</summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.security.SecretManager.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">12659</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-14 21:47:53" id="13201" opendate="2016-03-03 19:22:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Compaction shouldn&amp;apos;t be allowed on non-ACID table</summary>
			
			
			<description>Looks like compaction is allowed on non-ACID table, although that&amp;amp;apos;s of no sense and does nothing. Moreover the compaction request will be enqueued into COMPACTION_QUEUE metastore table, which brings unnecessary overhead.
We should prevent compaction commands being allowed on non-ACID tables.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.ErrorMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-16 17:08:48" id="13260" opendate="2016-03-10 21:12:33" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ReduceSinkDeDuplication throws exception when pRS key is empty</summary>
			
			
			<description>Steps to reproduce:



set hive.mapred.mode=nonstrict;

set hive.cbo.enable=false;



set hive.map.aggr=false;



set hive.groupby.skewindata=false;

set mapred.reduce.tasks=31;



select compute_stats(a,16),compute_stats(b,16),compute_stats(c,16),compute_stats(d,16)

from

(

select

  avg(DISTINCT substr(src.value,5)) as a,

  max(substr(src.value,5)) as b,

  variance(substr(src.value,5)) as c,

  var_samp(substr(src.value,5)) as d

 from src)subq;


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">11160</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-19 16:21:47" id="13299" opendate="2016-03-17 18:26:45" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Column Names trimmed of leading and trailing spaces</summary>
			
			
			<description>PROBLEM:
As per the Hive Language DDL: 
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL
In Hive 0.12 and earlier, only alphanumeric and underscore characters are allowed in table and column names.
In Hive 0.13 and later, column names can contain any Unicode character (see HIVE-6013). Any column name that is specified within backticks (`) is treated literally.
However column names



` left` resulted in `left`

` middle ` resulted in `middle`

`right ` resulted in `right`

`middle space` resulted in `middle space`

` middle space ` resulted in `middle space`


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">13618</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-20 17:54:16" id="13125" opendate="2016-02-23 15:29:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Support masking and filtering of rows/columns</summary>
			
			
			<description>Traditionally, access control at the row and column level is implemented through views. Using views as an access control method works well only when access rules, restrictions, and conditions are monolithic and simple. It however becomes ineffective when view definitions become too complex because of the complexity and granularity of privacy and security policies. It also becomes costly when a large number of views must be manually updated and maintained. In addition, the ability to update views proves to be challenging. As privacy and security policies evolve, required updates to views may negatively affect the security logic particularly when database applications reference the views directly by name. HIVE row and column access control helps resolve all these problems.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>New Feature</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizationValidator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.DummyHiveAuthorizationValidator.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="Dependent" type="Dependent">895</link>
			
			
			<link description="is duplicated by" type="Duplicate">4046</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-22 21:05:57" id="13286" opendate="2016-03-15 01:06:33" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Query ID is being reused across queries</summary>
			
			
			<description>Aihua Xu I see this commit made via HIVE-11488. I see that query id is being reused across queries. This defeats the purpose of a query id. I am not sure what the purpose of the change in that jira is but it breaks the assumption about a query id being unique for each query. Please take a look into this at the earliest.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.cli.CliDriver.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.TestHiveSessionImpl.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">13329</link>
			
			
			<link description="is required by" type="Required">11488</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-24 18:11:09" id="13300" opendate="2016-03-17 20:37:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive on spark throws exception for multi-insert with join</summary>
			
			
			<description>For certain multi-insert queries, Hive on Spark throws a deserialization error.

create table status_updates(userid int,status string,ds string);

create table profiles(userid int,school string,gender int);

drop table school_summary; create table school_summary(school string,cnt int) partitioned by (ds string);

drop table gender_summary; create table gender_summary(gender int,cnt int) partitioned by (ds string);



insert into status_updates values (1, &quot;status_1&quot;, &quot;2016-03-16&quot;);

insert into profiles values (1, &quot;school_1&quot;, 0);



set hive.auto.convert.join=false;

set hive.execution.engine=spark;



FROM (SELECT a.status, b.school, b.gender

FROM status_updates a JOIN profiles b

ON (a.userid = b.userid and

a.ds=&amp;amp;apos;2009-03-20&amp;amp;apos; )

) subq1

INSERT OVERWRITE TABLE gender_summary

PARTITION(ds=&amp;amp;apos;2009-03-20&amp;amp;apos;)

SELECT subq1.gender, COUNT(1) GROUP BY subq1.gender

INSERT OVERWRITE TABLE school_summary

PARTITION(ds=&amp;amp;apos;2009-03-20&amp;amp;apos;)

SELECT subq1.school, COUNT(1) GROUP BY subq1.school



Error:

16/03/17 13:29:00 [task-result-getter-3]: WARN scheduler.TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3, localhost): java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error: Unable to deserialize reduce input key from x1x128x0x0 with properties {serialization.sort.order.null=a, columns=reducesinkkey0, serialization.lib=org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe, serialization.sort.order=+, columns.types=int}

	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.processRow(SparkReduceRecordHandler.java:279)

	at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.processNextRecord(HiveReduceFunctionResultList.java:49)

	at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.processNextRecord(HiveReduceFunctionResultList.java:28)

	at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:95)

	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)

	at org.apache.spark.scheduler.Task.run(Task.scala:89)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:724)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error: Unable to deserialize reduce input key from x1x128x0x0 with properties {serialization.sort.order.null=a, columns=reducesinkkey0, serialization.lib=org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe, serialization.sort.order=+, columns.types=int}

	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.processRow(SparkReduceRecordHandler.java:251)

	... 12 more

Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.io.EOFException

	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(BinarySortableSerDe.java:241)

	at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.processRow(SparkReduceRecordHandler.java:249)

	... 12 more

Caused by: java.io.EOFException

	at org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.read(InputByteBuffer.java:54)

	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserializeInt(BinarySortableSerDe.java:597)

	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(BinarySortableSerDe.java:288)

	at org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(BinarySortableSerDe.java:237)

	... 13 more


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-25 05:39:28" id="13151" opendate="2016-02-24 23:04:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Clean up UGI objects in FileSystem cache for transactions</summary>
			
			
			<description>One issue with FileSystem.CACHE is that it does not clean itself. The key in that cache includes UGI object. When new UGI objects are created and used with the FileSystem api, new entries get added to the cache.
We need to manually clean up those UGI objects once they are no longer in use.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-03-29 20:03:50" id="13326" opendate="2016-03-21 23:18:56" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HiveServer2: Make ZK config publishing configurable</summary>
			
			
			<description>We should revert to older behaviour when config publishing is disabled.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.authorization.TestHS2AuthzContext.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.java</file>
			
			
			<file type="M">org.apache.hive.service.server.HiveServer2.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">11581</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-03-31 02:38:00" id="12569" opendate="2015-12-02 19:05:47" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Excessive console message from SparkClientImpl [Spark Branch]</summary>
			
			
			<description>


15/12/02 11:00:46 INFO client.SparkClientImpl: 15/12/02 11:00:46 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)

15/12/02 11:00:47 INFO client.SparkClientImpl: 15/12/02 11:00:47 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)

15/12/02 11:00:48 INFO client.SparkClientImpl: 15/12/02 11:00:48 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)

15/12/02 11:00:49 INFO client.SparkClientImpl: 15/12/02 11:00:49 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)

15/12/02 11:00:50 INFO client.SparkClientImpl: 15/12/02 11:00:50 INFO Client: Application report for application_1442517343449_0038 (state: RUNNING)



I see this using Hive CLI after a spark job is launched and it goes non-stopping even if the job is finished.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13376</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-01 18:49:52" id="13376" opendate="2016-03-29 17:42:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HoS emits too many logs with application state</summary>
			
			
			<description>The logs get flooded with something like:
&amp;gt; Mar 28, 3:12:21.851 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)
&amp;gt; Mar 28, 3:12:21.912 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:21 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)
&amp;gt; Mar 28, 3:12:22.853 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)
&amp;gt; Mar 28, 3:12:22.913 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:22 INFO yarn.Client: Application report for application_1458679386200_0149 (state: RUNNING)
&amp;gt; Mar 28, 3:12:23.855 PM        INFO    org.apache.hive.spark.client.SparkClientImpl
&amp;gt; [stderr-redir-1]: 16/03/28 15:12:23 INFO yarn.Client: Application report for application_1458679386200_0161 (state: RUNNING)
While this is good information, it is a bit much.
Seems like SparkJobMonitor hard-codes its interval to 1 second.  It should be higher and perhaps made configurable.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">12569</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-08 00:10:16" id="13333" opendate="2016-03-23 00:34:22" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>StatsOptimizer throws ClassCastException</summary>
			
			
			<description>mvn test -Dtest=TestCliDriver -Dtest.output.overwrite=true -Dqfile=cbo_rp_udf_udaf.q -Dhive.compute.query.using.stats=true repros the issue.
In StatsOptimizer with return path on, we may have aggr($f0), aggr($f1) in GBY
and then select aggr($f1), aggr($f0) in SEL.
Thus we need to use colExp to find out which position is
corresponding to which position.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-08 18:53:56" id="13428" opendate="2016-04-05 20:53:49" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ZK SM in LLAP should have unique paths per cluster</summary>
			
			
			<description>Noticed this while working on some other patch</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.llap.security.SecretManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-10 00:18:32" id="13405" opendate="2016-04-01 17:16:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix Connection Leak in OrcRawRecordMerger</summary>
			
			
			<description>In OrcRawRecordMerger.getLastFlushLength, if the opened stream throws an IOException on .available() or on .readLong(), the function will exit without closing the stream.
This patch adds a try-with-resources to fix this.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-13 18:34:34" id="13410" opendate="2016-04-02 01:22:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>PerfLog metrics scopes not closed if there are exceptions on HS2</summary>
			
			
			<description>If there are errors, the HS2 PerfLog api scopes are not closed.  Then there are sometimes messages like &amp;amp;apos;java.io.IOException: Scope named api_parse is not closed, cannot be opened.&amp;amp;apos;
I had simply forgetting to close the dangling scopes if there is an exception.  Doing so now.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14521</link>
			
			
			<link description="relates to" type="Reference">14521</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-04-18 21:09:07" id="13523" opendate="2016-04-14 23:07:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix connection leak in ORC RecordReader and refactor for unit testing</summary>
			
			
			<description>In RecordReaderImpl, a MetadataReaderImpl object was being created (opening a file), but never closed, causing a leak. This change closes the Metadata object in RecordReaderImpl, and does substantial refactoring to make RecordReaderImpl testable:

Created DataReaderFactory and MetadataReaderFactory (plus default implementations) so that the create() methods can be mocked to verify that the objects are actually closed in RecordReaderImpl.close()
Created MetadataReaderProperties and DataReaderProperties to clean up argument lists, making code more readable
Created a builder() for RecordReaderImpl to make the code more readable
DataReader and MetadataReader now extend closeable (there was no reason for them not to in the first place) so I can use the guava Closer interface: http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/io/Closer.html
Use the Closer interface to guarantee that regardless of if either close() call fails, both will be attempted (preventing further potential leaks)
Create builders for MetadataReaderProperties, DataReaderProperties, and RecordReaderImpl to help with code readability

</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.orc.DataReader.java</file>
			
			
			<file type="M">org.apache.orc.impl.MetadataReader.java</file>
			
			
			<file type="M">org.apache.orc.impl.MetadataReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.io.orc.ReaderImpl.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-24 00:01:22" id="13553" opendate="2016-04-20 01:04:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CTE with upperCase alias throws exception</summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-24 23:59:34" id="13570" opendate="2016-04-21 00:51:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Some queries with Union all fail when CBO is off</summary>
			
			
			<description>Some queries with union all throws IndexOutOfBoundsException
when:
set hive.cbo.enable=false;
set hive.ppd.remove.duplicatefilters=true;
The stack is as:

java.lang.IndexOutOfBoundsException: Index: 67, Size: 67 

        at java.util.ArrayList.rangeCheck(ArrayList.java:635) 

        at java.util.ArrayList.get(ArrayList.java:411) 

        at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.genColLists(ColumnPrunerProcCtx.java:161) 

        at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.handleFilterUnionChildren(ColumnPrunerProcCtx.java:273) 

        at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory$ColumnPrunerFilterProc.process(ColumnPrunerProcFactory.java:108) 

        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) 

        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94) 

        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78) 

        at org.apache.hadoop.hive.ql.optimizer.ColumnPruner$ColumnPrunerWalker.walk(ColumnPruner.java:172) 

        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109) 

        at org.apache.hadoop.hive.ql.optimizer.ColumnPruner.transform(ColumnPruner.java:135) 

        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:198) 

        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10327) 

        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:192) 

        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222) 

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:432) 

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305) 

        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1119) 

        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1167) 

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055) 

        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045) 

        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207) 

        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159) 

        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370) 

        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305) 

        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:403) 

        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:419) 

        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:708) 

        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675) 

        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615) 


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-04-26 22:38:21" id="13463" opendate="2016-04-08 20:04:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Fix ImportSemanticAnalyzer to allow for different src/dst filesystems</summary>
			
			
			<description>In ImportSemanticAnalyzer, there is an assumption that the src filesystem for import and the final location are on the same filesystem. Therefore the check for emptiness and getExternalTmpLocation will be looking on the wrong filesystem and will cause an error. The output path should be fed into getExternalTmpLocation to get a temporary file on the correct filesystem. The check for emptiness should use the output filesystem.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0, 2.0.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-02 01:20:25" id="13512" opendate="2016-04-14 02:54:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Make initializing dag ids in TezWork thread safe for parallel compilation</summary>
			
			
			<description>When parallel query compilation is enabled, it is possible for concurrent running threads to create TezWork objects that have the same dag id. This is because the counter used to obtain the next dag id is not thread safe. The counter should be an AtomicInteger rather than an int.



  private static int counter;

  ...

  public TezWork(String queryId, Configuration conf) {

    this.dagId = queryId + &quot;:&quot; + (++counter);

    ...

  }


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.TezWork.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-05 21:37:15" id="13619" opendate="2016-04-26 22:48:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Bucket map join plan is incorrect</summary>
			
			
			<description>Same as HIVE-12992. Missed a single line check. TPCDS query 4 with bucketing can produce this issue.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">12992</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-06 20:56:17" id="13542" opendate="2016-04-18 23:10:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Missing stats for tables in TPCDS performance regression suite</summary>
			
			
			<description>These are the tables whose stats are missing in data/files/tpcds-perf/metastore_export/csv/TAB_COL_STATS.txt:

catalog_returns
catalog_sales
inventory
store_returns
store_sales
web_returns
web_sales

Thanks to Jesus Camacho Rodriguez for discovering this issue.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">13269</link>
			
			
			<link description="relates to" type="Reference">13601</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-09 05:42:28" id="13618" opendate="2016-04-26 22:43:51" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Trailing spaces in partition column will be treated differently</summary>
			
			
			<description>We store the partition spec value in the metastore. In mysql (and derby i think), the trailing space is ignored. That is, if you have a partition column &quot;col&quot; (type varchar or string) with value &quot;a &quot; and then select from the table where col = &quot;a&quot;, it will return. However, in postgres and Oracle, the trailing space is not ignored. </description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.ObjectStore.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">13608</link>
			
			
			<link description="is related to" type="Reference">13299</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-11 18:21:43" id="12996" opendate="2016-02-03 23:08:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Temp tables shouldn&amp;apos;t be locked</summary>
			
			
			<description>Internally, INSERT INTO ... VALUES statements use temp table to accomplish its functionality. But temp tables shouldn&amp;amp;apos;t be stored in the metastore tables for ACID, because they are by definition only visible inside the session that created them, and we don&amp;amp;apos;t allow multiple threads inside a session. If a temp table is used in a query, it should be ignored by lock manager.



mysql&amp;gt; select * from COMPLETED_TXN_COMPONENTS;

+-----------+--------------+-----------------------+------------------+

| CTC_TXNID | CTC_DATABASE | CTC_TABLE             | CTC_PARTITION    |

+-----------+--------------+-----------------------+------------------+

|         1 | acid         | t1                    | NULL             |

|         1 | acid         | values__tmp__table__1 | NULL             |

|         2 | acid         | t1                    | NULL             |

|         2 | acid         | values__tmp__table__2 | NULL             |

|         3 | acid         | values__tmp__table__3 | NULL             |

|         3 | acid         | t1                    | NULL             |

|         4 | acid         | values__tmp__table__1 | NULL             |

|         4 | acid         | t2p                   | ds=today         |

|         5 | acid         | values__tmp__table__1 | NULL             |

|         5 | acid         | t3p                   | ds=today/hour=12 |

+-----------+--------------+-----------------------+------------------+


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">10632</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-17 08:17:23" id="13293" opendate="2016-03-16 06:17:44" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Query occurs performance degradation after enabling parallel order by for Hive on Spark</summary>
			
			
			<description>I use TPCx-BB to do some performance test on Hive on Spark engine. And found query 10 has performance degradation when enabling parallel order by.
It seems that sampling cost much time before running the real query.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-17 20:43:33" id="13608" opendate="2016-04-25 21:12:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>We should provide better error message while constraints with duplicate names are created</summary>
			
			
			<description>


PREHOOK: query: create table t1(x int, constraint pk1 primary key (x) disable novalidate)

PREHOOK: type: CREATETABLE

PREHOOK: Output: database:default

PREHOOK: Output: default@t1

POSTHOOK: query: create table t1(x int, constraint pk1 primary key (x) disable novalidate)

POSTHOOK: type: CREATETABLE

POSTHOOK: Output: database:default

POSTHOOK: Output: default@t1

PREHOOK: query: create table t2(x int, constraint pk1 primary key (x) disable novalidate)

PREHOOK: type: CREATETABLE

PREHOOK: Output: database:default

PREHOOK: Output: default@t2

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don&amp;amp;apos;t support retries at the client level.)



In the above case, it seems like useful error message is lost. It looks like a  generic problem with metastore server/client exception handling and message propagation. Seems like exception parsing logic of RetryingMetaStoreClient::invoke() needs to be updated.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">13618</link>
			
			
			<link description="is duplicated by" type="Duplicate">11918</link>
			
			
			<link description="is related to" type="Reference">13290</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-05-21 01:16:14" id="13699" opendate="2016-05-05 22:14:30" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Make JavaDataModel#get thread safe for parallel compilation</summary>
			
			
			<description>The class JavaDataModel has a static method, #get, that is not thread safe. This may be an issue when parallel query compilation is enabled because two threads may attempt to call JavaDataModel#get at the same time, etc.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.util.JavaDataModel.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-23 23:52:03" id="12643" opendate="2015-12-10 01:23:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>For self describing InputFormat don&amp;apos;t replicate schema information in partitions</summary>
			
			
			<description>Since self describing Input Formats don&amp;amp;apos;t use individual partition schemas for schema resolution, there is no need to send that info to tasks.
Doing this should cut down plan size.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-31 05:31:42" id="13837" opendate="2016-05-24 23:13:33" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>current_timestamp() output format is different in some cases</summary>
			
			
			<description>As Jason Dere reports:



current_timestamp() udf returns result with different format in some cases.



select current_timestamp() returns result with decimal precision:

{noformat}

hive&amp;gt; select current_timestamp();

OK

2016-04-14 18:26:58.875

Time taken: 0.077 seconds, Fetched: 1 row(s)

{noformat}



But output format is different for select current_timestamp() from all100k union select current_timestamp() from over100k limit 5; 

{noformat}

hive&amp;gt; select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;

Query ID = hrt_qa_20160414182956_c4ed48f2-9913-4b3b-8f09-668ebf55b3e3

Total jobs = 1

Launching Job 1 out of 1

Tez session was closed. Reopening...

Session re-established.





Status: Running (Executing on YARN cluster with App id application_1460611908643_0624)



----------------------------------------------------------------------------------------------

        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED  

----------------------------------------------------------------------------------------------

Map 1 ..........      llap     SUCCEEDED      1          1        0        0       0       0  

Map 4 ..........      llap     SUCCEEDED      1          1        0        0       0       0  

Reducer 3 ......      llap     SUCCEEDED      1          1        0        0       0       0  

----------------------------------------------------------------------------------------------

VERTICES: 03/03  [==========================&amp;gt;&amp;gt;] 100%  ELAPSED TIME: 0.92 s     

----------------------------------------------------------------------------------------------

OK

2016-04-14 18:29:56

Time taken: 10.558 seconds, Fetched: 1 row(s)

{noformat}



explain plan for select current_timestamp();

{noformat}

hive&amp;gt; explain extended select current_timestamp();

OK

ABSTRACT SYNTAX TREE:

  

TOK_QUERY

   TOK_INSERT

      TOK_DESTINATION

         TOK_DIR

            TOK_TMP_FILE

      TOK_SELECT

         TOK_SELEXPR

            TOK_FUNCTION

               current_timestamp





STAGE DEPENDENCIES:

  Stage-0 is a root stage



STAGE PLANS:

  Stage: Stage-0

    Fetch Operator

      limit: -1

      Processor Tree:

        TableScan

          alias: _dummy_table

          Row Limit Per Split: 1

          GatherStats: false

          Select Operator

            expressions: 2016-04-14 18:30:57.206 (type: timestamp)

            outputColumnNames: _col0

            ListSink



Time taken: 0.062 seconds, Fetched: 30 row(s)

{noformat}



explain plan for select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;

{noformat}

hive&amp;gt; explain extended select current_timestamp() from all100k union select current_timestamp() from over100k limit 5;

OK

ABSTRACT SYNTAX TREE:

  

TOK_QUERY

   TOK_FROM

      TOK_SUBQUERY

         TOK_QUERY

            TOK_FROM

               TOK_SUBQUERY

                  TOK_UNIONALL

                     TOK_QUERY

                        TOK_FROM

                           TOK_TABREF

                              TOK_TABNAME

                                 all100k

                        TOK_INSERT

                           TOK_DESTINATION

                              TOK_DIR

                                 TOK_TMP_FILE

                           TOK_SELECT

                              TOK_SELEXPR

                                 TOK_FUNCTION

                                    current_timestamp

                     TOK_QUERY

                        TOK_FROM

                           TOK_TABREF

                              TOK_TABNAME

                                 over100k

                        TOK_INSERT

                           TOK_DESTINATION

                              TOK_DIR

                                 TOK_TMP_FILE

                           TOK_SELECT

                              TOK_SELEXPR

                                 TOK_FUNCTION

                                    current_timestamp

                  _u1

            TOK_INSERT

               TOK_DESTINATION

                  TOK_DIR

                     TOK_TMP_FILE

               TOK_SELECTDI

                  TOK_SELEXPR

                     TOK_ALLCOLREF

         _u2

   TOK_INSERT

      TOK_DESTINATION

         TOK_DIR

            TOK_TMP_FILE

      TOK_SELECT

         TOK_SELEXPR

            TOK_ALLCOLREF

      TOK_LIMIT

         5





STAGE DEPENDENCIES:

  Stage-1 is a root stage

  Stage-0 depends on stages: Stage-1



STAGE PLANS:

  Stage: Stage-1

    Tez

      DagId: hrt_qa_20160414183119_ec8e109e-8975-4799-a142-4a2289f85910:7

      Edges:

        Map 1 &amp;lt;- Union 2 (CONTAINS)

        Map 4 &amp;lt;- Union 2 (CONTAINS)

        Reducer 3 &amp;lt;- Union 2 (SIMPLE_EDGE)

      DagName: 

      Vertices:

        Map 1 

            Map Operator Tree:

                TableScan

                  alias: all100k

                  Statistics: Num rows: 100000 Data size: 15801336 Basic stats: COMPLETE Column stats: COMPLETE

                  GatherStats: false

                  Select Operator

                    Statistics: Num rows: 100000 Data size: 4000000 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: 2016-04-14 18:31:19.0 (type: timestamp)

                      outputColumnNames: _col0

                      Statistics: Num rows: 200000 Data size: 8000000 Basic stats: COMPLETE Column stats: COMPLETE

                      Group By Operator

                        keys: _col0 (type: timestamp)

                        mode: hash

                        outputColumnNames: _col0

                        Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col0 (type: timestamp)

                          null sort order: a

                          sort order: +

                          Map-reduce partition columns: _col0 (type: timestamp)

                          Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE

                          tag: -1

                          TopN: 5

                          TopN Hash Memory Usage: 0.04

                          auto parallelism: true

            Execution mode: llap

            LLAP IO: no inputs

            Path -&amp;gt; Alias:

              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k [all100k]

            Path -&amp;gt; Partition:

              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k 

                Partition

                  base file name: all100k

                  input format: org.apache.hadoop.mapred.TextInputFormat

                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                  properties:

                    COLUMN_STATS_ACCURATE {&quot;BASIC_STATS&quot;:&quot;true&quot;,&quot;COLUMN_STATS&quot;:{&quot;t&quot;:&quot;true&quot;,&quot;si&quot;:&quot;true&quot;,&quot;i&quot;:&quot;true&quot;,&quot;b&quot;:&quot;true&quot;,&quot;f&quot;:&quot;true&quot;,&quot;d&quot;:&quot;true&quot;,&quot;s&quot;:&quot;true&quot;,&quot;dc&quot;:&quot;true&quot;,&quot;bo&quot;:&quot;true&quot;,&quot;v&quot;:&quot;true&quot;,&quot;c&quot;:&quot;true&quot;,&quot;ts&quot;:&quot;true&quot;}}

                    EXTERNAL TRUE

                    bucket_count -1

                    columns t,si,i,b,f,d,s,dc,bo,v,c,ts,dt

                    columns.comments 

                    columns.types tinyint:smallint:int:bigint:float:double:string:decimal(38,18):boolean:varchar(25):char(25):timestamp:date

                    field.delim |

                    file.inputformat org.apache.hadoop.mapred.TextInputFormat

                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                    location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k

                    name default.all100k

                    numFiles 1

                    numRows 100000

                    rawDataSize 15801336

                    serialization.ddl struct all100k { byte t, i16 si, i32 i, i64 b, float f, double d, string s, decimal(38,18) dc, bool bo, varchar(25) v, char(25) c, timestamp ts, date dt}

                    serialization.format |

                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                    totalSize 15901336

                    transient_lastDdlTime 1460612683

                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                

                    input format: org.apache.hadoop.mapred.TextInputFormat

                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                    properties:

                      COLUMN_STATS_ACCURATE {&quot;BASIC_STATS&quot;:&quot;true&quot;,&quot;COLUMN_STATS&quot;:{&quot;t&quot;:&quot;true&quot;,&quot;si&quot;:&quot;true&quot;,&quot;i&quot;:&quot;true&quot;,&quot;b&quot;:&quot;true&quot;,&quot;f&quot;:&quot;true&quot;,&quot;d&quot;:&quot;true&quot;,&quot;s&quot;:&quot;true&quot;,&quot;dc&quot;:&quot;true&quot;,&quot;bo&quot;:&quot;true&quot;,&quot;v&quot;:&quot;true&quot;,&quot;c&quot;:&quot;true&quot;,&quot;ts&quot;:&quot;true&quot;}}

                      EXTERNAL TRUE

                      bucket_count -1

                      columns t,si,i,b,f,d,s,dc,bo,v,c,ts,dt

                      columns.comments 

                      columns.types tinyint:smallint:int:bigint:float:double:string:decimal(38,18):boolean:varchar(25):char(25):timestamp:date

                      field.delim |

                      file.inputformat org.apache.hadoop.mapred.TextInputFormat

                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                      location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k

                      name default.all100k

                      numFiles 1

                      numRows 100000

                      rawDataSize 15801336

                      serialization.ddl struct all100k { byte t, i16 si, i32 i, i64 b, float f, double d, string s, decimal(38,18) dc, bool bo, varchar(25) v, char(25) c, timestamp ts, date dt}

                      serialization.format |

                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                      totalSize 15901336

                      transient_lastDdlTime 1460612683

                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                    name: default.all100k

                  name: default.all100k

            Truncated Path -&amp;gt; Alias:

              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/all100k [all100k]

        Map 4 

            Map Operator Tree:

                TableScan

                  alias: over100k

                  Statistics: Num rows: 100000 Data size: 6631229 Basic stats: COMPLETE Column stats: COMPLETE

                  GatherStats: false

                  Select Operator

                    Statistics: Num rows: 100000 Data size: 4000000 Basic stats: COMPLETE Column stats: COMPLETE

                    Select Operator

                      expressions: 2016-04-14 18:31:19.0 (type: timestamp)

                      outputColumnNames: _col0

                      Statistics: Num rows: 200000 Data size: 8000000 Basic stats: COMPLETE Column stats: COMPLETE

                      Group By Operator

                        keys: _col0 (type: timestamp)

                        mode: hash

                        outputColumnNames: _col0

                        Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE

                        Reduce Output Operator

                          key expressions: _col0 (type: timestamp)

                          null sort order: a

                          sort order: +

                          Map-reduce partition columns: _col0 (type: timestamp)

                          Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE

                          tag: -1

                          TopN: 5

                          TopN Hash Memory Usage: 0.04

                          auto parallelism: true

            Execution mode: llap

            LLAP IO: no inputs

            Path -&amp;gt; Alias:

              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k [over100k]

            Path -&amp;gt; Partition:

              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k 

                Partition

                  base file name: over100k

                  input format: org.apache.hadoop.mapred.TextInputFormat

                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                  properties:

                    COLUMN_STATS_ACCURATE {&quot;BASIC_STATS&quot;:&quot;true&quot;,&quot;COLUMN_STATS&quot;:{&quot;t&quot;:&quot;true&quot;,&quot;si&quot;:&quot;true&quot;,&quot;i&quot;:&quot;true&quot;,&quot;b&quot;:&quot;true&quot;,&quot;f&quot;:&quot;true&quot;,&quot;d&quot;:&quot;true&quot;,&quot;bo&quot;:&quot;true&quot;,&quot;s&quot;:&quot;true&quot;,&quot;bin&quot;:&quot;true&quot;}}

                    EXTERNAL TRUE

                    bucket_count -1

                    columns t,si,i,b,f,d,bo,s,bin

                    columns.comments 

                    columns.types tinyint:smallint:int:bigint:float:double:boolean:string:binary

                    field.delim :

                    file.inputformat org.apache.hadoop.mapred.TextInputFormat

                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                    location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k

                    name default.over100k

                    numFiles 1

                    numRows 100000

                    rawDataSize 6631229

                    serialization.ddl struct over100k { byte t, i16 si, i32 i, i64 b, float f, double d, bool bo, string s, binary bin}

                    serialization.format :

                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                    totalSize 6731229

                    transient_lastDdlTime 1460612798

                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                

                    input format: org.apache.hadoop.mapred.TextInputFormat

                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                    properties:

                      COLUMN_STATS_ACCURATE {&quot;BASIC_STATS&quot;:&quot;true&quot;,&quot;COLUMN_STATS&quot;:{&quot;t&quot;:&quot;true&quot;,&quot;si&quot;:&quot;true&quot;,&quot;i&quot;:&quot;true&quot;,&quot;b&quot;:&quot;true&quot;,&quot;f&quot;:&quot;true&quot;,&quot;d&quot;:&quot;true&quot;,&quot;bo&quot;:&quot;true&quot;,&quot;s&quot;:&quot;true&quot;,&quot;bin&quot;:&quot;true&quot;}}

                      EXTERNAL TRUE

                      bucket_count -1

                      columns t,si,i,b,f,d,bo,s,bin

                      columns.comments 

                      columns.types tinyint:smallint:int:bigint:float:double:boolean:string:binary

                      field.delim :

                      file.inputformat org.apache.hadoop.mapred.TextInputFormat

                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

                      location hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k

                      name default.over100k

                      numFiles 1

                      numRows 100000

                      rawDataSize 6631229

                      serialization.ddl struct over100k { byte t, i16 si, i32 i, i64 b, float f, double d, bool bo, string s, binary bin}

                      serialization.format :

                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                      totalSize 6731229

                      transient_lastDdlTime 1460612798

                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                    name: default.over100k

                  name: default.over100k

            Truncated Path -&amp;gt; Alias:

              hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/user/hcat/tests/data/over100k [over100k]

        Reducer 3 

            Execution mode: vectorized, llap

            Needs Tagging: false

            Reduce Operator Tree:

              Group By Operator

                keys: KEY._col0 (type: timestamp)

                mode: mergepartial

                outputColumnNames: _col0

                Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE

                Limit

                  Number of rows: 5

                  Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE

                  File Output Operator

                    compressed: false

                    GlobalTableId: 0

                    directory: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/tmp/hive/hrt_qa/ec0773d7-0ac2-45c7-b9cb-568bbed2c49c/hive_2016-04-14_18-31-19_532_3480081382837900888-1/-mr-10001/.hive-staging_hive_2016-04-14_18-31-19_532_3480081382837900888-1/-ext-10002

                    NumFilesPerFileSink: 1

                    Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE

                    Stats Publishing Key Prefix: hdfs://os-r6-qugztu-hive-1-5.novalocal:8020/tmp/hive/hrt_qa/ec0773d7-0ac2-45c7-b9cb-568bbed2c49c/hive_2016-04-14_18-31-19_532_3480081382837900888-1/-mr-10001/.hive-staging_hive_2016-04-14_18-31-19_532_3480081382837900888-1/-ext-10002/

                    table:

                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat

                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat

                        properties:

                          columns _col0

                          columns.types timestamp

                          escape.delim \

                          hive.serialization.extend.additional.nesting.levels true

                          serialization.escape.crlf true

                          serialization.format 1

                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

                    TotalFiles: 1

                    GatherStats: false

                    MultiFileSpray: false

        Union 2 

            Vertex: Union 2



  Stage: Stage-0

    Fetch Operator

      limit: 5

      Processor Tree:

        ListSink



Time taken: 0.301 seconds, Fetched: 284 row(s)

{noformat}



Both the queries used return timestamp with YYYY-MM-DD HH:MM:SS.fff format in past releases.


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-05-31 15:36:37" id="13844" opendate="2016-05-25 09:58:36" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Invalid index handler in org.apache.hadoop.hive.ql.index.HiveIndex class</summary>
			
			
			<description>Class org.apache.hadoop.hive.ql.index.HiveIndex has invalid handler name &amp;amp;apos;org.apache.hadoop.hive.ql.AggregateIndexHandler&amp;amp;apos;. The actual FQ class name is &amp;amp;apos;org.apache.hadoop.hive.ql.index.AggregateIndexHandler&amp;amp;apos;



  public static enum IndexType {

    AGGREGATE_TABLE(&quot;aggregate&quot;, &quot;org.apache.hadoop.hive.ql.AggregateIndexHandler&quot;),

    COMPACT_SUMMARY_TABLE(&quot;compact&quot;, &quot;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&quot;),

    BITMAP_TABLE(&quot;bitmap&quot;,&quot;org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler&quot;);



    private IndexType(String indexType, String className) {

      indexTypeName = indexType;

      this.handlerClsName = className;

    }



    private final String indexTypeName;

    private final String handlerClsName;



    public String getName() {

      return indexTypeName;

    }



    public String getHandlerClsName() {

      return handlerClsName;

    }

  }

  



Because all of the above statement like &amp;amp;apos;SHOW INDEXES ON MY_TABLE&amp;amp;apos; doesn&amp;amp;apos;t work in case of configured &amp;amp;apos;org.apache.hadoop.hive.ql.index.AggregateIndexHandler&amp;amp;apos; as index handler. In hive server log is observed java.lang.NullPointerException.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-01 18:09:43" id="13858" opendate="2016-05-26 01:41:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>LLAP: A preempted task can end up waiting on completeInitialization if some part of the executing code suppressed the interrupt</summary>
			
			
			<description>An interrupt along with a HiveProcessor.abort call is made when attempting to preempt a task.
In this specific case, the task was in the middle of HDFS IO - which &amp;amp;apos;handled&amp;amp;apos; the interrupt by retrying. As a result the interrupt status on the thread was reset - so instead of skipping the future.get in completeInitialization - the task ended up blocking there.
End result - a single executor slot permanently blocked in LLAP. Depending on what else is running - this can cause a cluster level deadlock.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Operator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-08 05:33:01" id="13838" opendate="2016-05-24 23:43:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Set basic stats as inaccurate for all ACID tables</summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Sub-task</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13971</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-11 01:16:21" id="13971" opendate="2016-06-08 04:26:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Address testcase failures of acid_globallimit.q and etc</summary>
			
			
			<description/>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13838</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-14 22:36:53" id="13833" opendate="2016-05-24 17:51:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Add an initial delay when starting the heartbeat</summary>
			
			
			<description>Since the scheduling of heartbeat happens immediately after lock acquisition, it&amp;amp;apos;s unnecessary to send heartbeat at the time when locks is acquired. Add an initial delay to skip this.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-16 15:16:23" id="14006" opendate="2016-06-13 20:32:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive query with UNION ALL fails with ArrayIndexOutOfBoundsException</summary>
			
			
			<description>set hive.cbo.enable=false;
DROP VIEW IF EXISTS a_view;
DROP TABLE IF EXISTS table_a1;
DROP TABLE IF EXISTS table_a2;
DROP TABLE IF EXISTS table_b1;
DROP TABLE IF EXISTS table_b2;
CREATE TABLE table_a1
(composite_key STRING);
CREATE TABLE table_a2
(composite_key STRING);
CREATE TABLE table_b1
(composite_key STRING, col1 STRING);
CREATE TABLE table_b2
(composite_key STRING);
CREATE VIEW a_view AS
SELECT
substring(a1.composite_key, 1, locate(&amp;amp;apos;|&amp;amp;apos;,a1.composite_key) - 1) AS autoname,
NULL AS col1
FROM table_a1 a1
FULL OUTER JOIN table_a2 a2
ON a1.composite_key = a2.composite_key
UNION ALL
SELECT
substring(b1.composite_key, 1, locate(&amp;amp;apos;|&amp;amp;apos;,b1.composite_key) - 1) AS autoname,
b1.col1 AS col1
FROM table_b1 b1
FULL OUTER JOIN table_b2 b2
ON b1.composite_key = b2.composite_key;
INSERT INTO TABLE table_b1
SELECT * FROM (
SELECT &amp;amp;apos;something|awful&amp;amp;apos;, &amp;amp;apos;col1&amp;amp;apos;
)s ;
SELECT autoname
FROM a_view
WHERE autoname=&amp;amp;apos;something&amp;amp;apos;;
fails with 
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
{&quot;_col0&quot;:&quot;something&quot;}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:179)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
{&quot;_col0&quot;:&quot;something&quot;}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)
	... 8 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.hive.ql.exec.UnionOperator.processOp(UnionOperator.java:134)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)
The same query succeeds when hive.ppd.remove.duplicatefilters=false with or without CBO on. It also succeeds with just CBO on.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-06-20 22:34:03" id="13809" opendate="2016-05-20 17:55:44" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hybrid Grace Hash Join memory usage estimation didn&amp;apos;t take into account the bloom filter size</summary>
			
			
			<description>Memory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there&amp;amp;apos;s enough memory but it turns out not the case, we will run into OOM problem.
Currently hybrid grace hash join memory usage estimation didn&amp;amp;apos;t take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.
The solution is to count in the bloom filter size into memory estimation.
Another issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">13934</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-06-22 18:37:47" id="13159" opendate="2016-02-25 22:20:09" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TxnHandler should support datanucleus.connectionPoolingType = None</summary>
			
			
			<description>Right now, one has to choose bonecp or dbcp.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">12579</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-07 22:35:40" id="14132" opendate="2016-06-29 22:48:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Don&amp;apos;t fail config validation for removed configs</summary>
			
			
			<description>Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">14133</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-07 22:36:00" id="14133" opendate="2016-06-29 22:50:00" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Don&amp;apos;t fail config validation for removed configs</summary>
			
			
			<description>Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">14132</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-08 17:44:01" id="14038" opendate="2016-06-16 18:45:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>miscellaneous acid improvements</summary>
			
			
			<description>1. fix thread name inHouseKeeperServiceBase (currently they are all &quot;org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase$1-0&quot;)
2. dump metastore configs from HiveConf on start up to help record values of properties
3. add some tests</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
			
			
			<file type="M">org.apache.hive.hcatalog.templeton.AppConfig.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-17 17:16:14" id="13191" opendate="2016-03-01 19:29:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>DummyTable map joins mix up columns between tables</summary>
			
			
			<description>


SELECT

  a.key,

  a.a_one,

  b.b_one,

  a.a_zero,

  b.b_zero

FROM

(

    SELECT

      11 key,

      0 confuse_you,

      1 a_one,

      0 a_zero

) a

LEFT JOIN

(

    SELECT

      11 key,

      0 confuse_you,

      1 b_one,

      0 b_zero

) b

ON a.key = b.key

;



11      1       0       0       1



This should be 11, 1, 1, 0, 0 instead. 
Disabling map-joins &amp;amp; using shuffle-joins returns the right result.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>1.3.0, 2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-17 19:31:27" id="14236" opendate="2016-07-14 04:42:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CTAS with UNION ALL puts the wrong stats in Tez</summary>
			
			
			<description>to repo. in Tez, create table t as select * from src union all select * from src;</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">11863</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-19 00:23:50" id="11918" opendate="2015-09-22 20:18:14" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Implement/Enable constant related optimization rules in Calcite</summary>
			
			
			<description>Right now, Hive optimizer (Calcite) is short of the constant related optimization rules. For example, constant folding, constant propagation and constant transitive rules. Although Hive later provides those rules in the logical optimizer, we would like to implement those inside Calcite. This will benefit the current optimization as well as the optimization based on return path that we are planning to use in the future. This JIRA is the umbrella JIRA to implement/enable those rules.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.1.0</fixedVersion>
			
			
			<type>Improvement</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.QTestUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13608</link>
			
			
			<link description="is related to" type="Reference">902</link>
			
			
			<link description="is related to" type="Reference">909</link>
			
			
			<link description="is related to" type="Reference">11110</link>
			
			
			<link description="is related to" type="Reference">9132</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-07-21 15:24:21" id="14229" opendate="2016-07-13 20:03:59" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>the jars in hive.aux.jar.paths are not added to session classpath </summary>
			
			
			<description>The jars in hive.reloadable.aux.jar.paths are being added to HiveServer2 classpath while hive.aux.jar.paths is not. 
Then the local task like &amp;amp;apos;select udf from src&amp;amp;apos; will fail to find needed udf class.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.SessionState.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.Utilities.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.processors.ReloadProcessor.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-07-27 20:10:01" id="14296" opendate="2016-07-20 19:22:16" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Session count is not decremented when HS2 clients do not shutdown cleanly.</summary>
			
			
			<description>When a JDBC client like beeline abruptly disconnects from HS2, the session gets closed on the serverside but the session count reported in the logs is incorrect. It never gets decremented.
For example, I created 6 connections from the same instance of beeline to HS2.



2016-07-20T15:05:17,987  INFO [HiveServer2-Handler-Pool: Thread-40] thrift.ThriftCLIService: Opened a session SessionHandle [28b225ee-204f-4b3e-b4fd-0039ef8e276e], current sessions: 1

.....

2016-07-20T15:05:24,239  INFO [HiveServer2-Handler-Pool: Thread-45] thrift.ThriftCLIService: Opened a session SessionHandle [1d267de8-ff9a-4e76-ac5c-e82c871588e7], current sessions: 2

.....

2016-07-20T15:05:25,710  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Opened a session SessionHandle [04d53deb-8965-464b-aa3f-7042304cfb54], current sessions: 3

.....

2016-07-20T15:05:26,795  INFO [HiveServer2-Handler-Pool: Thread-55] thrift.ThriftCLIService: Opened a session SessionHandle [b4bb8b86-74e1-4e3c-babb-674d34ad1caf], current sessions: 4

2016-07-20T15:05:28,160  INFO [HiveServer2-Handler-Pool: Thread-60] thrift.ThriftCLIService: Opened a session SessionHandle [6d3c3ed9-fadb-4673-8c15-3315b7e2995d], current sessions: 5

.....

2016-07-20T15:05:29,136  INFO [HiveServer2-Handler-Pool: Thread-65] thrift.ThriftCLIService: Opened a session SessionHandle [88b630c0-f272-427d-8263-febfe2222f8d], current sessions: 6



When I CNTRL-C the beeline process, in the HS2 logs I see



2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-55] thrift.ThriftCLIService: Session disconnected without closing properly. 

2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-40] thrift.ThriftCLIService: Session disconnected without closing properly. 

2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-65] thrift.ThriftCLIService: Session disconnected without closing properly. 

2016-07-20T15:11:37,858  INFO [HiveServer2-Handler-Pool: Thread-60] thrift.ThriftCLIService: Session disconnected without closing properly. 

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Session disconnected without closing properly. 

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-45] thrift.ThriftCLIService: Session disconnected without closing properly. 

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-55] thrift.ThriftCLIService: Closing the session: SessionHandle [b4bb8b86-74e1-4e3c-babb-674d34ad1caf]

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-40] thrift.ThriftCLIService: Closing the session: SessionHandle [28b225ee-204f-4b3e-b4fd-0039ef8e276e]

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-65] thrift.ThriftCLIService: Closing the session: SessionHandle [88b630c0-f272-427d-8263-febfe2222f8d]

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-60] thrift.ThriftCLIService: Closing the session: SessionHandle [6d3c3ed9-fadb-4673-8c15-3315b7e2995d]

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-45] thrift.ThriftCLIService: Closing the session: SessionHandle [1d267de8-ff9a-4e76-ac5c-e82c871588e7]

2016-07-20T15:11:37,859  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Closing the session: SessionHandle [04d53deb-8965-464b-aa3f-7042304cfb54]



The next time I connect to HS2 via beeline, I see



2016-07-20T15:14:33,679  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V8

2016-07-20T15:14:33,710  INFO [HiveServer2-Handler-Pool: Thread-50] session.SessionState: Created HDFS directory: /tmp/hive/hive/d47759e8-df3a-4504-9f28-99ff5247352c

2016-07-20T15:14:33,725  INFO [HiveServer2-Handler-Pool: Thread-50] session.SessionState: Created local directory: /var/folders/_3/0w477k4j5bjd6h967rw4vflw0000gp/T/ngangam/d47759e8-df3a-4504-9f28-99ff5247352c

2016-07-20T15:14:33,735  INFO [HiveServer2-Handler-Pool: Thread-50] session.SessionState: Created HDFS directory: /tmp/hive/hive/d47759e8-df3a-4504-9f28-99ff5247352c/_tmp_space.db

2016-07-20T15:14:33,737  INFO [HiveServer2-Handler-Pool: Thread-50] session.HiveSessionImpl: Operation log session directory is created: /var/folders/_3/0w477k4j5bjd6h967rw4vflw0000gp/T/ngangam/operation_logs/d47759e8-df3a-4504-9f28-99ff5247352c

2016-07-20T15:14:33,737  INFO [HiveServer2-Handler-Pool: Thread-50] thrift.ThriftCLIService: Opened a session SessionHandle [d47759e8-df3a-4504-9f28-99ff5247352c], current sessions: 7



So while the sessions itself are closed and cleaned up, the session count reported is not accurate.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.service.cli.session.SessionManager.java</file>
			
			
			<file type="M">org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-01 07:46:56" id="14367" opendate="2016-07-28 02:20:54" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Estimated size for constant nulls is 0</summary>
			
			
			<description>since type is incorrectly assumed as void.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-08-01 18:44:45" id="14322" opendate="2016-07-24 06:42:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Postgres db issues after Datanucleus 4.x upgrade</summary>
			
			
			<description>With the upgrade to  datanucleus 4.x versions in HIVE-6113, hive does not work properly with postgres.
The nullable fields in the database have string &quot;NULL::character varying&quot; instead of real NULL values. This causes various issues.
One example is -



hive&amp;gt; create table t(i int);

OK

Time taken: 1.9 seconds

hive&amp;gt; create view v as select * from t;

OK

Time taken: 0.542 seconds

hive&amp;gt; select * from v;

FAILED: SemanticException Unable to fetch table v. java.net.URISyntaxException: Relative path in absolute URI: NULL::character%20varying



</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0, 2.1.1, 2.0.2</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.conf.HiveConf.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">15204</link>
			
			
			<link description="is related to" type="Reference">14371</link>
			
			
			<link description="is related to" type="Reference">6113</link>
			
			
			<link description="is related to" type="Reference">1841</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-08-12 23:27:53" id="14521" opendate="2016-08-11 20:07:40" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>codahale metrics exceptions</summary>
			
			
			<description>One some random setup, I see bazillions of errors like this in HS2 log:

2016-08-08 04:52:18,619 WARN  [HiveServer2-Handler-Pool: Thread-101]: log.PerfLogger (PerfLogger.java:beginMetrics(226)) - Error recording metrics

java.io.IOException: Scope named api_Driver.run is not closed, cannot be opened.

        at org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics$CodahaleMetricsScope.open(CodahaleMetrics.java:133)

        at org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.startStoredScope(CodahaleMetrics.java:220)

        at org.apache.hadoop.hive.ql.log.PerfLogger.beginMetrics(PerfLogger.java:223)

        at org.apache.hadoop.hive.ql.log.PerfLogger.PerfLogBegin(PerfLogger.java:143)

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:378)

        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:320)

        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1214)

        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1208)

        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)

        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)

        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)

        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)

        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)



I suspect that either, just like the metastore deadline, this needs better error handling when whatever the metrics surround fails; or, it is just not thread safe.
But I actually haven&amp;amp;apos;t looked at the code yet.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.Driver.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
			
			
			<file type="M">org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">13410</link>
			
			
			<link description="is related to" type="Reference">13410</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-09-03 04:13:15" id="13383" opendate="2016-03-30 07:11:39" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>RetryingMetaStoreClient retries non retriable embedded metastore client </summary>
			
			
			<description>Embedded metastore clients can&amp;amp;apos;t be retried, they throw an exception - &quot;For direct MetaStore DB connections, we don&amp;amp;apos;t support retries at the client level.&quot;
This tends to mask the real error that caused the attempts to retry. RetryingMetaStoreClient shouldn&amp;amp;apos;t even attempt to reconnect when direct/embedded metastore client is used.
</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-09-14 18:52:54" id="14251" opendate="2016-07-15 18:25:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Union All of different types resolves to incorrect data</summary>
			
			
			<description>create table src(c1 date, c2 int, c3 double);
insert into src values (&amp;amp;apos;2016-01-01&amp;amp;apos;,5,1.25);
select * from 
(select c1 from src union all
select c2 from src union all
select c3 from src) t;
It will return NULL for the c1 values. Seems the common data type is resolved to the last c3 which is double.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
			
			
			<file type="M">org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2016-10-20 13:40:06" id="13423" opendate="2016-04-05 13:49:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Handle the overflow case for decimal datatype for sum()</summary>
			
			
			<description>When a column col1 defined as decimal and if the sum of the column overflows, we will try to increase the decimal precision by 10. But if it&amp;amp;apos;s reaching 38 (the max precision), the overflow still could happen. Right now, if such case happens, the following exception will throw since hive is writing incorrect data.
Follow the following steps to repro. 

CREATE TABLE DECIMAL_PRECISION(dec decimal(38,18));

INSERT INTO DECIMAL_PRECISION VALUES(98765432109876543210.12345), (98765432109876543210.12345);

SELECT SUM(dec) FROM DECIMAL_PRECISION;




Caused by: java.lang.ArrayIndexOutOfBoundsException: 1

        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.readVInt(LazyBinaryUtils.java:314) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.checkObjectByteInfo(LazyBinaryUtils.java:219) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]

        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.parse(LazyBinaryStruct.java:142) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]


</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">6459</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-11-07 03:27:09" id="15054" opendate="2016-10-25 15:53:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Hive insertion query execution fails on Hive on Spark</summary>
			
			
			<description>The query of insert overwrite table tbl1 sometimes will fail with the following errors. Seems we are constructing taskAttemptId with partitionId which is not unique if there are multiple attempts.

ava.lang.IllegalStateException: Hit error while closing operators - failing tree: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_task_tmp.-ext-10002/_tmp.002148_0 to: hdfs://table1/.hive-staging_hive_2016-06-14_01-53-17_386_3231646810118049146-9/_tmp.-ext-10002/002148_0

at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.close(SparkMapRecordHandler.java:202)

at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.closeRecordProcessor(HiveMapFunctionResultList.java:58)

at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.hasNext(HiveBaseFunctionResultList.java:106)

at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:41)

at scala.collection.Iterator$class.foreach(Iterator.scala:727)

at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)

at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$15.apply(AsyncRDDActions.scala:120)



</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HivePairFlatMapFunction.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is duplicated by" type="Duplicate">13066</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-11-07 03:28:24" id="13066" opendate="2016-02-17 06:47:59" resolution="Duplicate">
		
		
		<buginformation>
			
			
			<summary>Hive on Spark gives incorrect results when speculation is on</summary>
			
			
			<description>The issue is reported by users. One possible reason is that we always append 0 as the attempt ID for each task so that hive won&amp;amp;apos;t be able to distinguish between speculative tasks and original ones.</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hive.ql.exec.spark.HivePairFlatMapFunction.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="duplicates" type="Duplicate">15054</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2016-11-13 03:17:53" id="15096" opendate="2016-10-29 03:00:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hplsql registerUDF conflicts with pom.xml</summary>
			
			
			<description>in hplsql code, registerUDF code is
    sql.add(&quot;ADD JAR &quot; + dir + &quot;hplsql.jar&quot;);
    sql.add(&quot;ADD JAR &quot; + dir + &quot;antlr-runtime-4.5.jar&quot;);
    sql.add(&quot;ADD FILE &quot; + dir + Conf.SITE_XML);
but pom configufation is
  &amp;lt;parent&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.hive&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;hive&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;2.2.0-SNAPSHOT&amp;lt;/version&amp;gt;
    &amp;lt;relativePath&amp;gt;../pom.xml&amp;lt;/relativePath&amp;gt;
  &amp;lt;/parent&amp;gt;
  &amp;lt;artifactId&amp;gt;hive-hplsql&amp;lt;/artifactId&amp;gt;
  &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;
  &amp;lt;name&amp;gt;Hive HPL/SQL&amp;lt;/name&amp;gt;
    &amp;lt;dependency&amp;gt;
       &amp;lt;groupId&amp;gt;org.antlr&amp;lt;/groupId&amp;gt;
       &amp;lt;artifactId&amp;gt;antlr4-runtime&amp;lt;/artifactId&amp;gt;
       &amp;lt;version&amp;gt;4.5&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
when run hplsql , errors occur as below
 Error while processing statement: /opt/apps/apache-hive-2.0.0-bin/lib/hplsql.jar does not exist</description>
			
			
			<version>2.0.0</version>
			
			
			<fixedVersion>2.2.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hive.hplsql.Exec.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
</bugrepository>
