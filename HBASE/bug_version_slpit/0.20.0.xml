<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="HBASE">
	<bug fixdate="2009-03-06 06:42:22" id="1239" opendate="2009-03-04 20:42:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>input buffer reading in the REST interface does not correctly clear the character buffer each iteration</summary>
			
			
			<description>when reading the input buffer in the REST interface the character buffer is not cleared for each iteration of the loop.  This can cause malformed data to be read from the input stream in cases where the input is greater than 640 characters.
See lines numbered 366-376 in org.apache.hadoop.hbase.rest.Dispatcher.java
I have prepared a patch for this.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.rest.Dispatcher.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-03-06 06:47:25" id="1245" opendate="2009-03-06 06:15:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hfile meta block handling bugs</summary>
			
			
			<description>HFile doesn&amp;amp;apos;t handle &amp;amp;apos;get meta block&amp;amp;apos; when there are no meta blocks.  It throws an unhelpful exception &quot;meta index not loaded&quot;, which is not the case.  No meta blocks = no meta index.  It should return null instead.
Additionally, hfile doesn&amp;amp;apos;t even get all meta names properly, due to the incorrect use of the file&amp;amp;apos;s comparator, instead of using just a bytes comparator in the index.  This is manifested by NPEs in some tests.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.Bytes.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-03-19 08:52:31" id="1267" opendate="2009-03-19 00:47:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>binary keys broken in trunk (again).</summary>
			
			
			<description>Binary keys, specifically ones where the first byte of the key is nul &amp;amp;apos;\0&amp;amp;apos; don&amp;amp;apos;t work:

Splits happen
Logfile indicates everything normal

But the .META. doesnt list all the regions.  It only lists the &amp;amp;apos;basic&amp;amp;apos; regions: &amp;amp;apos;table,,1234&amp;amp;apos;.  The other regions with the binary keys in the middle just dont seem to be in .META....</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Memcache.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HStoreKey.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-03-20 09:55:56" id="1275" opendate="2009-03-20 09:03:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestTable.testCreateTable broken</summary>
			
			
			<description>Test is broken, we seem to be able to create the same table 10x over.  ouch!</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.HMaster.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HStoreKey.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-03-26 01:14:23" id="1290" opendate="2009-03-25 21:28:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>table.jsp either 500s out or doesnt list the regions</summary>
			
			
			<description>The table.jsp page either 500 errors out if you are viewing a .META. or ROOT table, or for user tables it doesn&amp;amp;apos;t list the regions.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ServerManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.HMaster.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-03-27 06:09:28" id="1157" opendate="2009-01-27 02:55:58" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>If we do not take start code as a part of region server recovery, we could inadvertantly try to reassign regions assigned to a restarted server with a different start code</summary>
			
			
			<description/>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ServerManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.HMaster.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.TableOperation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.TableDelete.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HServerInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.BaseScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ModifyTableMeta.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ChangeTableState.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HRegionInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ColumnOperation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HLog.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="blocks" type="Blocker">1144</link>
			
			
			<link description="relates to" type="Reference">1156</link>
			
			
			<link description="relates to" type="Reference">1158</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-03-28 00:39:06" id="1293" opendate="2009-03-27 22:56:18" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hfile doesn&amp;apos;t recycle decompressors</summary>
			
			
			<description>The Compression codec stuff from hadoop has the concept of recycling compressors and decompressors - this is because a compression codec uses &quot;direct buffers&quot; which reside outside the JVM regular heap space.  There is a risk that under heavy concurrent load we could run out of that &amp;amp;apos;direct buffer&amp;amp;apos; heap space in the JVM.
HFile does not call algorithm.returnDecompressor and returnCompressor.  We should fix that.
I found this bug via OOM crashes under jdk 1.7 - it appears to be partially due to the size of my cluster (200gb, 800 regions, 19 servers) and partially due to weaknesses in JVM 1.7.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-04-07 18:23:08" id="1309" opendate="2009-04-05 02:18:31" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HFile rejects key in Memcache with empty value</summary>
			
			
			<description>2009-04-05 02:12:56,497 FATAL org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: content,,1238896745127
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:878)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:771)
	at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.flushRegion(MemcacheFlusher.java:229)
	at org.apache.hadoop.hbase.regionserver.MemcacheFlusher.run(MemcacheFlusher.java:139)
Caused by: java.io.IOException: Value cannot be null or empty
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkValue(HFile.java:485)
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:447)
	at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:501)
	at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:463)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:863)
	... 3 more</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-04-20 07:11:17" id="1330" opendate="2009-04-20 06:09:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>binary keys broken on trunk</summary>
			
			
			<description>The symptom is commits fail with &amp;amp;apos;table not found&amp;amp;apos; exception - even though the table does in fact exist!
Digging in a little with debug logs indicate that getClosestRowBefore() is returning NULL, which for a table that exists should never be!  A key always falls into a region - either the first or the last one at the very least.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-04-21 08:14:28" id="1332" opendate="2009-04-21 07:37:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>regionserver carrying .META. starts sucking all cpu, drives load up - infinite loop?</summary>
			
			
			<description>the symptom is the cluster comes to a dead halt.  Lookups on meta don&amp;amp;apos;t seem to work, and the regionserver carrying .META. goes hot - using 800% CPU or more, driving system LA up really really high (I&amp;amp;apos;ve seen it as high as 26).  Thread dumps seem to indicate every IPC handler is stuck in Bytes.binarySearch().
</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-04-21 23:53:56" id="1334" opendate="2009-04-21 20:45:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>.META. region running into hfile errors</summary>
			
			
			<description>my .META. region refuses to do anything, I get this snippet of a error in the log file:
Caused by: java.lang.IllegalArgumentException at 
                java.nio.Buffer.position(Buffer.java:236) at 
org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.blockSeek(HFile.java:1121)
Looks like the seek code is breaking somehow - seeking before the beginning of the block maybe?</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Scanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-05-06 00:37:13" id="1336" opendate="2009-04-22 20:31:34" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Splitting up the compare of family+column into 2 different compares</summary>
			
			
			<description>When comparing family+column you can end up in  a situation like column1 is &quot;abcd:efg&quot; and column2 is &quot;abc:defg&quot; which in the current implementation of
KeyValue.KeyComparator.compare will result in a faulty result.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.TestKeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.MetaUtils.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HAbstractScanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-05-06 21:26:32" id="1370" opendate="2009-05-05 07:04:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>re-enable LZO using hadoop-gpl-compression library</summary>
			
			
			<description>now that hadoop-gpl-compression has been released, we can add an optional run time dependency to allow LZO compression again.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.Compression.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HColumnDescriptor.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-05-08 15:40:25" id="1392" opendate="2009-05-08 05:59:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>change how we build/configure lzocodec</summary>
			
			
			<description>i got a reply to my proposed patch for lzocodec:
Instead of deriving from DefaultCodec, you probably want to do the followng:
CompressionCodec lzoCodec = (CompressionCodec)
ReflectionUtils.newInstance(Class.forName(&quot;com.hadoop.compression.lzo.LzoCodec&quot;), conf);
setConf is automatically handled by RefletionUtils.newInstance.
We should do that.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.Compression.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-05-09 06:12:29" id="1398" opendate="2009-05-09 05:58:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TableOperation doesnt format keys for meta scan properly</summary>
			
			
			<description>to scan the meta table, the start row must be in the format &amp;amp;apos;table_name,,&amp;amp;apos; - the commas are not optional.
I found another place in TableOperation which was missing this, causing hbase to close too many regions from unrelated tables (ouch!)</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.TableOperation.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-05-18 17:44:34" id="1401" opendate="2009-05-10 22:32:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>close HLog (and open new one) if there hasnt been edits in N minutes/hours</summary>
			
			
			<description>this will help narrow the write hole on clusters with periods of light load.  </description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0, 0.19.3</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HLog.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-05-31 16:29:44" id="1457" opendate="2009-05-29 05:46:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Taking down ROOT/META regionserver can result in cluster becoming in-operational</summary>
			
			
			<description>Take down a regionserver via controlled or uncontrolled shutdown, the master doesn&amp;amp;apos;t properly reassign the root/meta regions. </description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessRegionStatusChange.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionServerOperation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ServerManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.HMaster.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RetryableMetaOperation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RootScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.MetaRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-01 23:05:47" id="1471" opendate="2009-06-01 19:08:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>During cluster shutdown, deleting zookeeper regionserver nodes causes exceptions</summary>
			
			
			<description>Exception happens during some unit tests:

2009-06-01 11:05:18,075 INFO  [RegionServer:0] regionserver.HRegionServer(665): stopping server at: 192.168.249.1:41463

2009-06-01 11:05:18,076 INFO  [RegionServer:0] regionserver.HRegionServer(679): RegionServer:0 exiting

2009-06-01 11:05:18,077 DEBUG [HMaster] zookeeper.ZooKeeperWrapper(531): Deleting node: 1243883037682

Exception in thread &quot;HMaster&quot; java.lang.IllegalArgumentException: Path must start with / character

	at org.apache.zookeeper.ZooKeeper.validatePath(ZooKeeper.java:537)

	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:642)

	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.clearRSDirectory(ZooKeeperWrapper.java:532)

	at org.apache.hadoop.hbase.master.RegionManager.stop(RegionManager.java:620)

	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:425)

2009-06-01 11:05:19,077 INFO  [main] hbase.LocalHBaseCluster(294): Shutdown HMaster 1 region server(s)

2009-06-01 11:05:19,081 INFO  [main] hbase.HBaseTestCase(592): Shutting down FileSystem

2009-06-01 11:05:19,081 INFO  [main] hbase.HBaseTestCase(599): Shutting down Mini DFS 

Shutting down the Mini HDFS Cluster

Shutting down DataNode 1

2009-06-01 11:05:19,178 DEBUG [HMaster-EventThread] client.HConnectionManager$TableServers(203): Got ZooKeeper event, state: Disconnected, type: None, path: null

2009-06-01 11:05:19,178 DEBUG [RegionManager.rootScanner-EventThread] client.HConnectionManager$TableServers(203): Got ZooKeeper event, state: Disconnected, type: None, path: null

2009-06-01 11:05:19,179 DEBUG [main-EventThread] client.HConnectionManager$TableServers(203): Got ZooKeeper event, state: Disconnected, type: None, path: null

2009-06-01 11:05:19,179 INFO  [main-EventThread] regionserver.HRegionServer(367): Got ZooKeeper event, state: Disconnected, type: None, path: null

2009-06-01 11:05:19,179 DEBUG [main-EventThread] regionserver.HRegionServer(372): Ignoring ZooKeeper event while shutting down

2009-06-01 11:05:19,185 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@d522de2] datanode.DataXceiverServer(137): DatanodeRegistration(127.0.0.1:36486, storageID=DS-350082559-192.168.249.1-36486-1243883037153, infoPort=58600, ipcPort=57545):DataXceiveServer: java.nio.channels.AsynchronousCloseException

	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)

	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:170)

	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:102)

	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:130)

	at java.lang.Thread.run(Thread.java:636)



Shutting down DataNode 0

2009-06-01 11:05:20,293 WARN  [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@42238016] datanode.DataXceiverServer(137): DatanodeRegistration(127.0.0.1:37436, storageID=DS-1878503705-192.168.249.1-37436-1243883036827, infoPort=41289, ipcPort=36123):DataXceiveServer: java.nio.channels.AsynchronousCloseException

	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)

	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:170)

	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:102)

	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:130)

	at java.lang.Thread.run(Thread.java:636)



2009-06-01 11:05:21,395 WARN  [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$ReplicationMonitor@286e4365] namenode.FSNamesystem$ReplicationMonitor(2306): ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted

2009-06-01 11:05:21,399 INFO  [Thread-121] regionserver.HRegionServer$ShutdownThread(951): Starting shutdown thread.

2009-06-01 11:05:21,400 INFO  [Thread-121] regionserver.HRegionServer$ShutdownThread(959): Shutdown thread complete



This is from the test org.apache.hadoop.hbase.client.TestHTable.testHTable()</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-02 17:54:25" id="1357" opendate="2009-04-29 18:38:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>If one sets the hbase.master to 0.0.0.0 non local regionservers can&amp;apos;t find the master</summary>
			
			
			<description>(2:11:20 PM) posix4e: so i want to run a back master on each node
(2:11:29 PM) posix4e: and i have my hbase.master set to 0.0.0.0
(2:14:59 PM) posix4e: each master only gets the local regionserver connecting
(2:15:08 PM) posix4e: as it must be using that variable to know what to connect to
(2:15:32 PM) nitay: the RS don&amp;amp;apos;t use hbase.master* anymore
(2:15:36 PM) nitay: ohhh i think i know th eproblem
(2:15:44 PM) nitay: so the RS use ZK to get the master address
(2:15:49 PM) nitay: but the masters are writing 0.0.0.0 to it
(2:15:58 PM) nitay: b/c they write whatever was in their conf
(2:16:20 PM) posix4e: yea
(2:16:42 PM) nitay: can u do a zookeeper dump of that node to verify my thinking?
(2:16:55 PM) posix4e: yea
(2:17:12 PM) nitay: it should be /hbase/master, unless u&amp;amp;apos;ve changed the defaults
(2:17:59 PM) nitay: hmm s o ye this is a problem, we solved this in RS (allowing 0.0.0.0) by having master actually write RS&amp;amp;apos;s address to ZK when it gets contacted
(2:18:21 PM) nitay: so now we need to find a way to find out the actual address the master has bound to
(2:19:47 PM) posix4e: is their a way to do that?
(2:20:16 PM) nitay: i dont know, good question
(2:20:18 PM) posix4e: or does it require code changes i.e. regionserver checking zk
(2:20:27 PM) nitay: did u verify the master address?
(2:20:48 PM) posix4e: one sec
(2:21:03 PM) nitay: its almost like we want ZK to be able to tell us what address we&amp;amp;apos;re using to talk to it
(2:21:20 PM) nitay: that assumes u dont have different NICs to talk to ZK vs. HBase
(2:21:59 PM) nitay: posix4e, u can&amp;amp;apos;t really use the RS as far as i can tell b/c the RS knows nothing about the master until the master address appears in ZK
(2:22:25 PM) posix4e: 0:0:0:0:0:0:0:0:60000
(2:22:40 PM) nitay: yep that&amp;amp;apos;s the magic
(2:22:45 PM) nitay: k thx for verifying
(2:22:54 PM) nitay: u want to open up a JIRA?
(2:22:57 PM) posix4e: but if i could tell hbase.site to just use my hostname:port it would work ok
(2:22:58 PM) posix4e: yea
(2:23:09 PM) posix4e: can i quote this conversation?
(2:23:18 PM) nitay: yes please do
(2:23:45 PM) nitay: also, to fix this here and now for u, u&amp;amp;apos;d essentially need to actually set hbase.master* to the ip/host u&amp;amp;apos;re using
(2:23:55 PM) nitay: and change it on each backup master to that guy&amp;amp;apos;s host/ip
(2:24:02 PM) nitay: i know, its a royal PITA
(2:24:59 PM) posix4e: yea
(2:25:03 PM) posix4e: no problem
(2:25:20 PM) nitay: but that should work till we find a better solution
(2:25:21 PM) posix4e: I am trying to think how a patch would work
(2:25:25 PM) posix4e: have a masters file?
(2:25:44 PM) nitay: yeah if u have any ideas please offer them
(2:25:46 PM) nitay: hmm interesting idea
(2:26:16 PM) nitay: and then do some local gethostbyname() type thing checking against masters file?
(2:26:26 PM) posix4e: yea
(2:28:23 PM) nitay: one thing to note is we&amp;amp;apos;ve talked about eventually getting to a place where any RS can be master
(2:28:30 PM) nitay: but i like your idea
(2:28:37 PM) nitay: post it on the JIRA
(2:30:24 PM) nitay: i gotta run, thanks for the info posix4e - very helpful, its great to hear from people actually using this stuff
(2:32:56 PM) posix4e: yep
I also solved this by manually setting the hbase.master  on each host to point to the local hostname, which sucks.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ServerManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.HMaster.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HConstants.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">1445</link>
			
			
			<link description="is depended upon by" type="dependent">1448</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-06-03 16:48:03" id="1462" opendate="2009-05-31 09:07:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>hclient still seems to depend on master</summary>
			
			
			<description>during a master down, but cluster up event, my clients seem to not work.
clients shouldnt need to talk to the master anymore in 0.20.  We should double check this.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-08 02:56:52" id="1466" opendate="2009-06-01 08:26:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Binary keys are not first class citizens</summary>
			
			
			<description>If you use binary keys, you don&amp;amp;apos;t get full features as if you were not using binary keys.  Some things that are broken:

grep/less cant search in logfiles with binary data
displays are unreadable due to weird utf8/other issues
can&amp;amp;apos;t use the region historian
etc

</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.TestTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.HMaster.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.TableOperation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.HeapSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HMsg.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HFilePerformanceEvaluation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.BaseScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessRegionClose.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ServerManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HStoreKey.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.RegionHistorian.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.Bytes.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HRegionInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.mapred.TableSplit.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.TableDelete.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-14 03:17:51" id="1518" opendate="2009-06-13 23:51:01" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Delete Trackers using compareRow, should just use raw binary comparator</summary>
			
			
			<description>Doesn&amp;amp;apos;t matter when using a normal table, but this is using a special comparator on columns for the catalog tables.  Replace comparator.compareRows with Bytes.compareTo</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestGetDeleteTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.GetDeleteTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.QueryMatcher.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-15 05:45:43" id="1522" opendate="2009-06-15 05:40:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>We delete splits before their time occasionally</summary>
			
			
			<description>the master logfile says:
2009-06-13 01:47:50,993 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Looking for reference files in: hdfs://borg13:9000/hbase-1304/table/582659871/default2009-06-13 01:47:50,994 DEBUG org.apache.hadoop.hbase.master.BaseScanner: isReference: hdfs://borg13:9000/hbase-1304/table/582659871/default/7693630986938671024
2009-06-13 01:47:50,995 DEBUG org.apache.hadoop.hbase.master.BaseScanner: table,,1244880487129/582659871 no longer has references to table,,1244877673805
2009-06-13 01:47:51,007 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Looking for reference files in: hdfs://borg13:9000/hbase-1304/table/582659871/default2009-06-13 01:47:51,007 DEBUG org.apache.hadoop.hbase.master.BaseScanner: isReference: hdfs://borg13:9000/hbase-1304/table/582659871/default/7693630986938671024
2009-06-13 01:47:51,008 DEBUG org.apache.hadoop.hbase.master.BaseScanner: table,,1244880487129/582659871 no longer has references to table,,12448776738
052009-06-13 01:47:51,009 INFO org.apache.hadoop.hbase.master.BaseScanner: Deleting region table,,1244877673805 (encoded=1481906432) because daughter splits no longe
r hold references2009-06-13 01:47:51,009 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region hdfs://borg13:9000/hbase-1304/table/1481906432
As you can see, apparently splitA and splitB point to the same region!</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-15 05:48:04" id="1500" opendate="2009-06-08 23:00:50" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>KeyValue$KeyComparator array overrun</summary>
			
			
			<description>


09/06/08 22:58:47 INFO zookeeper.ZooKeeper: Initiating client connection, host=B

OA03:2181,BOA02:2181,BOA01:2181,BOA04:2181 sessionTimeout=10000 watcher=org.apac

he.hadoop.hbase.zookeeper.WatcherWrapper@518bf072

09/06/08 22:58:47 INFO zookeeper.ClientCnxn: zookeeper.disableAutoWatchReset is

false

09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Attempting connection to server BOA

04/172.20.3.231:2181

09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Priming connection to java.nio.chan

nels.SocketChannel[connected local=/172.20.3.232:40296 remote=BOA04/172.20.3.231

:2181]

09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Server connection successful

09/06/08 22:58:47 WARN mapred.JobClient: Use GenericOptionsParser for parsing th

e arguments. Applications should implement Tool for the same.

09/06/08 22:58:47 WARN mapred.JobClient: No job jar file set.  User classes may

not be found. See JobConf(Class) or JobConf#setJar(String).

09/06/08 22:58:47 INFO zookeeper.ZooKeeper: Initiating client connection, host=B

OA03:2181,BOA02:2181,BOA01:2181,BOA04:2181 sessionTimeout=10000 watcher=org.apac

he.hadoop.hbase.zookeeper.WatcherWrapper@362f0d54

09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Attempting connection to server BOA

03/172.20.3.230:2181

09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Priming connection to java.nio.chan

nels.SocketChannel[connected local=/172.20.3.232:42792 remote=BOA03/172.20.3.230

:2181]

09/06/08 22:58:47 INFO zookeeper.ClientCnxn: Server connection successful

09/06/08 22:58:48 INFO mapred.TableInputFormatBase: split: 0-&amp;gt;BOA04.trendmicro.c

om:,01e33c601a7a9dd0ddb5c8427438f2f1

Exception in thread &quot;main&quot; java.lang.ArrayIndexOutOfBoundsException: 32

        at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:798)

        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compareRows(KeyValue.j

ava:1760)

        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:

1696)

        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:

1755)

        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:

1687)

        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getCac

hedLocation(HConnectionManager.java:697)

        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locate

RegionInMeta(HConnectionManager.java:541)

        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locate

Region(HConnectionManager.java:525)

        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locate

Region(HConnectionManager.java:488)

        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getReg

ionLocation(HConnectionManager.java:342)

        at org.apache.hadoop.hbase.client.HTable.getRegionLocation(HTable.java:1

91)

        at org.apache.hadoop.hbase.mapred.TableInputFormatBase.getSplits(TableIn

putFormatBase.java:296)

        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)

        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)

        at net.iridiant.crawler.mapred.DocumentParser.main(Unknown Source)


</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.HalfHFileReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.MinorCompactingStoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-15 20:18:38" id="1525" opendate="2009-06-15 19:20:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HTable.incrementColumnValue hangs() </summary>
			
			
			<description>In the following code, 
   @Test
    public void usingIncrement() throws Exception 
    {
        long siteId = 1234;
        long publisherId = 5678;
        Date eventTime = DATE_INPUT_FORMAT.parse(&quot;2009-06-15 13:08:54&quot;);
        long[] metrics = new long[] 
{ 10, 22, 32 }
;
        byte[] rowKey = Bytes.toBytes(siteId + &quot;_&quot; + ROW_KEY_FORMAT.format(eventTime));
        byte[] family = Bytes.toBytes(FAMILY_PUBLISHER);
        byte[] qualifier = Bytes.toBytes(publisherId);
        HTable table = getTable();
        for (int i1 = 0, n1 = metrics.length; n1 &amp;gt; 0; i1++, n1--) {
            LOGGER.info(&quot;processing [
{0}] ...&quot;, i1);
            table.incrementColumnValue(rowKey, family, qualifier, metrics[i1]);
            LOGGER.info(&quot;processing [{0}
] completed&quot;, i1);
        }
        table.close();
        queryMetrics(table, siteId, publisherId, eventTime);
    }
The call table.incrementColumnValue hangs. Have to kill the hbase client and the master processes to get around the problem.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-15 21:23:30" id="1523" opendate="2009-06-15 06:14:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>NPE in BaseScanner</summary>
			
			
			<description>ever since HBASE-1522 I get this in master:
2009-06-14 23:09:33,043 ERROR org.apache.hadoop.hbase.master.BaseScanner: Unexpected exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:74)
        at org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:118)
        at org.apache.hadoop.hbase.master.BaseScanner.hasReferences(BaseScanner.java:300)
        at org.apache.hadoop.hbase.master.BaseScanner.cleanupSplits(BaseScanner.java:267)
        at org.apache.hadoop.hbase.master.BaseScanner.scanRegion(BaseScanner.java:229)
        at org.apache.hadoop.hbase.master.MetaScanner.scanOneMetaRegion(MetaScanner.java:73)
        at org.apache.hadoop.hbase.master.MetaScanner.maintenanceScan(MetaScanner.java:129)
        at org.apache.hadoop.hbase.master.BaseScanner.chore(BaseScanner.java:136)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:68)
preventing ROOT/etc from getting assigned. ouch!</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.BaseScanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-17 22:28:07" id="1536" opendate="2009-06-17 20:56:42" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Controlled crash of regionserver not hosting meta/root leaves master in spinning state, regions not reassigned</summary>
			
			
			<description>Testing for HBASE-867 uncovered some nastiness introduced from HBASE-1304 when a regionserver goes down.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-18 23:17:10" id="1541" opendate="2009-06-18 06:10:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Scanning multiple column families in the presence of deleted families results in bad scans</summary>
			
			
			<description>This unit test fails:

  public void testScanner_DeleteOneFamilyNotAnother() throws IOException {

    byte [] tableName = Bytes.toBytes(&quot;test_table&quot;);

    byte [] fam1 = Bytes.toBytes(&quot;columnA&quot;);

    byte [] fam2 = Bytes.toBytes(&quot;columnB&quot;);

    initHRegion(tableName, getName(), fam1, fam2);



    byte [] rowA = Bytes.toBytes(&quot;rowA&quot;);

    byte [] rowB = Bytes.toBytes(&quot;rowB&quot;);



    byte [] value = Bytes.toBytes(&quot;value&quot;);



    Delete delete = new Delete(rowA);

    delete.deleteFamily(fam1);



    region.delete(delete, null, true);



    // now create data.

    Put put = new Put(rowA);

    put.add(fam2, null, value);

    region.put(put);



    put = new Put(rowB);

    put.add(fam1, null, value);

    put.add(fam2, null, value);

    region.put(put);



    Scan scan = new Scan();

    scan.addFamily(fam1).addFamily(fam2);

    InternalScanner s = region.getScanner(scan);

    List&amp;lt;KeyValue&amp;gt; results = new ArrayList&amp;lt;KeyValue&amp;gt;();

    s.next(results);

    assertTrue(Bytes.equals(rowA, results.get(0).getRow()));



    results.clear();

    s.next(results);

    assertTrue(Bytes.equals(rowB, results.get(0).getRow()));



  }


</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.DeleteCompare.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Result.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Delete.java</file>
			
			
			<file type="D">org.apache.hadoop.hbase.thrift.DisabledTestThriftServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Memcache.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestMemcache.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-19 22:02:23" id="1545" opendate="2009-06-19 07:50:08" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>atomicIncrements creating new values with Long.MAX_VALUE</summary>
			
			
			<description>Atomic increment is creating new key values with timestamp of Long.MAX_VALUE.  This is not good, makes it hard to do range queries (as most of Thrift queries are).
</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-19 22:15:10" id="1547" opendate="2009-06-19 08:26:22" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>atomicIncrement doesnt increase hregion.memcacheSize</summary>
			
			
			<description>this prevents flushing!  </description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">1557</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-06-22 17:23:31" id="1561" opendate="2009-06-22 07:50:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HTable Mismatch between javadoc and what it actually does</summary>
			
			
			<description>The code is:
  /** 

Delete all cells that match the passed row and column and whose
timestamp is equal-to or older than the passed timestamp, using an
existing row lock.
@param row Row to update
@param column name of column whose value is to be deleted
@param ts Delete all cells of the same timestamp or older.
@param rl Existing row lock
@throws IOException
@deprecated As of hbase 0.20.0, replaced by 
{@link #delete(Delete)}
   */
  public void deleteAll(final byte [] row, final byte [] column, final long ts,
      final RowLock rl)
  throws IOException 
{

    Delete d = new Delete(row, ts, rl);

    d.deleteColumn(column);

    delete(d);

  }

The code should call deleteColumns() instead.
</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Delete.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Get.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.RowWhileMatchFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.package-info.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-22 18:10:11" id="1558" opendate="2009-06-21 03:42:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>deletes use &amp;apos;HConstants.LATEST_TIMESTAMP&amp;apos; but no one translates that into &amp;apos;now&amp;apos;</summary>
			
			
			<description>Deletes don&amp;amp;apos;t update MAX_TIMESTAMP -&amp;gt; now like puts do.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableInputFormat.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HBaseTestCase.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-22 23:39:26" id="1508" opendate="2009-06-10 08:26:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Shell &quot;close_region&quot; reveals a Master&lt;&gt;HRS problem, regions are not reassigned</summary>
			
			
			<description>When issuing a &quot;close_region&quot; on the shell the Master logs these entries:



...

2009-06-09 22:11:31,141 DEBUG org.apache.hadoop.hbase.master.RegionManager: Applying operation in tasklists to region

2009-06-09 22:11:33,557 DEBUG org.apache.hadoop.hbase.master.HMaster: Attempting to close region: TestTable,0000291328,1244572849139

2009-06-09 22:11:33,560 INFO org.apache.hadoop.hbase.master.HMaster: Marking TestTable,0000291328,1244572849139 as closed on 192.168.2.103:63745; cleaning SERVER + STARTCODE; master will tell regionserver to close region on next heartbeat

2009-06-09 22:11:34,156 DEBUG org.apache.hadoop.hbase.master.RegionManager: Applying operation in tasklists to region

...



But that is it, no further processing is done. The regions stays closed, and even across a restart it stays closed. 
According to what I got told the region should be automatically reassigned to a new server. Please confirm that this is what is expected. If not and the above seems right, then please disregard and close issue.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.BaseScanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-23 16:19:48" id="1568" opendate="2009-06-23 02:24:13" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Client doesnt consult old row filter interface in filterSaysStop() - could result in NPE or excessive scanning</summary>
			
			
			<description>The implementation of HTable.ClientScanner.filterSaysStop() doesnt refer to the old filter, which could result in an NPE if you use an old-style filter.
It also ignores the old style filter, so if you want to use old filters only, you dont get the effect you need.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-24 05:34:47" id="1567" opendate="2009-06-23 02:18:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>cant serialize new filters: </summary>
			
			
			<description>09/06/22 19:14:28 ERROR io.HbaseObjectWritable: Unsupported type interface org.apache.hadoop.hbase.filter.Filter
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: writeClassCode
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: writeObject
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: write
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: writeObject
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: write
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: sendParam
09/06/22 19:14:28 ERROR io.HbaseObjectWritable: call
ooooopsy</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.TestHbaseObjectWritable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-24 06:02:57" id="1576" opendate="2009-06-23 21:00:58" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TIF needs to be able to set scanner caching size for smaller row tables &amp; performance</summary>
			
			
			<description>TIF goes with the default scanner caching size (1).  When each row is processed very fast and is small, this limits the overall performance.  By setting a higher scanner caching level you can achieve 100x+ the performance with the exact same map-reduce and table.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-24 20:03:33" id="1562" opendate="2009-06-22 16:51:37" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>How to handle the setting of 32 bit versus 64 bit machines</summary>
			
			
			<description>After adding the tests to verify the correctness of the HeapSize calculations the question of where to set the type of machines that are in the cluster arose.
I would think that most people are using 64 bit machines but we still need to support the use of 32 bit. So the way I see it we can solve this problem in two ways,
we can either have a settable parameter the the user sets when starting up the cluster or we can try to figure it out ourselves. I think that the second solution would
be the best, to make it as easy as possible on the user. 
That means that we need to add extra sizes to HeapSize and maybe even to Bytes.
Thoughts, comments?</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HLogKey.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.HeapSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.TestHeapSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.ClassSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Put.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.LruHashMap.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">1554</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-06-25 22:12:55" id="1563" opendate="2009-06-22 17:26:07" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>incrementColumnValue does not write to WAL</summary>
			
			
			<description>Incrementing never writes to the WAL.  Under failure scenarios, you will lose all increments since the last flush.
Do we want to expose the option to the client as to whether to write to WAL or not?</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-25 23:56:50" id="1566" opendate="2009-06-23 02:14:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>using Scan(startRow,stopRow) will cause you to iterate the entire table</summary>
			
			
			<description>Right now the only way for the client scanner to know that we are at the &amp;amp;apos;end&amp;amp;apos; of a scan is to client-side-wise use the filter to figure this out. 
This is not easy to fix because the server is unable to indicate the difference between &amp;amp;apos;done with this region&amp;amp;apos;, and &amp;amp;apos;you&amp;amp;apos;re at the end of your scan&amp;amp;apos;.  In both cases we return 0 results, and the client can&amp;amp;apos;t figure out what it means.
Right now the best solution is to use filters, which is tricky since there is no StopRowFilter because that functionality is built in 
We might have to hack the &amp;amp;apos;stop row&amp;amp;apos; functionality as a filter until we can improve the client-server API/RPC.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-25 23:57:37" id="1569" opendate="2009-06-23 07:42:47" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>rare race condition can take down a regionserver. </summary>
			
			
			<description>this happened after &amp;gt; 24 hours of heavy import load on my cluster.  Luckily the shutdown seemed to be clean:
java.lang.IllegalAccessError: Call open first
        at org.apache.hadoop.hbase.regionserver.StoreFile.getReader(StoreFile.java:356)
        at org.apache.hadoop.hbase.regionserver.Store.getStorefilesIndexSize(Store.java:1378)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doMetrics(HRegionServer.java:1075)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:454)
        at java.lang.Thread.run(Thread.java:619)</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-26 00:00:00" id="1560" opendate="2009-06-22 02:24:59" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TIF (and other clients?) cant seem to find one region (getClosestRowBefore issue?)</summary>
			
			
			<description>running a full TIF-mr on a table, it eventually fails, all on 1 of the splits, and all with the same exception set, which is:
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server null for region , row &amp;amp;apos;&amp;amp;apos;, but failed after 10 attempts.
Exceptions:
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:935)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1842)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1790)
	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:369)
	at org.apache.hadoop.hbase.mapred.TableInputFormatBase$TableRecordReader.restart(TableInputFormatBase.java:121)
	at org.apache.hadoop.hbase.mapred.TableInputFormatBase$TableRecordReader.next(TableInputFormatBase.java:222)
	at org.apache.hadoop.hbase.mapred.TableInputFormatBase$TableRecordReader.next(TableInputFormatBase.java:90)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:191)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:175)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:356)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Suspicion: We can&amp;amp;apos;t locate the &amp;amp;apos;root&amp;amp;apos; region with key &amp;amp;apos;&amp;amp;apos; or null.  Probably an issue with getClosestRowBefore.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Scan.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-26 05:26:49" id="1580" opendate="2009-06-24 19:59:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Store scanner does not consult filter.filterRow at end of scan</summary>
			
			
			<description>I have impelemented a columnValueFilter (with new interface) that should filter out the last of two rows in a table. However, I notice that filterRow is only being called on the first row, and the second row is returned.
This patch fixes it, but needs review. My first attempt at adding the call in the DONE_SCAN case did not fix it, but still seems right. The second addition at the end of the method fixed it.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-06-30 02:43:03" id="1591" opendate="2009-06-30 01:04:03" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HBASE-1554 broke org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.testResizeBlockCache</summary>
			
			
			<description>Modification of LRU heap size calculations altered some of the test expectations in the LRU test.

Error Message

expected:&amp;lt;0&amp;gt; but was:&amp;lt;1&amp;gt;



Stacktrace

junit.framework.AssertionFailedError: expected:&amp;lt;0&amp;gt; but was:&amp;lt;1&amp;gt;

	at org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.testResizeBlockCache(TestLruBlockCache.java:435)



Patch coming...</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.TestHeapSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.ClassSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-03 18:41:12" id="1597" opendate="2009-07-01 05:01:28" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Prevent unnecessary caching of blocks during compactions</summary>
			
			
			<description>When running any kind of compaction, we read every block of every storefile being compacted into the block cache.
We would like to reuse any already cached blocks, if available, but otherwise we should not bog down the LRU with unnecessary blocks.
This is not as bad as it was with the old LRU because the latest LRU implementation (HBASE-1460) is scan-resistant.  This ensures that we are not causing massive eviction of the blocks that are being read multiple times or from in-memory tables.  However, this does add to the GC-woes of an import because each block gets further referenced, and for longer periods of time.  There is also overhead in running the LRU evictions.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.HalfHFileReader.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">3976</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-07-03 19:44:33" id="1607" opendate="2009-07-02 20:54:13" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Redo MemStore heap sizing to be accurate, testable, and more like new LruBlockCache</summary>
			
			
			<description>MemStore sizing is inaccurate and does not include all overhead.
I&amp;amp;apos;m going to make it look like the LruBlockCache does.  Will provide a MemStore.heapSize() method that includes ALL overhead of the MemStore itself.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HLog.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.RegionHistorian.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.TestHeapSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.ClassSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.MemStore.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-03 23:53:07" id="1600" opendate="2009-07-01 17:19:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Multiple HBase clients to multiple indpendent HBase clusters from the same jvm should be allowed</summary>
			
			
			<description>ZK quorum servers list is static in ZooKeeperWrapper and is coupled to zoo.cfg. 
This prevents multiple clients from connecting to multiple distinct HBase clusters and requires zoo.cfg to exist on a cluster that needs to connect to a remote HBase cluster.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion/>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Table.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-07 18:10:00" id="1620" opendate="2009-07-07 07:29:40" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Need to use special StoreScanner constructor for major compactions (passed sf, no caching, etc)</summary>
			
			
			<description>Should not cache blocks during major compactions like with minor compactions.
Also, need to only work on passed StoreFiles.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-08 04:04:37" id="1625" opendate="2009-07-07 23:46:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Adding check to Put.add(KeyValue kv), to see that it has the same row as when instantiated</summary>
			
			
			<description>When using the add(KeyValue kv) in Put there is no check to see if the kv has the same row as the row already in the put.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Put.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.TestPut.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-09 17:56:07" id="1635" opendate="2009-07-09 17:48:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>PerformanceEvaluation should use scanner prefetching</summary>
			
			
			<description>Right now default scanner prefetching is set to 1.  In PerformanceEvaluation, this leads to basically benchmarking RPC round-trip performance.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-09 23:05:08" id="1629" opendate="2009-07-08 20:23:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HRS unable to contact master</summary>
			
			
			<description>HRS unable to contact master for initialization after expiration from ZK. Master thinks HRS is still up whereas HRS went down and now cannot restart. The RS logs have a flurry of the following warning messages:
2009-07-08 12:53:19,547 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: Unable to get master for initialization
More logs from the RS and the Master attached.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is related to" type="Reference">1601</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-07-10 19:07:55" id="1627" opendate="2009-07-08 11:43:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TableInputFormatBase#nextKeyValue catches the wrong exception</summary>
			
			
			<description>TableInputFormatBase#nextKeyValue only catches UnknownScannerException from Scanner.next. However, scanner may throw other exceptions:



/* from HTable.ClientScanner#next */

          try {

            values = getConnection().getRegionServerWithRetries(callable);

          } catch (IOException e) {

            if (e instanceof UnknownScannerException &amp;amp;&amp;amp;

                lastNext + scannerTimeout &amp;lt; System.currentTimeMillis()) {

              ScannerTimeoutException ex = new ScannerTimeoutException();

              ex.initCause(e);

              throw ex;

            }

            throw e;

          }





Is there any reason why TIFB does not catch ScannerTimeoutException?</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-10 21:34:36" id="1644" opendate="2009-07-10 21:26:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Result.row is cached in getRow; this breaks MapReduce</summary>
			
			
			<description>In Result#getRow row field is computed (if row is null) and then is cached for further uses. But since MapReduce uses the same Result instance through different map()/reduce() calls, row field is not updated when Result instance changes.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.95.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Result.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-12 23:22:52" id="1646" opendate="2009-07-11 11:26:21" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Scan-s can&amp;apos;t set a Filter</summary>
			
			
			<description>Scan#write:



      HbaseObjectWritable.writeObject(out, this.filter, Filter.class, null);



Because of the third argument (Filter.class), HbaseObjectWritable can not write or read the filter (as Filter is not instantiable).</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Scan.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.TestSerialization.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is cloned by" type="Cloners">1957</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-07-13 18:10:29" id="1649" opendate="2009-07-12 22:36:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ValueFilter may not reset its internal state</summary>
			
			
			<description>ValueFilter#reset is empty even though the class uses two internal variables. These values are reset in filterRow, however there are instances where filterRow may not be called. For example, if you chain two filters through FilterList (with PASS_ALL and the second filter being a ValueFilter) then during FilterList#filterRow if the first filter#filterRow returns true then ValueFilter#filterRow will not be called.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.ValueFilter.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-14 18:10:05" id="1651" opendate="2009-07-14 01:41:06" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>client is broken, it requests ROOT region location from ZK too much</summary>
			
			
			<description>something bad happened to the client, now it requests the ROOT region location literally a hundred times a second:
09/07/13 18:39:59 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:00 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020
09/07/13 18:40:01 DEBUG zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.20.20.158:60020</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.ServerConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-15 00:49:14" id="1659" opendate="2009-07-15 00:34:52" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>merge tool doesnt take binary regions with \x escape format</summary>
			
			
			<description>as per short</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.Merge.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-15 14:28:19" id="1661" opendate="2009-07-15 14:19:12" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HBASE-1215 partial commit broke trunk, does not compile</summary>
			
			
			<description>
compile-core:

    [javac] Compiling 309 source files to /home/hbase20/src/hbase-0.20.0-trunk/build/classes

    [javac] /home/hbase20/src/hbase-0.20.0-trunk/src/java/org/apache/hadoop/hbase/util/FSUtils.java:300: missing return statement

    [javac]   }

    [javac]   ^

    [javac] /home/hbase20/src/hbase-0.20.0-trunk/src/java/org/apache/hadoop/hbase/util/Migrate.java:220: cannot find symbol

    [javac] symbol  : method allMajorCompacted()

    [javac] location: class org.apache.hadoop.hbase.util.Migrate

    [javac]     if (!allMajorCompacted()) {

    [javac]          ^

    [javac] Note: Some input files use or override a deprecated API.

    [javac] Note: Recompile with -Xlint:deprecation for details.

    [javac] Note: Some input files use unchecked or unsafe operations.

    [javac] Note: Recompile with -Xlint:unchecked for details.

    [javac] 2 errors


</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.util.Migrate.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.FSUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-25 16:00:11" id="1702" opendate="2009-07-24 16:58:46" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>TestMergeUtil fails on trunk</summary>
			
			
			<description>Last hudson reports a failed test on TestMergeUtil.  Same failure when run locally.

junit.framework.AssertionFailedError: &amp;amp;apos;merging regions 0 and 1&amp;amp;apos; failed

	at org.apache.hadoop.hbase.util.TestMergeTool.mergeAndVerify(TestMergeTool.java:178)

	at org.apache.hadoop.hbase.util.TestMergeTool.testMergeTool(TestMergeTool.java:253)


</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.util.MetaUtils.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-27 17:22:03" id="1686" opendate="2009-07-23 00:39:27" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>major compaction can create empty store files, causing AIOOB when trying to read</summary>
			
			
			<description>here is the backtrace:
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.getFirstKey(HFile.java:991)
        at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.getStoreFile(StoreFileGetScan.java:84)
        at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.get(StoreFileGetScan.java:65)
        at org.apache.hadoop.hbase.regionserver.Store.get(Store.java:1548)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2263)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2252)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1739)
This can happen if your table only has deletes, and everything evaporates during a major compaction.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFileGetScan.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-27 23:48:34" id="1714" opendate="2009-07-27 22:30:20" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>convenience functions in Scan and the thrift API along with a few other bug fixes</summary>
			
			
			<description>a number of handy things i&amp;amp;apos;ve added to my own repo recently</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.DeleteCompare.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Delete.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-28 08:51:31" id="1703" opendate="2009-07-24 20:43:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ICVs across /during a flush can cause multiple keys with the same TS (bad)</summary>
			
			
			<description>We noticed a bug whereby the value in a hbase ICV&amp;amp;apos;ed counter was lower, and the bug turned out to be that during a flush, the ICV will grab the KeyValue from &amp;amp;apos;memcache&amp;amp;apos; and reuse the timestamp... if we grab the KeyValue from the snapshot we end up with 2 key values, one in memcache, one in a hfile, both with the same timestamp, but one with a lower value than the other.
The fix is to not reuse timestamps if they come out of the snapshot.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.MemStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.QueryMatcher.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-28 20:12:17" id="1717" opendate="2009-07-28 19:36:38" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Put on client-side uses passed-in byte[]s rather than always using copies</summary>
			
			
			<description>During review of Put with ryan, found that we are using a passed in reference to family in add() rather than a local copy.  If the backing array changed values, this could cause trouble.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Put.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-28 20:29:34" id="1647" opendate="2009-07-11 12:04:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Filter#filterRow is called too often, filters rows it shouldn&amp;apos;t have</summary>
			
			
			<description>Filter#filterRow is called from ScanQueryMatcher#filterEntireRow which is called from StoreScanner.next. However, if I understood the code correctly, StoreScanner processes KeyValue-s in a column-oriented order (i.e. after row1-col1 comes row2-col1, not row1-col2). Thus, when filterEntireRow is called, in reality, the filter only processed (via filterKeyValue) only one column of a row.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.QueryMatcher.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-07-29 00:59:53" id="1718" opendate="2009-07-28 23:46:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Reuse of KeyValue during log replay could cause the wrong data to be used</summary>
			
			
			<description>Our meta table got a row key of METAROW in it.  Hard to explain how it happened, but under code inspection stack found that we are reusing the same KV instance for each replayed key.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-08-17 22:22:04" id="1738" opendate="2009-08-03 23:15:40" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Scanner doesnt reset when a snapshot is created, could miss new updates into the &amp;apos;kvset&amp;apos; (active part)</summary>
			
			
			<description>when a Scanner is created, it creates 2 MemStoreScanners on the kvset and the snapshot (internal names of Memstore)... if the snapshot is originally empty, it only creates the 1, for kvset.  When the snapshot is created, the outstanding Scanners now have a pointer to the tree that is now the snapshot, but no pointer to the kvset.
When the flush completes, the scanner will reset the memstore scanners and &amp;amp;apos;see&amp;amp;apos; the new values again.
If there is a large delay between snapshot and finalization of the flush, there can be a large period of time a scanner doesnt see &amp;amp;apos;new&amp;amp;apos; values that are being inserted. the canonical &amp;amp;apos;bad&amp;amp;apos; case where this can do things is the META scanner, and we end up with double assignment.
The snapshot is really lightweight, it only takes out a small lock in memstore, so im not sure there is an easy mechanism to hook to without building out a bit more code or restructuring the memstore scanner.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.TestHeapSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.MemStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.util.ClassSize.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-08-25 18:33:27" id="1791" opendate="2009-08-25 16:27:57" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Timeout in IndexRecordWriter</summary>
			
			
			<description>A MapReduce job to generate Lucene Indexes from HBase will fail on sufficiently large tables. After the indexing finished, the close() method of IndexRecordWriter is called.  The  writer.optimize() call in this method can take many minutes, forcing most MapReduce tasks to timeout. There is a HeartBeatsThread, but it does not seem to send progress updates. 
A suggested fix may be to add context.progress(); in the HeardbeatsThread run() method, after the context.setStatus call. Not sure why context.setStatus is not &quot;good enough&quot;. </description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.IndexRecordWriter.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-08-25 23:18:28" id="1737" opendate="2009-08-03 21:10:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Regions unbalanced when adding new node</summary>
			
			
			<description>When adding a new RegionServer to a cluster, the new RS will receive some regions but not enough to actually be considered balanced.
To recreate, just take an RS offline, allow regions to be reassigned, and then bring it back up.
Master will get itself into a broken, stuck state where it continuously outputs a line like this:

2009-08-03 12:54:57,812 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server dn4,60020,1249329081079 will be unloaded for balance. Server load: 341 avg: 318.0, regions can be moved: 55



This line is output every 3 seconds and never stops until another RS joins/leaves the cluster.
Making this a blocker because when your new RS only gets some regions (in my case, about half as many as it should have), then all new regions will be assigned to that RS.  This basically destroys any possibility for good load distribution with new data.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-08-26 00:05:08" id="1793" opendate="2009-08-25 19:37:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>[Regression] HTable.get/getRow with a ts is broken</summary>
			
			
			<description>If using the old API with 0.20, the behavior of get and getRow is changed when setting a timestamp. Previously, setting a ts was working like a time range and now it works like an exact time.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-08-26 00:05:57" id="1792" opendate="2009-08-25 18:32:19" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>[Regression] Cannot save timestamp in the future</summary>
			
			
			<description>0.20, compared to previous versions, doesn&amp;amp;apos;t let you save with a timestamp in the future and will set it to current time without telling you. This is really bad for users upgrading to 0.20 that were using those timestamps.
Example:
 hbase(main):004:0&amp;gt; put &amp;amp;apos;testtable&amp;amp;apos;, &amp;amp;apos;r1&amp;amp;apos;, &amp;amp;apos;f1:c1&amp;amp;apos;, &amp;amp;apos;val&amp;amp;apos;, 5373965335336911168
 0 row(s) in 0.0070 seconds
 hbase(main):005:0&amp;gt; scan &amp;amp;apos;testtable&amp;amp;apos;
 ROW                          COLUMN+CELL                                                                      
  r1                          column=f1:c1, timestamp=1251223892010, value=val                                 
 1 row(s) in 0.0380 seconds</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-08-27 00:58:09" id="1798" opendate="2009-08-26 18:20:48" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>[Regression] Unable to delete a row in the future</summary>
			
			
			<description>Deleting in the future doesn&amp;amp;apos;t work because KV resets everything to now.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-01 19:56:20" id="1784" opendate="2009-08-21 19:52:02" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Missing rows after medium intensity insert</summary>
			
			
			<description>This bug was uncovered by Mathias in his mail &quot;Issue on data load with 0.20.0-rc2&quot;. Basically, somehow, after a medium intensity insert a lot of rows goes missing. Easy way to reproduce : PE. Doing a PE scan or randomRead afterwards won&amp;amp;apos;t uncover anything since it doesn&amp;amp;apos;t bother about null rows. Simply do a count in the shell, easy to test (I changed my scanner caching in the shell to do it faster).
I tested some light insertions with force flush/compact/split in the shell and it doesn&amp;amp;apos;t break.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.BaseScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ServerManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-02 16:40:26" id="1810" opendate="2009-09-02 10:39:54" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ConcurrentModificationException in region assignment</summary>
			
			
			<description>


2009-09-01 11:28:24,106 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 16 on 60000, call regionServerReport(address: 192.168.105.39:60020, startcode: 1251828463043, load: (requests=0, regions=21, usedHeap=135, maxHeap=4093), [Lorg.apache.hadoop.hbase.HMsg;@6556c280, [Lorg.apache.hadoop.hbase.HRegionInfo;@22fb957a) from 192.168.105.39:60281: error: java.io.IOException: java.util.ConcurrentModificationException

java.io.IOException: java.util.ConcurrentModificationException

        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)

        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)

        at org.apache.hadoop.hbase.master.RegionManager.isMetaServer(RegionManager.java:837)

        at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:405)

        at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:202)

        at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:481)

        at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:415)

        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:324)

        at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:722)

        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

        at java.lang.reflect.Method.invoke(Method.java:597)

        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)

        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)



and 



2009-09-01 11:28:25,713 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 23 on 60000, call regionServerReport(address: 192.168.105.37:60020, startcode: 1251828463132, load: (requests=0, regions=14, usedHeap=141, maxHeap=4093), [Lorg.apache.hadoop.hbase.HMsg;@68345260, [Lorg.apache.hadoop.hbase.HRegionInfo;@430c5212) from 192.168.105.37:35432: error: java.io.IOException: java.util.ConcurrentModificationException

java.io.IOException: java.util.ConcurrentModificationException

        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)

        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)

        at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:428)

        at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:202)

        at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:481)

        at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:415)

        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:324)

        at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:722)

        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

        at java.lang.reflect.Method.invoke(Method.java:597)

        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)

        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)



As discussed with Mathias on IRC:
&amp;lt;hbs&amp;gt; regionsInTransition is a synchronized map so accesses to it are synchronized but not looping over it
&amp;lt;hbs&amp;gt; as stated in http://java.sun.com/j2se/1.5.0/docs/api/java/util/Collections.html#synchronizedMap%28java.util.Map%29
&amp;lt;hbs&amp;gt; so there is a missing synchronized(regionsInTransition) wherever regionsInTransition is iterated over.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-02 18:58:51" id="1804" opendate="2009-08-31 19:52:15" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Puts are permitted (and stored) when including an appended colon</summary>
			
			
			<description>If I have a table with family &quot;testFamily&quot;, currently I can do Puts using the new API by specifying the family name with or without a colon.  The KV is then stored w/ or w/o depending on how the Put was done.
If you try to Put.add(&quot;testFamily:&quot;, &quot;qualifier&quot;, &quot;value&quot;) this should throw a NoSuchColumnFamilyException</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HTableDescriptor.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-02 23:24:23" id="1790" opendate="2009-08-24 07:24:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>filters are not working correctly</summary>
			
			
			<description>Filters used in Scanning the table are not working correctly. For example a table with three rows:
1. rowkey = adminbackslash-nb0, desc:temp = &quot;temp&quot;
2. rowkey = adminbackslash-nb1, desc:temp = &quot;temp&quot;
3. rowkey = adminkleptoman, desc:temp = &quot;temp&quot;
If I scan all rows in the table without filter I get all the rows as expected. But applying a simple prefixfilter with parameter &quot;adminbackslash&quot; will return only first row. I searched it down to HRegion::nextInternal method, which will not output one passed row before denied row(by filter). </description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.0, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.PrefixFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.TestInclusiveStopFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.PageFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.ValueFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.TestValueFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.InclusiveStopFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.TestFilterList.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.TestPageFilter.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="incorporates" type="Incorporates">1353</link>
			
			
			<link description="incorporates" type="Incorporates">1710</link>
			
			
			<link description="incorporates" type="Incorporates">1807</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-09-04 05:02:17" id="1779" opendate="2009-08-19 09:36:35" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ThriftServer logged error if getVer() result is empty</summary>
			
			
			<description>Null pointer exception is logged by thrift server process if a client calls getVer() through thrift server and its result is empty.
The easiest fix is to check if result is empty or not.
09/08/18 15:58:30 ERROR server.TThreadPoolServer: Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.getVer(ThriftServer.java:281)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.getVer(ThriftServer.java:269)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$getVer.process(Hbase.java:2096)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor.process(Hbase.java:1859)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-11 19:53:39" id="1795" opendate="2009-08-26 01:02:32" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>log recovery doesnt reset the max sequence id, new logfiles can get tossed as &amp;apos;duplicates&amp;apos;</summary>
			
			
			<description>during log recovery, we dont reset Store.maxSeqId, thus new log entries are stamped starting off from the old files.  This can cause a problem if we fail and recover again, since the new mutations are deemed &quot;old&quot; and shouldnt be applied in a subsequent recovery scenario.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-11 19:54:04" id="1794" opendate="2009-08-25 23:36:26" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>recovered log files are not inserted into the storefile map</summary>
			
			
			<description>after a log recovery, the resulting flushed file is not introduced into the store.storefiles map. The new data isnt available until the region is closed or compacted.
</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-11 21:46:31" id="1740" opendate="2009-08-04 00:00:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ICV has a subtle race condition only visible under high load</summary>
			
			
			<description>ICV demonstrates a race condition under high load.  The result is a duplicate KeyValue with the same timestamp, at first in the memcache, and in hfile, then both in hfile.  The get/scan code doesnt know which one to read, and picks one arbitrarily.  One of the keyvalues is correct, one is incorrect.
What happens at a deeper level:

we start an ICV
a snapshot happens and moves the memstore to the snapshot
the ICV code puts a key-value into memstore that has the same timestamp as a keyvalue in the snapshot.

This is a deep race condition and several attempts to fix it failed in production here at SU.  This issue is about a more permanent fix.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.MemStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-12 01:02:39" id="1828" opendate="2009-09-11 03:10:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>CompareFilters are broken from client-side</summary>
			
			
			<description>Some filters pass region-level tests but seem to freeze client-side.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.TestClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.CompareFilter.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-15 20:23:27" id="1840" opendate="2009-09-15 19:29:23" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>RowLock fails when used with IndexTable</summary>
			
			
			<description>The following exception is thrown when using RowLock to update a row in an IndexedTable:
 [junit] java.io.IOException: java.io.IOException: Invalid row lock
[junit] at org.apache.hadoop.hbase.regionserver.HRegion.getLock(HRegion.java:1640)
[junit] at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1244)
[junit] at org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.put(IndexedRegion.java:97)
[junit] at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1216)
[junit] at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1818)
[junit] at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
[junit] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[junit] at java.lang.reflect.Method.invoke(Method.java:597)
[junit] at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
[junit] at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
NOTE #1: Line numbers in stacktrace may not make sense because I&amp;amp;apos;ve been hacking in loads of debug info. 
NOTE #2: I attaching a fix which includes unit tests</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.tableindexed.TestIndexedTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-15 20:30:26" id="1574" opendate="2009-06-23 18:51:37" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Client and server APIs to do batch deletes.</summary>
			
			
			<description>in 880 there is no way to do a batch delete (anymore?).  We should add one back in.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.TestClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnection.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Delete.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Put.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-17 21:17:18" id="1847" opendate="2009-09-17 00:44:04" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Delete latest of a null qualifier when non-null qualifiers exist throws a RuntimeException</summary>
			
			
			<description>Bug in delete latest code when deleting the null qualifier column when other columns also exist.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.TestClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-22 16:34:45" id="1815" opendate="2009-09-03 22:25:49" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HBaseClient can get stuck in an infinite loop while attempting to contact a failed regionserver</summary>
			
			
			<description>While using HBase Thrift server, if a regionserver goes down due to shutdown or failure clients will timeout because the thrift server cannot contact the dead regionserver.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.ipc.HBaseRPC.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-22 19:23:17" id="1857" opendate="2009-09-22 16:25:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>WrongRegionException when setting region online after .META. split</summary>
			
			
			<description>After splitting .META. when updating region information in .META. (e.g. ProcessRegionOpen) the wrong .META. region was retrieved in RegionManager from onlineMetaRegions map. 
This is due to a bug in RegionManager.getFirstMetaRegionForRegion that was using the wrong key to get data out of the map (the table name instead of the region name) 
return onlineMetaRegions.get(onlineMetaRegions.headMap(newRegion.getTableDesc().getName()).lastKey());
and when adding the region to the map it was added with 
onlineMetaRegions.put(metaRegion.getStartKey(), metaRegion);
so it&amp;amp;apos;s supposed to be taken out with: 
return onlineMetaRegions.get(onlineMetaRegions.headMap(newRegion.getRegionName()).lastKey());</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionManager.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-24 23:06:26" id="1865" opendate="2009-09-24 21:02:25" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>0.20.0 TableInputFormatBase NPE</summary>
			
			
			<description>Spot the bug in this code:
public List&amp;lt;InputSplit&amp;gt; getSplits(JobContext context) throws IOException {
    byte [][] startKeys = table.getStartKeys();
    if (startKeys == null || startKeys.length == 0) 
{

      throw new IOException(&quot;Expecting at least one region.&quot;);

    }
    if (table == null) 
{

      throw new IOException(&quot;No table was provided.&quot;);

    }
...
}
Should check if the table is null before calling a method on it.
Admittedly, this isn&amp;amp;apos;t the worst bug in the world, it&amp;amp;apos;s really just more of a nuisance in that the &quot;No table was provided&quot; message becomes an NPE
This bug is in both
org.apache.hadoop.hbase.mapred.TableInputFormatBase
org.apache.hadoop.hbase.mapreduce.TableInputFormatBase</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-24 23:20:52" id="1866" opendate="2009-09-24 22:49:49" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Scan(Scan) copy constructor does not copy value of cacheBlocks</summary>
			
			
			<description/>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Scan.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-25 23:22:21" id="1869" opendate="2009-09-25 19:46:24" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>IndexedTable delete fails when used in conjunction with RowLock()</summary>
			
			
			<description>Created the following test in TestIndexedTable,
  public void testLockedRowDelete() throws IOException 
{

    writeInitalRows();

    // Delete the first row;

    byte[] row = PerformanceEvaluation.format(0);

    RowLock lock = table.lockRow(row);

    table.delete(new Delete(row, HConstants.LATEST_TIMESTAMP, lock));

    table.unlockRow(lock);    



    assertRowDeleted(NUM_ROWS - 1);  

  }
}
which fails and throws the following exception,
java.io.IOException: java.io.IOException: Invalid row lock
	at org.apache.hadoop.hbase.regionserver.HRegion.getLock(HRegion.java:1621)
	at org.apache.hadoop.hbase.regionserver.HRegion.delete(HRegion.java:1094)
	at org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.delete(IndexedRegion.java:269)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:2014)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:648)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Patch coded for the latest version in SVN (looks like 0.21.0) , just going through final testing and packaging. Will attach shortly.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.tableindexed.TestIndexedTable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-09-26 21:23:34" id="1871" opendate="2009-09-26 20:23:19" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Wrong type used in TableMapReduceUtil.initTableReduceJob()</summary>
			
			
			<description>Since we changed it so that TableOutputFormat can handle Put and Delete it is necessary to set the output value class to Writable.



  public static void initTableReducerJob(String table,

    Class&amp;lt;? extends TableReducer&amp;gt; reducer, Job job, Class partitioner)

  throws IOException {

    job.setOutputFormatClass(TableOutputFormat.class);

    if (reducer != null) job.setReducerClass(reducer);

    job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, table);

    job.setOutputKeyClass(ImmutableBytesWritable.class);

    job.setOutputValueClass(Put.class);

   ....



The last line should be 



    job.setOutputValueClass(Writable.class);


</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-10-03 05:07:56" id="1883" opendate="2009-10-02 22:14:10" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HRegion passes the wrong minSequenceNumber to doReconstructionLog</summary>
			
			
			<description>HRegion initializes by opens up all store files which may recover from the WAL. It then calls protected doReconstructionLog which THBase uses to go through the log and look for pending transactions that may need to be recovered. Currently HRegion is passing down the minSequenceNumber after WAL recovery. What we want is the lowest sequence number before the wal recovery.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-10-03 05:17:31" id="1879" opendate="2009-10-01 21:03:17" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ReadOnly transactions generate WAL activity.</summary>
			
			
			<description>Currently we write a start entry in the WAL each time a transaction starts, and a commit/abort at the end of the transaction. This means read-only transactions unnecessarily generate two WAL entries per region.
Can avoid this by removing the start entry from the WAL (this is implicit in the first trx OP entry we see), and only writing commit/abort when the transaction has a write.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.transactional.TestTHLog.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.transactional.THLog.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.transactional.THLogRecoveryManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.transactional.THLogKey.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-10-06 03:26:26" id="1831" opendate="2009-09-12 01:06:29" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Scanning API must be reworked to allow for fully functional Filters client-side</summary>
			
			
			<description>Right now, a client replays part of the Filter locally by calling filterRowKey() and filterAllRemaining() to determine whether it should continue to the next region.
A number of new filters rely on filterKeyValue() and other calls to alter state.  It&amp;amp;apos;s also a false assumption that all rows/keys affecting a filter returning true for FAR will be seen client-side (what about those that failed the filter).
This issue is about dealing with Filters properly from the client-side.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.1, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.TestClient.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.HRegion.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.package-info.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.ScannerCallable.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTable.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-10-18 16:30:46" id="1912" opendate="2009-10-16 10:08:13" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>When adding a secondary index to an existing table, it will cause NPE during re-indexing. </summary>
			
			
			<description>When adding a secondary index to an existing table, an IndexSpecification must be constructed.
If we construct a simple index using the following constructor: IndexSpecification(String indexId, byte[] indexedColumn), then the program will cause NPE during re-indexing. 
Exception in thread &quot;main&quot; java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.tableindexed.IndexMaintenanceUtils.createIndexUpdate(IndexMaintenanceUtils.java:57)
        at org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.reIndexTable(IndexedTableAdmin.java:144)
        at org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.addIndex(IndexedTableAdmin.java:132)
        at MyIndexedTable.addSecondaryIndexToExistingTable(MyIndexedTable.java:256)
        at MyIndexedTable.main(MyIndexedTable.java:276)</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.2, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.tableindexed.IndexSpecification.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-10-21 23:38:03" id="1777" opendate="2009-08-18 19:10:11" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>column length is not checked before saved to memstore</summary>
			
			
			<description>I added some debuging to line 511 in HFile.java and found that the column is causing my problem it was &amp;gt; max size
we should check this before saving the record to memstore
As of 0.20.0-RC2 the server dies and cause the hlogs to be read again by the next region server that gets the region in the end it cause the whole cluster to go down sense the bad data is in the hlog at this point.



2009-08-18 12:54:16,572 FATAL 

org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog 

required. Forcing server shutdown

org.apache.hadoop.hbase.DroppedSnapshotException: region: 

webdata,http:\x2F\x2Fanaal-genomen.isporno.nl\x2F,1250569930062

        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:950)

        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:843)

        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:241)

        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)

Caused by: java.io.IOException: Key length 183108 &amp;gt; 65536

        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkKey(HFile.java:511)

        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:479)

        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:447)

        at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:525)

        at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:489)

        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:935)

        ... 3 more


</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.2, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.io.hfile.HFile.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-10-23 20:00:30" id="1930" opendate="2009-10-22 20:39:49" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Put.setTimeStamp misleading (doesn&amp;apos;t change timestamp on existing KeyValues, not copied in copy constructor)</summary>
			
			
			<description>In the process of migrating some code from 0.19, and was changing BatchUpdate&amp;amp;apos;s to Put&amp;amp;apos;s.  I was hit by a bit of a gotcha.  In the old code, I populated the BatchUpdate, then set the timestamp.  However, this doesn&amp;amp;apos;t wotk for Put, because Put creates KeyValue&amp;amp;apos;s with the currently set timestamp when adding values.  Setting the timestamp at the end has no effect.  Also, the copy constructor doesn&amp;amp;apos;t copy the timestamp (or writeToWAL) setting.
One option would be to simply update the javadoc to make it clear that the timestamp needs to be set prior to adding values.  I&amp;amp;apos;m attaching a proposed patch which moves the timestamp setting to constructor only so that it isn&amp;amp;apos;t possible to trigger the confusing case at all.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HBaseTestCase.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.TimestampTestBase.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.TestGetRowVersions.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Put.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.TestScanMultipleVersions.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">1941</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-11-04 05:25:22" id="1781" opendate="2009-08-20 16:41:14" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Weird behavior of WildcardColumnTracker.checkColumn(), looks like recursive loop</summary>
			
			
			<description>I got a weird error twice on a MR job which eventually though completed



...

09/08/19 11:28:31 INFO mapreduce.TableInputFormatBase: split: 2591-&amp;gt;foo.bar.net:1fff99f02088fe,1fffdcbdb0476b

09/08/19 11:28:31 INFO mapreduce.TableInputFormatBase: split: 2592-&amp;gt;foo.bar.net:1fffdcbdb0476b,

09/08/19 11:28:31 INFO mapred.JobClient: Running job: job_200908120615_0015

09/08/19 11:28:32 INFO mapred.JobClient:  map 0% reduce 0%

09/08/19 11:35:43 INFO mapred.JobClient:  map 1% reduce 0%

09/08/19 11:39:53 INFO mapred.JobClient:  map 2% reduce 0%

09/08/19 11:42:58 INFO mapred.JobClient:  map 3% reduce 0%

09/08/19 11:47:02 INFO mapred.JobClient:  map 4% reduce 0%

09/08/19 11:50:41 INFO mapred.JobClient:  map 5% reduce 0%

09/08/19 11:54:25 INFO mapred.JobClient:  map 6% reduce 0%

09/08/19 11:58:31 INFO mapred.JobClient:  map 7% reduce 0%

09/08/19 12:02:36 INFO mapred.JobClient:  map 8% reduce 0%

09/08/19 12:06:12 INFO mapred.JobClient:  map 9% reduce 0%

09/08/19 12:10:01 INFO mapred.JobClient:  map 10% reduce 0%

09/08/19 12:13:40 INFO mapred.JobClient:  map 11% reduce 0%

09/08/19 12:17:04 INFO mapred.JobClient:  map 12% reduce 0%

09/08/19 12:21:07 INFO mapred.JobClient:  map 13% reduce 0%

09/08/19 12:24:46 INFO mapred.JobClient:  map 14% reduce 0%

09/08/19 12:29:27 INFO mapred.JobClient:  map 15% reduce 0%

09/08/19 12:33:42 INFO mapred.JobClient:  map 16% reduce 0%

09/08/19 12:38:04 INFO mapred.JobClient:  map 17% reduce 0%

09/08/19 12:44:16 INFO mapred.JobClient:  map 18% reduce 0%

09/08/19 12:50:20 INFO mapred.JobClient:  map 19% reduce 0%

09/08/19 12:55:44 INFO mapred.JobClient:  map 20% reduce 0%

09/08/19 13:01:11 INFO mapred.JobClient:  map 21% reduce 0%

09/08/19 13:06:21 INFO mapred.JobClient:  map 22% reduce 0%

09/08/19 13:11:24 INFO mapred.JobClient:  map 23% reduce 0%

09/08/19 13:15:39 INFO mapred.JobClient:  map 24% reduce 0%

09/08/19 13:20:33 INFO mapred.JobClient:  map 25% reduce 0%

09/08/19 13:25:58 INFO mapred.JobClient:  map 26% reduce 0%

09/08/19 13:29:52 INFO mapred.JobClient:  map 27% reduce 0%

09/08/19 13:34:44 INFO mapred.JobClient:  map 28% reduce 0%

09/08/19 13:38:19 INFO mapred.JobClient:  map 29% reduce 0%

09/08/19 13:41:53 INFO mapred.JobClient:  map 30% reduce 0%

09/08/19 13:45:09 INFO mapred.JobClient:  map 31% reduce 0%

09/08/19 13:49:06 INFO mapred.JobClient:  map 32% reduce 0%

09/08/19 13:52:47 INFO mapred.JobClient:  map 33% reduce 0%

09/08/19 13:56:37 INFO mapred.JobClient:  map 34% reduce 0%

09/08/19 13:59:48 INFO mapred.JobClient:  map 35% reduce 0%

09/08/19 14:04:14 INFO mapred.JobClient:  map 36% reduce 0%

09/08/19 14:09:32 INFO mapred.JobClient:  map 37% reduce 0%

09/08/19 14:14:00 INFO mapred.JobClient:  map 38% reduce 0%

09/08/19 14:17:42 INFO mapred.JobClient:  map 39% reduce 0%

09/08/19 14:21:50 INFO mapred.JobClient:  map 40% reduce 0%

09/08/19 14:25:17 INFO mapred.JobClient:  map 41% reduce 0%

09/08/19 14:28:57 INFO mapred.JobClient:  map 42% reduce 0%

09/08/19 14:33:03 INFO mapred.JobClient:  map 43% reduce 0%

09/08/19 14:36:51 INFO mapred.JobClient:  map 44% reduce 0%

09/08/19 14:40:49 INFO mapred.JobClient:  map 45% reduce 0%

09/08/19 14:44:44 INFO mapred.JobClient:  map 46% reduce 0%

09/08/19 14:48:37 INFO mapred.JobClient:  map 47% reduce 0%

09/08/19 14:52:15 INFO mapred.JobClient:  map 48% reduce 0%

09/08/19 14:55:57 INFO mapred.JobClient:  map 49% reduce 0%

09/08/19 14:59:21 INFO mapred.JobClient:  map 50% reduce 0%

09/08/19 15:02:58 INFO mapred.JobClient:  map 51% reduce 0%

09/08/19 15:07:23 INFO mapred.JobClient:  map 52% reduce 0%

09/08/19 15:10:19 INFO mapred.JobClient:  map 53% reduce 0%

09/08/19 15:13:19 INFO mapred.JobClient:  map 54% reduce 0%

09/08/19 15:16:38 INFO mapred.JobClient:  map 55% reduce 0%

09/08/19 15:19:36 INFO mapred.JobClient:  map 56% reduce 0%

09/08/19 15:22:41 INFO mapred.JobClient:  map 57% reduce 0%

09/08/19 15:25:35 INFO mapred.JobClient:  map 58% reduce 0%

09/08/19 15:30:07 INFO mapred.JobClient:  map 59% reduce 0%

09/08/19 15:37:41 INFO mapred.JobClient:  map 60% reduce 0%

09/08/19 15:42:04 WARN zookeeper.ClientCnxn: Exception closing session 0x422d8cc8cbb310e to sun.nio.ch.SelectionKeyImpl@4737371

java.io.IOException: TIMED OUT

        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:858)

09/08/19 15:42:05 INFO zookeeper.ClientCnxn: Attempting connection to server tr-bt-dal-03/10.12.205.194:2181

09/08/19 15:42:05 INFO zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/10.16.182.238:41125 remote=tr-bt-dal-03/10.12.205.194:2181]

09/08/19 15:42:05 INFO zookeeper.ClientCnxn: Server connection successful

09/08/19 15:43:08 INFO mapred.JobClient:  map 61% reduce 0%

09/08/19 15:49:24 INFO mapred.JobClient:  map 62% reduce 0%

09/08/19 15:54:04 INFO mapred.JobClient:  map 63% reduce 0%

09/08/19 15:57:01 INFO mapred.JobClient:  map 64% reduce 0%

09/08/19 16:00:42 INFO mapred.JobClient:  map 65% reduce 0%

09/08/19 16:03:20 INFO mapred.JobClient:  map 66% reduce 0%

09/08/19 16:06:53 INFO mapred.JobClient:  map 67% reduce 0%

09/08/19 16:09:20 INFO mapred.JobClient:  map 68% reduce 0%

09/08/19 16:12:02 INFO mapred.JobClient:  map 69% reduce 0%

09/08/19 16:14:35 INFO mapred.JobClient:  map 70% reduce 0%

09/08/19 16:17:26 INFO mapred.JobClient:  map 71% reduce 0%

09/08/19 16:20:19 INFO mapred.JobClient:  map 72% reduce 0%

09/08/19 16:23:14 INFO mapred.JobClient:  map 73% reduce 0%

09/08/19 16:25:56 INFO mapred.JobClient:  map 74% reduce 0%

09/08/19 16:30:16 INFO mapred.JobClient:  map 75% reduce 0%

09/08/19 16:34:14 INFO mapred.JobClient:  map 76% reduce 0%

09/08/19 16:37:45 INFO mapred.JobClient:  map 77% reduce 0%

09/08/19 16:41:38 INFO mapred.JobClient:  map 78% reduce 0%

09/08/19 16:44:33 INFO mapred.JobClient:  map 79% reduce 0%

09/08/19 16:47:54 INFO mapred.JobClient:  map 80% reduce 0%

09/08/19 16:52:20 INFO mapred.JobClient:  map 81% reduce 0%

09/08/19 16:56:44 INFO mapred.JobClient:  map 82% reduce 0%

09/08/19 17:00:50 INFO mapred.JobClient:  map 83% reduce 0%

09/08/19 17:02:57 INFO mapred.JobClient: Task Id : attempt_200908120615_0015_m_002231_0, Status : FAILED

org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row &amp;amp;apos;7c78ca95-4d82-420f-ab2f-495fe139bad0&amp;amp;apos;, but failed after 10 attempts.

Exceptions:

java.io.IOException: java.io.IOException: java.lang.StackOverflowError

        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:847)

        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:837)

        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1770)

        at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

        at java.lang.reflect.Method.invoke(Method.java:597)

        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)

        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)

Caused by: java.lang.StackOverflowError

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:136)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracke

09/08/19 17:04:01 INFO mapred.JobClient:  map 84% reduce 0%

09/08/19 17:07:16 INFO mapred.JobClient:  map 85% reduce 0%

09/08/19 17:10:38 INFO mapred.JobClient:  map 86% reduce 0%

09/08/19 17:13:44 INFO mapred.JobClient:  map 87% reduce 0%

09/08/19 17:16:12 INFO mapred.JobClient:  map 88% reduce 0%

09/08/19 17:19:29 INFO mapred.JobClient:  map 89% reduce 0%

09/08/19 17:23:00 INFO mapred.JobClient:  map 90% reduce 0%

09/08/19 17:29:25 INFO mapred.JobClient: Task Id : attempt_200908120615_0015_m_002231_1, Status : FAILED

org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row &amp;amp;apos;7c78ca95-4d82-420f-ab2f-495fe139bad0&amp;amp;apos;, but failed after 10 attempts.

Exceptions:

java.io.IOException: java.io.IOException: java.lang.StackOverflowError

        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:847)

        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:837)

        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1770)

        at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

        at java.lang.reflect.Method.invoke(Method.java:597)

        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)

        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)

Caused by: java.lang.StackOverflowError

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:136)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracke

09/08/19 17:31:22 INFO mapred.JobClient:  map 91% reduce 0%

09/08/19 17:43:27 INFO mapred.JobClient:  map 92% reduce 0%

09/08/19 17:57:27 INFO mapred.JobClient:  map 93% reduce 0%

09/08/19 18:09:43 INFO mapred.JobClient:  map 94% reduce 0%

09/08/19 18:19:16 INFO mapred.JobClient:  map 95% reduce 0%

09/08/19 18:21:49 INFO mapred.JobClient: Task Id : attempt_200908120615_0015_m_002231_2, Status : FAILED

org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row &amp;amp;apos;7c78ca95-4d82-420f-ab2f-495fe139bad0&amp;amp;apos;, but failed after 10 attempts.

Exceptions:

java.io.IOException: java.io.IOException: java.lang.StackOverflowError

        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:847)

        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:837)

        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1770)

        at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

        at java.lang.reflect.Method.invoke(Method.java:597)

        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)

        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)

Caused by: java.lang.StackOverflowError

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:136)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.checkColumn(WildcardColumnTracker.java:158)

        at org.apache.hadoop.hbase.regionserver.WildcardColumnTracke

09/08/19 18:31:22 INFO mapred.JobClient:  map 96% reduce 0%

09/08/19 18:42:54 INFO mapred.JobClient:  map 97% reduce 0%

09/08/19 18:43:02 INFO mapred.JobClient: Job complete: job_200908120615_0015

09/08/19 18:43:02 INFO mapred.JobClient: Counters: 11

09/08/19 18:43:02 INFO mapred.JobClient:   Job Counters

09/08/19 18:43:02 INFO mapred.JobClient:     Rack-local map tasks=30

09/08/19 18:43:02 INFO mapred.JobClient:     Launched map tasks=2530

09/08/19 18:43:02 INFO mapred.JobClient:     Data-local map tasks=2500

09/08/19 18:43:02 INFO mapred.JobClient:     Failed map tasks=1

09/08/19 18:43:02 INFO mapred.JobClient:   net.foo.hadoop.hbase.mapred.SetCurrentData$SetCurrentMapper$Counters

09/08/19 18:43:02 INFO mapred.JobClient:     ROWS=22692113

09/08/19 18:43:02 INFO mapred.JobClient:     NETWORK=101481740

09/08/19 18:43:02 INFO mapred.JobClient:     ASSOC=231471790

09/08/19 18:43:02 INFO mapred.JobClient:     CURR_NETWORK=23944682

09/08/19 18:43:02 INFO mapred.JobClient:     CURR_PROVIDER=1801150

09/08/19 18:43:02 INFO mapred.JobClient:   Map-Reduce Framework

09/08/19 18:43:02 INFO mapred.JobClient:     Map input records=22692113

09/08/19 18:43:02 INFO mapred.JobClient:     Spilled Records=0


</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.2, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestWildcardColumnTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="relates to" type="Reference">1951</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-11-06 19:12:27" id="1957" opendate="2009-11-06 08:40:14" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Get-s can&amp;apos;t set a Filter</summary>
			
			
			<description>This is an issue directly related to HBASE-1646. Get#write and Get#readFields both use  HbaseObjectWritable to write filters and when it comes to custom filters or filters in general that are not hardcoded in HbaseObjectWritable , an exception is thrown. 
It has been fixed in the issue noted above for Scan. Attached patch fixes it fot Get too.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.2, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Get.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is a clone of" type="Cloners">1646</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2009-11-07 23:43:24" id="1928" opendate="2009-10-22 17:32:22" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>ROOT and META tables stay in transition state (making the system not usable) if the designated regionServer dies before the assignment is complete</summary>
			
			
			<description>During a ROOT or META table re-assignment if the designated regionServer dies before the assignment is complete then the whole cluster becomes unavailble since the ROOT or META tables cannot be accessed (and never recover since they are kept in a transition state).
These are the 4 steps to replicate this issue (this is the easiest way to replicate. You can imagine that the following can occur in any real system).
Pre condition
============
1. a cluster of 3 nodes (cache01, cache02, search01).
2. start the system (start-hbase)
3. cache02 has META, search01 has ROOT, cache01 has regionServer and Master.
Case 1:
=======
1. kill cache01
2. kill cache02
3. now search01 has both ROOT and META.
4. re-start RegionServers on cache01 and cache02
5. Tail the master logs and grep for &quot;Assigning region ROOT&quot; and also &quot;Assigning region .META.&quot; (need to windows for easiness)
6. kill search01
7. wait to see to which server the ROOT will be assigned (from the tail)
8. quickly kill that server
9. you should notice that the ROOT server never gets re-assigned (because it is stuck in the regionsInTransitions)
The termination occurs through the ServerManager::removeServerInfo since the regionServer sends back to the master in a report that it is shutting down.
Case 2:
========
Repeat Case1 and in step 7 and 8 kill the server that has the META region assigned to it. Again the cluster becomes unavailble because the META region stays in the regionsInTransitions.
The termination occurs through the ServerManager::removeServerInfo since the regionServer sends back to the master in a report that it is shutting down.
Case 3:
========
Repeat Case1 and in step 7 and 8 kill the server with kill -9 instead of kill. This will not give the opportunity to the regionServer to send back the master in the report that it is terminating. The master will realize this because the znode will expire (but it is a different code path from before - it goes to the ProcessServerShutdown).
Case 4:
========
Repeat Case3 and in step 7 and 8 kill the server with kill -9 instead of kill. This will not give the opportunity to the regionServer to send back the master in the report that it is terminating. The master will realize this because the znode will expire (but it is a different code path from before - it goes to the ProcessServerShutdown).
The solution would be to check the in the ServerManager:removeServerInfo and in  ProcessServerShutdown::closeMetaRegions whether the server that has been terminated has been assigned either the ROOT or META table. And if they have make sure we make those table ready to be re-assigned again.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.2, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.master.RegionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ServerManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
	<bug fixdate="2009-12-08 00:25:39" id="2027" opendate="2009-12-03 01:27:55" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>HConnectionManager.HBASE_INSTANCES leaks TableServers</summary>
			
			
			<description>HConnectionManager.HBASE_INSTANCES is a WeakHashMap from HBaseConfiguration to TableServers.  However, each TableServers has a strong reference back to the HBaseConfiguration key so they are never freed.  (See note at http://java.sun.com/javase/6/docs/api/java/util/WeakHashMap.html : &quot;Implementation note: The value objects in a WeakHashMap are held by ordinary strong references. Thus care should be taken to ensure that value objects do not strongly refer to their own keys, either directly or indirectly, since that will prevent the keys from being discarded.&quot;)
Moreover, HBaseConfiguration implements hashCode() but not equals() so identical HBaseConfiguration objects each get their own TableServers object.
We had a long running HBase client process that was creating new HTable() objects, each creating a new HBaseConfiguration() and thus a new TableServers object.  It eventually went OOM, and gave a heap dump indicating 360 MB of data retained by HBASE_INSTANCES.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.20.3, 0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HConnectionManager.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.HBaseConfiguration.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is cloned by" type="Cloners">2925</link>
			
			
			<link description="is related to" type="Reference">1251</link>
			
			
			<link description="is related to" type="Reference">1976</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2010-07-17 15:38:05" id="1876" opendate="2009-09-30 16:48:43" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>DroppedSnapshotException when flushing memstore after a datanode dies</summary>
			
			
			<description>A dead datanode in the cluster can lead to multiple HRegionServer failures and corrupted data. The HRegionServer failures can be reproduced  consistently on a 7 machines cluster with approx 2000 regions.
Steps to reproduce
The easiest and safest way is to reproduce it for the .META. table, however it will work with any table. 
Locate a datanode that stores the .META. files and kill -9 it. 
In order to get multiple writes to the .META. table bring up or shut down a region server this will eventually cause a flush on the memstore
2009-09-25 09:26:17,775 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on .META.,demo__assets,asset_283132172,1252898166036,1253265069920
2009-09-25 09:26:17,775 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for region .META.,demo__assets,asset_283132172,1252898166036,1253265069920. Current region memstore si
ze 16.3k
2009-09-25 09:26:17,791 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Bad connect ack with firstBadLink 10.72.79.108:50010
2009-09-25 09:26:17,791 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-8767099282771605606_176852
The DFSClient will retry for 3 times, but there&amp;amp;apos;s a high chance it will try on the same failed datanode (it takes around 10 minutes for dead datanode to be removed from cluster)
2009-09-25 09:26:41,810 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2814)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2078)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2264)
2009-09-25 09:26:41,810 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_5317304716016587434_176852 bad datanode[2] nodes == null
2009-09-25 09:26:41,810 WARN org.apache.hadoop.hdfs.DFSClient: Could not get block locations. Source file &quot;/hbase/.META./225980069/info/5573114819456511457&quot; - Aborting...
2009-09-25 09:26:41,810 FATAL org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: .META.,demo__assets,asset_283132172,1252898166036,1253265069920
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:942)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:835)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:241)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
Caused by: java.io.IOException: Bad connect ack with firstBadLink 10.72.79.108:50010
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2872)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2795)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2078)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2264)
After the HRegionServer shuts down itself the regions will be reassigned however you might hit this 
2009-09-26 08:04:23,646 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: .META.,demo__assets,asset_283132172,1252898166036,1253265069920
2009-09-26 08:04:23,684 WARN org.apache.hadoop.hbase.regionserver.Store: Skipping hdfs://b0:9000/hbase/.META./225980069/historian/1432202951743803786 because its empty. HBASE-646 DATA LOSS?
...
2009-09-26 08:04:23,776 INFO org.apache.hadoop.hbase.regionserver.HRegion: region .META.,demo__assets,asset_283132172,1252898166036,1253265069920/225980069 available; sequence id is 1331458484
We ended up with corrupted data in .META. &quot;info:server&quot; after master got confirmation that it was updated from the HRegionServer that got DroppedSnapshotException
Since after a cluster restart server:info will be correct, .META. is safer to test with. Also to detect data corruption you can just scan .META. get the start key for each region and attempt to retrieve it from the corresponding table. If .META. is corrupted you get a NotServingRegionException. 
This issue is related to https://issues.apache.org/jira/browse/HDFS-630 
I attached a patch for HDFS-630 https://issues.apache.org/jira/secure/attachment/12420919/HDFS-630.patch that fixes this problem. </description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.client.HTablePool.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.CompareFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.SingleColumnValueFilter.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.Result.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.filter.package-info.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">630</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2010-09-08 17:23:06" id="1485" opendate="2009-06-05 18:08:05" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>Wrong or indeterminate behavior when there are duplicate versions of a column</summary>
			
			
			<description>As of now, both gets and scanners will end up returning all duplicate versions of a column.  The ordering of them is indeterminate.
We need to decide what the desired/expected behavior should be and make it happen.
Note:  It&amp;amp;apos;s nearly impossible for this to work with Gets as they are now implemented in 1304 so this is really a Scanner issue.  To implement this correctly with Gets, we would have to undo basically all the optimizations that Gets do and making them far slower than a Scanner.</description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.90.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.MemStore.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.ColumnTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.Store.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.KeyValueScanFixture.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.MinorCompactingStoreScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestExplicitColumnTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.TestScanWildcardColumnTracker.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.KeyValueScanner.java</file>
			
			
			<file type="M">org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
			
		
		</fixedFiles>
		
		
		<links>
			
			
			<link description="is blocked by" type="Blocker">2265</link>
			
			
			<link description="is blocked by" type="Blocker">2406</link>
			
			
			<link description="relates to" type="Reference">997</link>
			
		
		</links>
		
	
	</bug>
	<bug fixdate="2010-12-07 22:36:25" id="1888" opendate="2009-10-06 14:27:41" resolution="Fixed">
		
		
		<buginformation>
			
			
			<summary>KeyValue methods throw NullPointerException instead of IllegalArgumentException during parameter sanity check</summary>
			
			
			<description>Methods of org.apache.hadoop.hbase.KeyValue
public static int getDelimiter(final byte [] b, int offset, final int length, final int delimiter)
public static int getDelimiterInReverse(final byte [] b, final int offset, final int length, final int delimiter)
throw NullPointerException instead of IllegalArgumentException when byte array b is check for null  - which is very bad practice!
Please refactor this because this can be very misleading.  </description>
			
			
			<version>0.20.0</version>
			
			
			<fixedVersion>0.92.0</fixedVersion>
			
			
			<type>Bug</type>
			
		
		</buginformation>
		
		
		<fixedFiles>
			
			
			<file type="M">org.apache.hadoop.hbase.KeyValue.java</file>
			
		
		</fixedFiles>
		
	
	</bug>
</bugrepository>
